[{"title":"二进制部署 Kubernetes v1.29.2 ipv4ipv6双栈","url":"/2024/04/25/二进制部署 Kubernetes v1.29.2 ipv4ipv6双栈/","content":"\n# 二进制部署 Kubernetes v1.29.2 ipv4/ipv6双栈\n\n# 介绍\n\nkubernetes（k8s）二进制高可用安装部署，支持IPv4+IPv6双栈。\n\n我使用IPV6的目的是在公网进行访问，所以我配置了IPV6静态地址。\n\n若您没有IPV6环境，或者不想使用IPv6，不对主机进行配置IPv6地址即可。\n\n不配置IPV6，不影响后续，不过集群依旧是支持IPv6的。为后期留有扩展可能性。\n\n若不要IPv6 ，不给网卡配置IPv6即可，不要对IPv6相关配置删除或操作，否则会出问题。\n\n\n# 1.环境\n\n| 主机名称 | IP地址       | 说明       | 软件                                                         |\n| -------- | ------------ | ---------- | ------------------------------------------------------------ |\n|          | 192.168.1.60 | 外网节点   | 下载各种所需安装包                                           |\n| Master01 | 192.168.1.31 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Master02 | 192.168.1.32 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Master03 | 192.168.1.33 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Node01   | 192.168.1.34 | node节点   | kubelet、kube-proxy、nfs-client、nginx                       |\n| Node02   | 192.168.1.35 | node节点   | kubelet、kube-proxy、nfs-client、nginx                       |\n|          | 192.168.1.36 | VIP        |                                                              |\n\n\n网段\n物理主机：192.168.1.0/24\nservice：10.96.0.0/12\npod：172.16.0.0/12\n\n安装包已经整理好：https://mirrors.chenby.cn/https://github.com/cby-chen/Kubernetes/releases/download/v1.29.2/kubernetes-v1.29.2.tar\n\n\n\n## 1.1.k8s基础系统环境配置\n\n### 1.2.配置IP\n\n```shell\n# 注意！\n# 若虚拟机是进行克隆的那么网卡的UUID会重复\n# 若UUID重复需要重新生成新的UUID\n# UUID重复无法获取到IPV6地址\n# \n# 查看当前的网卡列表和 UUID：\n# nmcli con show\n# 删除要更改 UUID 的网络连接：\n# nmcli con delete uuid <原 UUID>\n# 重新生成 UUID：\n# nmcli con add type ethernet ifname <接口名称> con-name <新名称>\n# 重新启用网络连接：\n# nmcli con up <新名称>\n\n# 更改网卡的UUID\nssh root@192.168.1.31 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.32 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.33 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.34 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.35 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh ssh root@192.168.1.31\n# 使用SSH登录到IP为192.168.1.31的主机，使用root用户身份。\n# \n# nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44\n# 删除 UUID 为 708a1497-2192-43a5-9f03-2ab936fb3c44 的网络连接，这是 NetworkManager 中一种特定网络配置的唯一标识符。\n# \n# nmcli con add type ethernet ifname eth0 con-name eth0\n# 添加一种以太网连接类型，并指定接口名为 eth0，连接名称也为 eth0。\n# \n# nmcli con up eth0\n# 开启 eth0 这个网络连接。\n# \n# 简单来说，这个命令的作用是删除一个特定的网络连接配置，并添加一个名为 eth0 的以太网连接，然后启用这个新的连接。\n\n# 修改静态的IPv4地址\nssh root@192.168.1.104 \"nmcli con mod eth0 ipv4.addresses 192.168.1.31/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.106 \"nmcli con mod eth0 ipv4.addresses 192.168.1.32/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.107 \"nmcli con mod eth0 ipv4.addresses 192.168.1.33/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.109 \"nmcli con mod eth0 ipv4.addresses 192.168.1.34/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.110 \"nmcli con mod eth0 ipv4.addresses 192.168.1.35/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh root@192.168.1.154\n# 使用SSH登录到IP为192.168.1.154的主机，使用root用户身份。\n# \n# \"nmcli con mod eth0 ipv4.addresses 192.168.1.31/24\"\n# 修改eth0网络连接的IPv4地址为192.168.1.31，子网掩码为 24。\n# \n# \"nmcli con mod eth0 ipv4.gateway 192.168.1.1\"\n# 修改eth0网络连接的IPv4网关为192.168.1.1。\n# \n# \"nmcli con mod eth0 ipv4.method manual\"\n# 将eth0网络连接的IPv4配置方法设置为手动。\n# \n# \"nmcli con mod eth0 ipv4.dns \"8.8.8.8\"\n# 将eth0网络连接的IPv4 DNS服务器设置为 8.8.8.8。\n# \n# \"nmcli con up eth0\"\n# 启动eth0网络连接。\n# \n# 总体来说，这条命令是通过SSH远程登录到指定的主机，并使用网络管理命令 (nmcli) 修改eth0网络连接的配置，包括IP地址、网关、配置方法和DNS服务器，并启动该网络连接。\n\n# 没有IPv6选择不配置即可\nssh root@192.168.1.31 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::10; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.32 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::20; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.33 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::30; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.34 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::40; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.35 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::50; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh root@192.168.1.31\n# 通过SSH连接到IP地址为192.168.1.31的远程主机，使用root用户进行登录。\n# \n# \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::10\"\n# 使用nmcli命令修改eth0接口的IPv6地址为fc00:43f4:1eea:1::10。\n# \n# \"nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1\"\n# 使用nmcli命令修改eth0接口的IPv6网关为fc00:43f4:1eea:1::1。\n# \n# \"nmcli con mod eth0 ipv6.method manual\"\n# 使用nmcli命令将eth0接口的IPv6配置方法修改为手动配置。\n# \n# \"nmcli con mod eth0 ipv6.dns \"2400:3200::1\"\n# 使用nmcli命令设置eth0接口的IPv6 DNS服务器为2400:3200::1。\n# \n# \"nmcli con up eth0\"\n# 使用nmcli命令启动eth0接口。\n# \n# 这个命令的目的是在远程主机上配置eth0接口的IPv6地址、网关、配置方法和DNS服务器，并启动eth0接口。\n\n# 查看网卡配置\n# nmcli device show eth0\n# nmcli con show eth0\n[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 \nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=none\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=no\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=no\nIPV6_ADDR_GEN_MODE=stable-privacy\nNAME=eth0\nUUID=2aaddf95-3f36-4a48-8626-b55ebf7f53e7\nDEVICE=eth0\nONBOOT=yes\nIPADDR=192.168.1.31\nPREFIX=24\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\n[root@localhost ~]# \n\n# 参数解释\n# \n# TYPE=Ethernet\n# 指定连接类型为以太网。\n# \n# PROXY_METHOD=none\n# 指定不使用代理方法。\n# \n# BROWSER_ONLY=no\n# 指定不仅仅在浏览器中使用代理。\n# \n# BOOTPROTO=none\n# 指定自动分配地址的方式为无（即手动配置IP地址）。\n# \n# DEFROUTE=yes\n# 指定默认路由开启。\n# \n# IPV4_FAILURE_FATAL=no\n# 指定IPv4连接失败时不宣告严重错误。\n# \n# IPV6INIT=yes\n# 指定启用IPv6。\n# \n# IPV6_AUTOCONF=no\n# 指定不自动配置IPv6地址。\n# \n# IPV6_DEFROUTE=yes\n# 指定默认IPv6路由开启。\n# \n# IPV6_FAILURE_FATAL=no\n# 指定IPv6连接失败时不宣告严重错误。\n# \n# IPV6_ADDR_GEN_MODE=stable-privacy\n# 指定IPv6地址生成模式为稳定隐私模式。\n# \n# NAME=eth0\n# 指定设备名称为eth0。\n# \n# UUID=424fd260-c480-4899-97e6-6fc9722031e8\n# 指定设备的唯一标识符。\n# \n# DEVICE=eth0\n# 指定设备名称为eth0。\n# \n# ONBOOT=yes\n# 指定开机自动启用这个连接。\n# \n# IPADDR=192.168.1.31\n# 指定IPv4地址为192.168.1.31。\n# \n# PREFIX=24\n# 指定IPv4地址的子网掩码为24。\n# \n# GATEWAY=192.168.8.1\n# 指定IPv4的网关地址为192.168.8.1。\n# \n# DNS1=8.8.8.8\n# 指定首选DNS服务器为8.8.8.8。\n# \n# IPV6ADDR=fc00:43f4:1eea:1::10/128\n# 指定IPv6地址为fc00:43f4:1eea:1::10，子网掩码为128。\n# \n# IPV6_DEFAULTGW=fc00:43f4:1eea:1::1\n# 指定IPv6的默认网关地址为fc00:43f4:1eea:1::1。\n# \n# DNS2=2400:3200::1\n# 指定备用DNS服务器为2400:3200::1。\n```\n\n### 1.3.设置主机名\n\n```shell\nhostnamectl set-hostname k8s-master01\nhostnamectl set-hostname k8s-master02\nhostnamectl set-hostname k8s-master03\nhostnamectl set-hostname k8s-node01\nhostnamectl set-hostname k8s-node02\n\n# 参数解释\n# \n# 参数: set-hostname\n# 解释: 这是hostnamectl命令的一个参数，用于设置系统的主机名。\n# \n# 参数: k8s-master01\n# 解释: 这是要设置的主机名，将系统的主机名设置为\"k8s-master01\"。\n```\n\n\n### 1.4.配置yum源\n\n```shell\n# 其他系统的源地址\n# https://mirrors.tuna.tsinghua.edu.cn/help/\n\n# 对于 Ubuntu\nsed -i 's/cn.archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n# 对于 CentOS 7\nsudo sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n         -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' \\\n         -i.bak \\\n         /etc/yum.repos.d/CentOS-*.repo\n\n# 对于 CentOS 8\nsudo sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n         -e 's|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' \\\n         -i.bak \\\n         /etc/yum.repos.d/CentOS-*.repo\n\n# 对于私有仓库\nsed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/\\$contentdir|baseurl=http://192.168.1.123/centos|g' -i.bak  /etc/yum.repos.d/CentOS-*.repo\n\n# 参数解释\n# \n# 以上命令是用于更改系统软件源的配置，以便从国内镜像站点下载软件包和更新。\n# \n# 对于 Ubuntu 系统，将 /etc/apt/sources.list 文件中的软件源地址 cn.archive.ubuntu.com 替换为 mirrors.ustc.edu.cn。\n# \n# 对于 CentOS 7 系统，将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为 https://mirrors.tuna.tsinghua.edu.cn/centos。\n# \n# 对于 CentOS 8 系统，同样将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为 https://mirrors.tuna.tsinghua.edu.cn/centos。\n# \n# 对于私有仓库，将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为私有仓库地址 http://192.168.1.123/centos。\n# \n# 这些命令通过使用 sed 工具和正则表达式，对相应的配置文件进行批量的替换操作，从而更改系统软件源配置。\n```\n\n### 1.5.安装一些必备工具\n\n```shell\n# 对于 Ubuntu\napt update && apt upgrade -y && apt install -y wget psmisc vim net-tools nfs-kernel-server telnet lvm2 git tar curl\n\n# 对于 CentOS 7\nyum update -y && yum -y install  wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl\n\n# 对于 CentOS 8\nyum update -y && yum -y install wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git network-scripts tar curl\n```\n\n#### 1.5.1 下载离线所需文件(可选)\n\n 在互联网服务器上安装一个一模一样的系统进行下载所需包\n\n##### CentOS7\n```shell\n# 下载必要工具\nyum -y install createrepo yum-utils wget epel*\n\n# 下载全量依赖包\nrepotrack createrepo wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\n\n# 删除libseccomp\nrm -rf libseccomp-*.rpm\n\n# 下载libseccomp\nwget http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n\n# 创建yum源信息\ncreaterepo -u -d /data/centos7/\n\n# 拷贝包到内网机器上\nscp -r /data/centos7/ root@192.168.1.31:\nscp -r /data/centos7/ root@192.168.1.32:\nscp -r /data/centos7/ root@192.168.1.33:\nscp -r /data/centos7/ root@192.168.1.34:\nscp -r /data/centos7/ root@192.168.1.35:\n\n# 在内网机器上创建repo配置文件\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos7/\ngpgcheck=0\nenabled=1\nEOF\n\n# 安装下载好的包\nyum clean all\nyum makecache\nyum install /root/centos7/* --skip-broken -y\n\n#### 备注 #####\n# 安装完成后，可能还会出现yum无法使用那么再次执行\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos7/\ngpgcheck=0\nenabled=1\nEOF\nyum clean all\nyum makecache\nyum install /root/centos7/* --skip-broken -y\n\n#### 备注 #####\n# 安装 chrony 和 libseccomp\n# yum install /root/centos7/libseccomp-2.5.1*.rpm -y\n# yum install /root/centos7/chrony-*.rpm -y\n```\n##### CentOS8\n```shell\n# 下载必要工具\nyum -y install createrepo yum-utils wget epel*\n\n# 下载全量依赖包\nrepotrack wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git network-scripts tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\n\n# 创建yum源信息\ncreaterepo -u -d /data/centos8/\n\n# 拷贝包到内网机器上\nscp -r centos8/ root@192.168.1.31:\nscp -r centos8/ root@192.168.1.32:\nscp -r centos8/ root@192.168.1.33:\nscp -r centos8/ root@192.168.1.34:\nscp -r centos8/ root@192.168.1.35:\n\n# 在内网机器上创建repo配置文件\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos8/\ngpgcheck=0\nenabled=1\nEOF\n\n# 安装下载好的包\nyum clean all\nyum makecache\nyum install /root/centos8/* --skip-broken -y\n\n#### 备注 #####\n# 安装完成后，可能还会出现yum无法使用那么再次执行\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos8/\ngpgcheck=0\nenabled=1\nEOF\nyum clean all\nyum makecache\nyum install /root/centos8/* --skip-broken -y\n```\n\n##### Ubuntu 下载包和依赖\n```shell\n#!/bin/bash\n\nlogfile=123.log\nret=\"\"\nfunction getDepends()\n{\n   echo \"fileName is\" $1>>$logfile\n   # use tr to del < >\n   ret=`apt-cache depends $1|grep Depends |cut -d: -f2 |tr -d \"<>\"`\n   echo $ret|tee  -a $logfile\n}\n# 需要获取其所依赖包的包\nlibs=\"wget psmisc vim net-tools nfs-kernel-server telnet lvm2 git tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\"\n\n# download libs dependen. deep in 3\ni=0\nwhile [ $i -lt 3 ] ;\ndo\n    let i++\n    echo $i\n    # download libs\n    newlist=\" \"\n    for j in $libs\n    do\n        added=\"$(getDepends $j)\"\n        newlist=\"$newlist $added\"\n        apt install $added --reinstall -d -y\n    done\n\n    libs=$newlist\ndone\n\n# 创建源信息\napt install dpkg-dev\nsudo cp /var/cache/apt/archives/*.deb /data/ubuntu/ -r\ndpkg-scanpackages . /dev/null |gzip > /data/ubuntu/Packages.gz -r\n\n# 拷贝包到内网机器上\nscp -r ubuntu/ root@192.168.1.31:\nscp -r ubuntu/ root@192.168.1.32:\nscp -r ubuntu/ root@192.168.1.33:\nscp -r ubuntu/ root@192.168.1.34:\nscp -r ubuntu/ root@192.168.1.35:\n\n# 在内网机器上配置apt源\nvim /etc/apt/sources.list\ncat /etc/apt/sources.list\ndeb file:////root/ ubuntu/\n\n# 安装deb包\napt install ./*.deb\n\n```\n\n\n### 1.6.选择性下载需要工具\n\n```shell\n#!/bin/bash\n\n# 查看版本地址：\n# \n# https://github.com/containernetworking/plugins/releases/\n# https://github.com/containerd/containerd/releases/\n# https://github.com/kubernetes-sigs/cri-tools/releases/\n# https://github.com/Mirantis/cri-dockerd/releases/\n# https://github.com/etcd-io/etcd/releases/\n# https://github.com/cloudflare/cfssl/releases/\n# https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG\n# https://download.docker.com/linux/static/stable/x86_64/\n# https://github.com/opencontainers/runc/releases/\n# https://mirrors.tuna.tsinghua.edu.cn/elrepo/kernel/el7/x86_64/RPMS/\n# https://github.com/helm/helm/tags\n# http://nginx.org/download/\n\n# Version numbers\ncni_plugins_version='v1.4.0'\ncri_containerd_cni_version='1.7.13'\ncrictl_version='v1.29.0'\ncri_dockerd_version='0.3.10'\netcd_version='v3.5.12'\ncfssl_version='1.6.4'\nkubernetes_server_version='1.29.2'\ndocker_version='25.0.3'\nrunc_version='1.1.12'\nkernel_version='5.4.268'\nhelm_version='3.14.1'\nnginx_version='1.25.4'\n\n# URLs \nbase_url='https://mirrors.chenby.cn/https://github.com'\nkernel_url=\"http://mirrors.tuna.tsinghua.edu.cn/elrepo/kernel/el7/x86_64/RPMS/kernel-lt-${kernel_version}-1.el7.elrepo.x86_64.rpm\"\nrunc_url=\"${base_url}/opencontainers/runc/releases/download/v${runc_version}/runc.amd64\"\ndocker_url=\"https://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-${docker_version}.tgz\"\ncni_plugins_url=\"${base_url}/containernetworking/plugins/releases/download/${cni_plugins_version}/cni-plugins-linux-amd64-${cni_plugins_version}.tgz\"\ncri_containerd_cni_url=\"${base_url}/containerd/containerd/releases/download/v${cri_containerd_cni_version}/cri-containerd-cni-${cri_containerd_cni_version}-linux-amd64.tar.gz\"\ncrictl_url=\"${base_url}/kubernetes-sigs/cri-tools/releases/download/${crictl_version}/crictl-${crictl_version}-linux-amd64.tar.gz\"\ncri_dockerd_url=\"${base_url}/Mirantis/cri-dockerd/releases/download/v${cri_dockerd_version}/cri-dockerd-${cri_dockerd_version}.amd64.tgz\"\netcd_url=\"${base_url}/etcd-io/etcd/releases/download/${etcd_version}/etcd-${etcd_version}-linux-amd64.tar.gz\"\ncfssl_url=\"${base_url}/cloudflare/cfssl/releases/download/v${cfssl_version}/cfssl_${cfssl_version}_linux_amd64\"\ncfssljson_url=\"${base_url}/cloudflare/cfssl/releases/download/v${cfssl_version}/cfssljson_${cfssl_version}_linux_amd64\"\nhelm_url=\"https://mirrors.huaweicloud.com/helm/v${helm_version}/helm-v${helm_version}-linux-amd64.tar.gz\"\nkubernetes_server_url=\"https://storage.googleapis.com/kubernetes-release/release/v${kubernetes_server_version}/kubernetes-server-linux-amd64.tar.gz\"\nnginx_url=\"http://nginx.org/download/nginx-${nginx_version}.tar.gz\"\n\n# Download packages\npackages=(\n  $kernel_url\n  $runc_url\n  $docker_url\n  $cni_plugins_url\n  $cri_containerd_cni_url\n  $crictl_url\n  $cri_dockerd_url\n  $etcd_url\n  $cfssl_url\n  $cfssljson_url\n  $helm_url\n  $kubernetes_server_url\n  $nginx_url\n)\n\nfor package_url in \"${packages[@]}\"; do\n  filename=$(basename \"$package_url\")\n  if curl --parallel --parallel-immediate -k -L -C - -o \"$filename\" \"$package_url\"; then\n    echo \"Downloaded $filename\"\n  else\n    echo \"Failed to download $filename\"\n    exit 1\n  fi\ndone\n```\n\n### 1.7.关闭防火墙\n\n```shell\n# Ubuntu忽略，CentOS执行\nsystemctl disable --now firewalld\n```\n\n### 1.8.关闭SELinux\n\n```shell\n# Ubuntu忽略，CentOS执行\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n\n# 参数解释\n# \n# setenforce 0\n# 此命令用于设置 SELinux 的执行模式。0 表示关闭 SELinux。\n# \n# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n# 该命令使用 sed 工具来编辑 /etc/selinux/config 文件。其中 '-i' 参数表示直接修改原文件，而不是输出到终端或另一个文件。's#SELINUX=enforcing#SELINUX=disabled#g' 是 sed 的替换命令，它将文件中所有的 \"SELINUX=enforcing\" 替换为 \"SELINUX=disabled\"。这里的 '#' 是分隔符，用于替代传统的 '/' 分隔符，以避免与路径中的 '/' 冲突。\n```\n\n### 1.9.关闭交换分区\n\n```shell\nsed -ri 's/.*swap.*/#&/' /etc/fstab\nswapoff -a && sysctl -w vm.swappiness=0\n\ncat /etc/fstab\n# /dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n# 参数解释：\n# \n# -ri: 这个参数用于在原文件中替换匹配的模式。-r表示扩展正则表达式，-i允许直接修改文件。\n# 's/.*swap.*/#&/': 这是一个sed命令，用于在文件/etc/fstab中找到包含swap的行，并在行首添加#来注释掉该行。\n# /etc/fstab: 这是一个文件路径，即/etc/fstab文件，用于存储文件系统表。\n# swapoff -a: 这个命令用于关闭所有启用的交换分区。\n# sysctl -w vm.swappiness=0: 这个命令用于修改vm.swappiness参数的值为0，表示系统在物理内存充足时更倾向于使用物理内存而非交换分区。\n```\n\n### 1.10.网络配置（俩种方式二选一）\n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 方式一\n# systemctl disable --now NetworkManager\n# systemctl start network && systemctl enable network\n\n# 方式二\ncat > /etc/NetworkManager/conf.d/calico.conf << EOF \n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*\nEOF\nsystemctl restart NetworkManager\n\n# 参数解释\n#\n# 这个参数用于指定不由 NetworkManager 管理的设备。它由以下两个部分组成\n# \n# interface-name:cali*\n# 表示以 \"cali\" 开头的接口名称被排除在 NetworkManager 管理之外。例如，\"cali0\", \"cali1\" 等接口不受 NetworkManager 管理。\n# \n# interface-name:tunl*\n# 表示以 \"tunl\" 开头的接口名称被排除在 NetworkManager 管理之外。例如，\"tunl0\", \"tunl1\" 等接口不受 NetworkManager 管理。\n# \n# 通过使用这个参数，可以将特定的接口排除在 NetworkManager 的管理范围之外，以便其他工具或进程可以独立地管理和配置这些接口。\n```\n\n### 1.11.进行时间同步 \n\n```shell\n# 服务端\n# apt install chrony -y\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \npool ntp.aliyun.com iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nallow 192.168.1.0/24\nlocal stratum 10\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n\nsystemctl restart chronyd ; systemctl enable chronyd\n\n# 客户端\n# apt install chrony -y\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \npool 192.168.1.31 iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n\nsystemctl restart chronyd ; systemctl enable chronyd\n\n#使用客户端进行验证\nchronyc sources -v\n\n# 参数解释\n#\n# pool ntp.aliyun.com iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度。\n# \n# driftfile /var/lib/chrony/drift\n# 指定用于保存时钟漂移信息的文件路径。\n# \n# makestep 1.0 3\n# 设置当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整。\n# \n# rtcsync\n# 启用硬件时钟同步功能，可以提高时钟的准确性。\n# \n# allow 192.168.0.0/24\n# 允许192.168.0.0/24网段范围内的主机与chrony进行时间同步。\n# \n# local stratum 10\n# 将本地时钟设为stratum 10，stratum值表示时钟的准确度，值越小表示准确度越高。\n# \n# keyfile /etc/chrony.keys\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证。\n# \n# leapsectz right/UTC\n# 指定时区为UTC。\n# \n# logdir /var/log/chrony\n# 指定日志文件存放目录。\n```\n\n### 1.12.配置ulimit\n\n```shell\nulimit -SHn 65535\ncat >> /etc/security/limits.conf <<EOF\n* soft nofile 655360\n* hard nofile 131072\n* soft nproc 655350\n* hard nproc 655350\n* seft memlock unlimited\n* hard memlock unlimitedd\nEOF\n\n# 参数解释\n#\n# soft nofile 655360\n# soft表示软限制，nofile表示一个进程可打开的最大文件数，默认值为1024。这里的软限制设置为655360，即一个进程可打开的最大文件数为655360。\n#\n# hard nofile 131072\n# hard表示硬限制，即系统设置的最大值。nofile表示一个进程可打开的最大文件数，默认值为4096。这里的硬限制设置为131072，即系统设置的最大文件数为131072。\n#\n# soft nproc 655350\n# soft表示软限制，nproc表示一个用户可创建的最大进程数，默认值为30720。这里的软限制设置为655350，即一个用户可创建的最大进程数为655350。\n#\n# hard nproc 655350\n# hard表示硬限制，即系统设置的最大值。nproc表示一个用户可创建的最大进程数，默认值为4096。这里的硬限制设置为655350，即系统设置的最大进程数为655350。\n#\n# seft memlock unlimited\n# seft表示软限制，memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的软限制设置为unlimited，即一个进程可锁定的最大内存为无限制。\n#\n# hard memlock unlimited\n# hard表示硬限制，即系统设置的最大值。memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的硬限制设置为unlimited，即系统设置的最大内存锁定为无限制。\n```\n\n### 1.13.配置免密登录\n\n```shell\n# apt install -y sshpass\nyum install -y sshpass\nssh-keygen -f /root/.ssh/id_rsa -P ''\nexport IP=\"192.168.1.31 192.168.1.32 192.168.1.33 192.168.1.34 192.168.1.35\"\nexport SSHPASS=123123\nfor HOST in $IP;do\n     sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $HOST\ndone\n\n# 这段脚本的作用是在一台机器上安装sshpass工具，并通过sshpass自动将本机的SSH公钥复制到多个远程主机上，以实现无需手动输入密码的SSH登录。\n# \n# 具体解释如下：\n# \n# 1. `apt install -y sshpass` 或 `yum install -y sshpass`：通过包管理器（apt或yum）安装sshpass工具，使得后续可以使用sshpass命令。\n# \n# 2. `ssh-keygen -f /root/.ssh/id_rsa -P ''`：生成SSH密钥对。该命令会在/root/.ssh目录下生成私钥文件id_rsa和公钥文件id_rsa.pub，同时不设置密码（即-P参数后面为空），方便后续通过ssh-copy-id命令自动复制公钥。\n# \n# 3. `export IP=\"192.168.1.31 192.168.1.32 192.168.1.33 192.168.1.34 192.168.1.35\"`：设置一个包含多个远程主机IP地址的环境变量IP，用空格分隔开，表示要将SSH公钥复制到这些远程主机上。\n# \n# 4. `export SSHPASS=123123`：设置环境变量SSHPASS，将sshpass所需的SSH密码（在这里是\"123123\"）赋值给它，这样sshpass命令可以自动使用这个密码进行登录。\n# \n# 5. `for HOST in $IP;do`：遍历环境变量IP中的每个IP地址，并将当前IP地址赋值给变量HOST。\n# \n# 6. `sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $HOST`：使用sshpass工具复制本机的SSH公钥到远程主机。其中，-e选项表示使用环境变量中的密码（即SSHPASS）进行登录，-o StrictHostKeyChecking=no选项表示连接时不检查远程主机的公钥，以避免交互式确认。\n# \n# 通过这段脚本，可以方便地将本机的SSH公钥复制到多个远程主机上，实现无需手动输入密码的SSH登录。\n```\n\n### 1.14.添加启用源 \n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 为 RHEL-8或 CentOS-8配置源\nyum install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm -y \nsed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo \nsed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo \n\n# 为 RHEL-7 SL-7 或 CentOS-7 安装 ELRepo \nyum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y \nsed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo \nsed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo \n\n# 查看可用安装包\nyum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available\n```\n\n### 1.15.升级内核至4.18版本以上 \n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 安装最新的内核\n# 我这里选择的是稳定版kernel-ml   如需更新长期维护版本kernel-lt  \nyum -y --enablerepo=elrepo-kernel  install  kernel-ml\n\n# 查看已安装那些内核\nrpm -qa | grep kernel\n\n# 查看默认内核\ngrubby --default-kernel\n\n# 若不是最新的使用命令设置\ngrubby --set-default $(ls /boot/vmlinuz-* | grep elrepo)\n\n# 重启生效\nreboot\n\n# v8 整合命令为：\nyum install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm -y ; sed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo ; sed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo ; yum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available -y ; yum  --enablerepo=elrepo-kernel  install kernel-lt -y ; grubby --default-kernel ; reboot \n\n# v7 整合命令为：\nyum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y ; sed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo ; sed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo ; yum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available -y ; yum  --enablerepo=elrepo-kernel  install  kernel-lt -y ; grubby --set-default $(ls /boot/vmlinuz-* | grep elrepo) ; grubby --default-kernel ; reboot \n\n# 离线版本 \nyum install -y /root/cby/kernel-lt-*-1.el7.elrepo.x86_64.rpm ; grubby --set-default $(ls /boot/vmlinuz-* | grep elrepo) ; grubby --default-kernel ; reboot \n```\n\n### 1.16.安装ipvsadm \n\n```shell\n# 对于CentOS7离线安装\n# yum install /root/centos7/ipset-*.el7.x86_64.rpm /root/centos7/lm_sensors-libs-*.el7.x86_64.rpm  /root/centos7/ipset-libs-*.el7.x86_64.rpm /root/centos7/sysstat-*.el7_9.x86_64.rpm  /root/centos7/ipvsadm-*.el7.x86_64.rpm  -y\n\n# 对于 Ubuntu\n# apt install ipvsadm ipset sysstat conntrack -y\n\n# 对于 CentOS\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\ncat >> /etc/modules-load.d/ipvs.conf <<EOF \nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\nEOF\n\nsystemctl restart systemd-modules-load.service\n\nlsmod | grep -e ip_vs -e nf_conntrack\nip_vs_sh               16384  0\nip_vs_wrr              16384  0\nip_vs_rr               16384  0\nip_vs                 180224  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr\nnf_conntrack          176128  1 ip_vs\nnf_defrag_ipv6         24576  2 nf_conntrack,ip_vs\nnf_defrag_ipv4         16384  1 nf_conntrack\nlibcrc32c              16384  3 nf_conntrack,xfs,ip_vs\n\n# 参数解释\n#\n# ip_vs\n# IPVS 是 Linux 内核中的一个模块，用于实现负载均衡和高可用性。它通过在前端代理服务器上分发传入请求到后端实际服务器上，提供了高性能和可扩展的网络服务。\n#\n# ip_vs_rr\n# IPVS 的一种调度算法之一，使用轮询方式分发请求到后端服务器，每个请求按顺序依次分发。\n#\n# ip_vs_wrr\n# IPVS 的一种调度算法之一，使用加权轮询方式分发请求到后端服务器，每个请求按照指定的权重比例分发。\n#\n# ip_vs_sh\n# IPVS 的一种调度算法之一，使用哈希方式根据源 IP 地址和目标 IP 地址来分发请求。\n#\n# nf_conntrack\n# 这是一个内核模块，用于跟踪和管理网络连接，包括 TCP、UDP 和 ICMP 等协议。它是实现防火墙状态跟踪的基础。\n#\n# ip_tables\n# 这是一个内核模块，提供了对 Linux 系统 IP 数据包过滤和网络地址转换（NAT）功能的支持。\n#\n# ip_set\n# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的 IP 地址集合操作。\n#\n# xt_set\n# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的数据包匹配和操作。\n#\n# ipt_set\n# 这是一个用户空间工具，用于配置和管理 xt_set 内核模块。\n#\n# ipt_rpfilter\n# 这是一个内核模块，用于实现反向路径过滤，用于防止 IP 欺骗和 DDoS 攻击。\n#\n# ipt_REJECT\n# 这是一个 iptables 目标，用于拒绝 IP 数据包，并向发送方发送响应，指示数据包被拒绝。\n#\n# ipip\n# 这是一个内核模块，用于实现 IP 封装在 IP（IP-over-IP）的隧道功能。它可以在不同网络之间创建虚拟隧道来传输 IP 数据包。\n```\n\n### 1.17.修改内核参数 \n\n```shell\ncat <<EOF > /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nfs.may_detach_mounts = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\n\nnet.ipv6.conf.all.disable_ipv6 = 0\nnet.ipv6.conf.default.disable_ipv6 = 0\nnet.ipv6.conf.lo.disable_ipv6 = 0\nnet.ipv6.conf.all.forwarding = 1\nEOF\n\nsysctl --system\n\n# 这些是Linux系统的一些参数设置，用于配置和优化网络、文件系统和虚拟内存等方面的功能。以下是每个参数的详细解释：\n# \n# 1. net.ipv4.ip_forward = 1\n#    - 这个参数启用了IPv4的IP转发功能，允许服务器作为网络路由器转发数据包。\n# \n# 2. net.bridge.bridge-nf-call-iptables = 1\n#    - 当使用网络桥接技术时，将数据包传递到iptables进行处理。\n#   \n# 3. fs.may_detach_mounts = 1\n#    - 允许在挂载文件系统时，允许被其他进程使用。\n#   \n# 4. vm.overcommit_memory=1\n#    - 该设置允许原始的内存过量分配策略，当系统的内存已经被完全使用时，系统仍然会分配额外的内存。\n# \n# 5. vm.panic_on_oom=0\n#    - 当系统内存不足（OOM）时，禁用系统崩溃和重启。\n# \n# 6. fs.inotify.max_user_watches=89100\n#    - 设置系统允许一个用户的inotify实例可以监控的文件数目的上限。\n# \n# 7. fs.file-max=52706963\n#    - 设置系统同时打开的文件数的上限。\n# \n# 8. fs.nr_open=52706963\n#    - 设置系统同时打开的文件描述符数的上限。\n# \n# 9. net.netfilter.nf_conntrack_max=2310720\n#    - 设置系统可以创建的网络连接跟踪表项的最大数量。\n# \n# 10. net.ipv4.tcp_keepalive_time = 600\n#     - 设置TCP套接字的空闲超时时间（秒），超过该时间没有活动数据时，内核会发送心跳包。\n# \n# 11. net.ipv4.tcp_keepalive_probes = 3\n#     - 设置未收到响应的TCP心跳探测次数。\n# \n# 12. net.ipv4.tcp_keepalive_intvl = 15\n#     - 设置TCP心跳探测的时间间隔（秒）。\n# \n# 13. net.ipv4.tcp_max_tw_buckets = 36000\n#     - 设置系统可以使用的TIME_WAIT套接字的最大数量。\n# \n# 14. net.ipv4.tcp_tw_reuse = 1\n#     - 启用TIME_WAIT套接字的重新利用，允许新的套接字使用旧的TIME_WAIT套接字。\n# \n# 15. net.ipv4.tcp_max_orphans = 327680\n#     - 设置系统可以同时存在的TCP套接字垃圾回收包裹数的最大数量。\n# \n# 16. net.ipv4.tcp_orphan_retries = 3\n#     - 设置系统对于孤立的TCP套接字的重试次数。\n# \n# 17. net.ipv4.tcp_syncookies = 1\n#     - 启用TCP SYN cookies保护，用于防止SYN洪泛攻击。\n# \n# 18. net.ipv4.tcp_max_syn_backlog = 16384\n#     - 设置新的TCP连接的半连接数（半连接队列）的最大长度。\n# \n# 19. net.ipv4.ip_conntrack_max = 65536\n#     - 设置系统可以创建的网络连接跟踪表项的最大数量。\n# \n# 20. net.ipv4.tcp_timestamps = 0\n#     - 关闭TCP时间戳功能，用于提供更好的安全性。\n# \n# 21. net.core.somaxconn = 16384\n#     - 设置系统核心层的连接队列的最大值。\n# \n# 22. net.ipv6.conf.all.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 23. net.ipv6.conf.default.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 24. net.ipv6.conf.lo.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 25. net.ipv6.conf.all.forwarding = 1\n#     - 允许IPv6数据包转发。\n```\n\n### 1.18.所有节点配置hosts本地解析\n\n```shell\ncat > /etc/hosts <<EOF\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n192.168.1.31 k8s-master01\n192.168.1.32 k8s-master02\n192.168.1.33 k8s-master03\n192.168.1.34 k8s-node01\n192.168.1.35 k8s-node02\n192.168.1.36 lb-vip\nEOF\n```\n\n# 2.k8s基本组件安装\n\n**注意 ：  2.1 和 2.2 二选其一即可**\n\n## 2.1.安装Containerd作为Runtime （推荐）\n\n```shell\n# https://github.com/containernetworking/plugins/releases/\n# wget https://mirrors.chenby.cn/https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz\n\ncd cby/\n\n#创建cni插件所需目录\nmkdir -p /etc/cni/net.d /opt/cni/bin \n#解压cni二进制包\ntar xf cni-plugins-linux-amd64-v*.tgz -C /opt/cni/bin/\n\n# https://github.com/containerd/containerd/releases/\n# wget https://mirrors.chenby.cn/https://github.com/containerd/containerd/releases/download/v1.7.13/cri-containerd-cni-1.7.13-linux-amd64.tar.gz\n\n#解压\ntar -xzf cri-containerd-cni-*-linux-amd64.tar.gz -C /\n\n#创建服务启动文件\ncat > /etc/systemd/system/containerd.service <<EOF\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target local-fs.target\n\n[Service]\nExecStartPre=-/sbin/modprobe overlay\nExecStart=/usr/local/bin/containerd\nType=notify\nDelegate=yes\nKillMode=process\nRestart=always\nRestartSec=5\nLimitNPROC=infinity\nLimitCORE=infinity\nLimitNOFILE=infinity\nTasksMax=infinity\nOOMScoreAdjust=-999\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n# 参数解释：\n#\n# 这是一个用于启动containerd容器运行时的systemd unit文件。下面是对该文件不同部分的详细解释：\n# \n# [Unit]\n# Description=containerd container runtime\n# 描述该unit的作用是作为containerd容器运行时。\n# \n# Documentation=https://containerd.io\n# 指向容器运行时的文档的URL。\n# \n# After=network.target local-fs.target\n# 定义了在哪些依赖项之后该unit应该被启动。在网络和本地文件系统加载完成后启动，确保了容器运行时在这些依赖项可用时才会启动。\n# \n# [Service]\n# ExecStartPre=-/sbin/modprobe overlay\n# 在启动containerd之前执行的命令。这里的命令是尝试加载内核的overlay模块，如果失败则忽略错误继续执行下面的命令。\n# \n# ExecStart=/usr/local/bin/containerd\n# 实际执行的命令，用于启动containerd容器运行时。\n# \n# Type=notify\n# 指定服务的通知类型。这里使用notify类型，表示当服务就绪时会通过通知的方式告知systemd。\n# \n# Delegate=yes\n# 允许systemd对此服务进行重启和停止操作。\n# \n# KillMode=process\n# 在终止容器运行时时使用的kill模式。这里使用process模式，表示通过终止进程来停止容器运行时。\n# \n# Restart=always\n# 定义了当容器运行时终止后的重启策略。这里设置为always，表示无论何时终止容器运行时，都会自动重新启动。\n# \n# RestartSec=5\n# 在容器运行时终止后重新启动之前等待的秒数。\n# \n# LimitNPROC=infinity\n# 指定容器运行时可以使用的最大进程数量。这里设置为无限制。\n# \n# LimitCORE=infinity\n# 指定容器运行时可以使用的最大CPU核心数量。这里设置为无限制。\n# \n# LimitNOFILE=infinity\n# 指定容器运行时可以打开的最大文件数。这里设置为无限制。\n# \n# TasksMax=infinity\n# 指定容器运行时可以创建的最大任务数。这里设置为无限制。\n# \n# OOMScoreAdjust=-999\n# 指定容器运行时的OOM（Out-Of-Memory）分数调整值。负数值表示容器运行时的优先级较高。\n# \n# [Install]\n# WantedBy=multi-user.target\n# 定义了服务的安装位置。这里指定为multi-user.target，表示将服务安装为多用户模式下的启动项。\n```\n\n### 2.1.1配置Containerd所需的模块\n\n```shell\ncat <<EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\n# 参数解释：\n#\n# containerd是一个容器运行时，用于管理和运行容器。它支持多种不同的参数配置来自定义容器运行时的行为和功能。\n# \n# 1. overlay：overlay是容器d默认使用的存储驱动，它提供了一种轻量级的、可堆叠的、逐层增量的文件系统。它通过在现有文件系统上叠加文件系统层来创建容器的文件系统视图。每个容器可以有自己的一组文件系统层，这些层可以共享基础镜像中的文件，并在容器内部进行修改。使用overlay可以有效地使用磁盘空间，并使容器更加轻量级。\n# \n# 2. br_netfilter：br_netfilter是Linux内核提供的一个网络过滤器模块，用于在容器网络中进行网络过滤和NAT转发。当容器和主机之间的网络通信需要进行DNAT或者SNAT时，br_netfilter模块可以将IP地址进行转换。它还可以提供基于iptables规则的网络过滤功能，用于限制容器之间或容器与外部网络之间的通信。\n# \n# 这些参数可以在containerd的配置文件或者命令行中指定。例如，可以通过设置--storage-driver参数来选择使用overlay作为存储驱动，通过设置--iptables参数来启用或禁用br_netfilter模块。具体的使用方法和配置细节可以参考containerd的官方文档。\n```\n\n### 2.1.2加载模块\n\n```shell\nsystemctl restart systemd-modules-load.service\n\n# 参数解释：\n# - `systemctl`: 是Linux系统管理服务的命令行工具，可以管理systemd init系统。\n# - `restart`: 是systemctl命令的一个选项，用于重新启动服务。\n# - `systemd-modules-load.service`: 是一个系统服务，用于加载内核模块。\n# \n# 将上述参数结合在一起来解释`systemctl restart systemd-modules-load.service`的含义：\n# 这个命令用于重新启动系统服务`systemd-modules-load.service`，它是负责加载内核模块的服务。在重新启动该服务后，系统会重新加载所有的内核模块。\n```\n\n### 2.1.3配置Containerd所需的内核\n\n```shell\ncat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\n# 加载内核\nsysctl --system\n\n# 参数解释：\n# \n# 这些参数是Linux操作系统中用于网络和网络桥接设置的参数。\n# \n# - net.bridge.bridge-nf-call-iptables：这个参数控制网络桥接设备是否调用iptables规则处理网络数据包。当该参数设置为1时，网络数据包将被传递到iptables进行处理；当该参数设置为0时，网络数据包将绕过iptables直接传递。默认情况下，这个参数的值是1，即启用iptables规则处理网络数据包。\n# \n# - net.ipv4.ip_forward：这个参数用于控制是否启用IP转发功能。IP转发使得操作系统可以将接收到的数据包从一个网络接口转发到另一个网络接口。当该参数设置为1时，启用IP转发功能；当该参数设置为0时，禁用IP转发功能。在网络环境中，通常需要启用IP转发功能来实现不同网络之间的通信。默认情况下，这个参数的值是0，即禁用IP转发功能。\n# \n# - net.bridge.bridge-nf-call-ip6tables：这个参数与net.bridge.bridge-nf-call-iptables类似，但是它用于IPv6数据包的处理。当该参数设置为1时，IPv6数据包将被传递到ip6tables进行处理；当该参数设置为0时，IPv6数据包将绕过ip6tables直接传递。默认情况下，这个参数的值是1，即启用ip6tables规则处理IPv6数据包。\n# \n# 这些参数的值可以通过修改操作系统的配置文件（通常是'/etc/sysctl.conf'）来进行设置。修改完成后，需要使用'sysctl -p'命令重载配置文件使参数生效。\n```\n\n### 2.1.4创建Containerd的配置文件\n\n```shell\n# 参数解释：\n# \n# 这段代码是用于修改并配置containerd的参数。\n# \n# 1. 首先使用命令`mkdir -p /etc/containerd`创建/etc/containerd目录，如果该目录已存在，则不进行任何操作。\n# 2. 使用命令`containerd config default | tee /etc/containerd/config.toml`创建默认配置文件，并将输出同时传递给/etc/containerd/config.toml文件。\n# 3. 使用sed命令修改/etc/containerd/config.toml文件，将SystemdCgroup参数的值从false改为true。-i参数表示直接在原文件中进行编辑。\n# 4. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中SystemdCgroup参数的值是否已修改为true。\n# 5. 使用sed命令修改/etc/containerd/config.toml文件，将registry.k8s.io的地址替换为m.daocloud.io/registry.k8s.io。-i参数表示直接在原文件中进行编辑。\n# 6. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中sandbox_image参数的值是否已修改为m.daocloud.io/registry.k8s.io。\n# 7. 使用sed命令修改/etc/containerd/config.toml文件，将config_path参数的值从\"\"改为\"/etc/containerd/certs.d\"。-i参数表示直接在原文件中进行编辑。\n# 8. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中certs.d参数的值是否已修改为/etc/containerd/certs.d。\n# 9. 使用mkdir命令创建/etc/containerd/certs.d/docker.io目录，如果目录已存在，则不进行任何操作。-p参数表示创建目录时，如果父级目录不存在，则自动创建父级目录。\n# \n# 最后，使用cat重定向操作符将内容写入/etc/containerd/certs.d/docker.io/hosts.toml文件。该文件会配置加速器，其中server参数设置为\"https://docker.io\"，host参数设置为\"https://hub-mirror.c.163.com\"，并添加capabilities参数。\n\n# 创建默认配置文件\nmkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml\n\n# 修改Containerd的配置文件\nsed -i \"s#SystemdCgroup\\ \\=\\ false#SystemdCgroup\\ \\=\\ true#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep SystemdCgroup\nsed -i \"s#registry.k8s.io#m.daocloud.io/registry.k8s.io#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep sandbox_image\nsed -i \"s#config_path\\ \\=\\ \\\"\\\"#config_path\\ \\=\\ \\\"/etc/containerd/certs.d\\\"#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep certs.d\n\n# 配置加速器\nmkdir /etc/containerd/certs.d/docker.io -pv\ncat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF\nserver = \"https://docker.io\"\n[host.\"https://docker.mirrors.ustc.edu.cn\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n\n# 注意！\n# SystemdCgroup参数是containerd中的一个配置参数，用于设置containerd在运行过程中使用的Cgroup（控制组）路径。Containerd使用SystemdCgroup参数来指定应该使用哪个Cgroup来跟踪和管理容器的资源使用。\n# \n# Cgroup是Linux内核提供的一种资源隔离和管理机制，可以用于限制、分配和监控进程组的资源使用。使用Cgroup，可以将容器的资源限制和隔离，以防止容器之间的资源争用和不公平的竞争。\n# \n# 通过设置SystemdCgroup参数，可以确保containerd能够找到正确的Cgroup路径，并正确地限制和隔离容器的资源使用，确保容器可以按照预期的方式运行。如果未正确设置SystemdCgroup参数，可能会导致容器无法正确地使用资源，或者无法保证资源的公平分配和隔离。\n# \n# 总而言之，SystemdCgroup参数的作用是为了确保containerd能够正确地管理容器的资源使用，以实现资源的限制、隔离和公平分配。\n```\n\n### 2.1.5启动并设置为开机启动\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now containerd.service\n# 启用并立即启动docker.service单元。docker.service是Docker守护进程的systemd服务单元。\n\nsystemctl stop containerd.service\n# 停止运行中的docker.service单元，即停止Docker守护进程。\n\nsystemctl start containerd.service\n# 启动docker.service单元，即启动Docker守护进程。\n\nsystemctl restart containerd.service\n# 重启docker.service单元，即重新启动Docker守护进程。\n\nsystemctl status containerd.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 2.1.6配置crictl客户端连接的运行时位置\n\n```shell\n# https://github.com/kubernetes-sigs/cri-tools/releases/\n# wget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.29.0/crictl-v1.29.0-linux-amd64.tar.gz\n\n#解压\ntar xf crictl-v*-linux-amd64.tar.gz -C /usr/bin/\n#生成配置文件\ncat > /etc/crictl.yaml <<EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n\n#测试\nsystemctl restart  containerd\ncrictl info\n\n# 注意！\n# 下面是参数`crictl`的详细解释\n# \n# `crictl`是一个用于与容器运行时通信的命令行工具。它是容器运行时接口（CRI）工具的一个实现，可以对容器运行时进行管理和操作。\n# \n# 1. `runtime-endpoint: unix:///run/containerd/containerd.sock`\n# 指定容器运行时的终端套接字地址。在这个例子中，指定的地址是`unix:///run/containerd/containerd.sock`，这是一个Unix域套接字地址。\n# \n# 2. `image-endpoint: unix:///run/containerd/containerd.sock`\n# 指定容器镜像服务的终端套接字地址。在这个例子中，指定的地址是`unix:///run/containerd/containerd.sock`，这是一个Unix域套接字地址。\n# \n# 3. `timeout: 10`\n# 设置与容器运行时通信的超时时间，单位是秒。在这个例子中，超时时间被设置为10秒。\n# \n# 4. `debug: false`\n# 指定是否开启调式模式。在这个例子中，调式模式被设置为关闭，即`false`。如果设置为`true`，则会输出更详细的调试信息。\n# \n# 这些参数可以根据需要进行修改，以便与容器运行时进行有效的通信和管理。\n```\n\n## 2.2 安装docker作为Runtime \n\n### 2.2.1 解压docker程序\n\n```shell\n# 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/\n# wget https://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-25.0.3.tgz\n\n#解压\ntar xf docker-*.tgz \n#拷贝二进制文件\ncp docker/* /usr/bin/\n```\n\n### 2.2.2 创建containerd的service文件\n\n```shell\n#创建containerd的service文件,并且启动\ncat >/etc/systemd/system/containerd.service <<EOF\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target local-fs.target\n\n[Service]\nExecStartPre=-/sbin/modprobe overlay\nExecStart=/usr/bin/containerd\nType=notify\nDelegate=yes\nKillMode=process\nRestart=always\nRestartSec=5\nLimitNPROC=infinity\nLimitCORE=infinity\nLimitNOFILE=1048576\nTasksMax=infinity\nOOMScoreAdjust=-999\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 参数解释：\n# \n# [Unit]\n# - Description=containerd container runtime：指定服务的描述信息。\n# - Documentation=https://containerd.io：指定服务的文档链接。\n# - After=network.target local-fs.target：指定服务的启动顺序，在网络和本地文件系统启动之后再启动该服务。\n# \n# [Service]\n# - ExecStartPre=-/sbin/modprobe overlay：在启动服务之前执行的命令，使用`-`表示忽略错误。\n# - ExecStart=/usr/bin/containerd：指定服务的启动命令。\n# - Type=notify：指定服务的类型，`notify`表示服务会在启动完成后向systemd发送通知。\n# - Delegate=yes：允许服务代理其他服务的应答，例如收到关机命令后终止其他服务。\n# - KillMode=process：指定服务终止时的行为，`process`表示终止服务进程。\n# - Restart=always：指定服务终止后是否自动重启，`always`表示总是自动重启。\n# - RestartSec=5：指定服务重启的时间间隔，单位为秒。\n# - LimitNPROC=infinity：限制服务的最大进程数，`infinity`表示没有限制。\n# - LimitCORE=infinity：限制服务的最大核心数，`infinity`表示没有限制。\n# - LimitNOFILE=1048576：限制服务的最大文件数，指定为1048576。\n# - TasksMax=infinity：限制服务的最大任务数，`infinity`表示没有限制。\n# - OOMScoreAdjust=-999：指定服务的OOM（Out of Memory）得分，负数表示降低被终止的概率。\n# \n# [Install]\n# - WantedBy=multi-user.target：指定服务的安装方式，`multi-user.target`表示该服务在多用户模式下安装。\n\n\n# 设置开机自启\nsystemctl enable --now containerd.service\n```\n\n### 2.2.3 准备docker的service文件\n\n```shell\n#准备docker的service文件\ncat > /etc/systemd/system/docker.service <<EOF\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\nOOMScoreAdjust=-500\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 参数解释：\n# \n# [Unit]\n# - Description: 描述服务的作用，这里是Docker Application Container Engine，即Docker应用容器引擎。\n# - Documentation: 提供关于此服务的文档链接，这里是Docker官方文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# \n# [Service]\n# - Type: 服务类型，这里是notify，表示服务在启动完成时发送通知。\n# - ExecStart: 命令，启动该服务时会执行的命令，这里是/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock，即启动dockerd并指定一些参数，其中-H指定dockerd的监听地址为fd://，--containerd指定containerd的sock文件位置。\n# - ExecReload: 重载命令，当接收到HUP信号时执行的命令，这里是/bin/kill -s HUP $MAINPID，即发送HUP信号给主进程ID。\n# - TimeoutSec: 服务超时时间，这里是0，表示没有超时限制。\n# - RestartSec: 重启间隔时间，这里是2秒，表示重启失败后等待2秒再重启。\n# - Restart: 重启策略，这里是always，表示总是重启。\n# - StartLimitBurst: 启动限制次数，这里是3，表示在启动失败后最多重试3次。\n# - StartLimitInterval: 启动限制时间间隔，这里是60秒，表示两次启动之间最少间隔60秒。\n# - LimitNOFILE: 文件描述符限制，这里是infinity，表示没有限制。\n# - LimitNPROC: 进程数限制，这里是infinity，表示没有限制。\n# - LimitCORE: 核心转储限制，这里是infinity，表示没有限制。\n# - TasksMax: 最大任务数，这里是infinity，表示没有限制。\n# - Delegate: 修改权限，这里是yes，表示启用权限修改。\n# - KillMode: 杀死模式，这里是process，表示杀死整个进程组。\n# - OOMScoreAdjust: 用于调整进程在系统内存紧张时的优先级调整，这里是-500，表示将OOM分数降低500。\n# \n# [Install]\n# - WantedBy: 安装目标，这里是multi-user.target，表示在多用户模式下安装。\n#      在WantedBy参数中，我们可以使用以下参数：\n#      1. multi-user.target：指定该服务应该在多用户模式下启动。\n#      2. graphical.target：指定该服务应该在图形化界面模式下启动。\n#      3. default.target：指定该服务应该在系统的默认目标（runlevel）下启动。\n#      4. rescue.target：指定该服务应该在系统救援模式下启动。\n#      5. poweroff.target：指定该服务应该在关机时启动。\n#      6. reboot.target：指定该服务应该在重启时启动。\n#      7. halt.target：指定该服务应该在停止时启动。\n#      8. shutdown.target：指定该服务应该在系统关闭时启动。\n#      这些参数可以根据需要选择一个或多个，以告知系统在何时启动该服务。\n\n```\n\n### 2.2.4 准备docker的socket文件\n\n```shell\n#准备docker的socket文件\ncat > /etc/systemd/system/docker.socket <<EOF\n[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=/var/run/docker.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n\n[Install]\nWantedBy=sockets.target\nEOF\n\n# 这是一个用于Docker API的socket配置文件，包含了以下参数：\n# \n# [Unit]\n# - Description：描述了该socket的作用，即为Docker API的socket。\n# \n# [Socket]\n# - ListenStream：指定了socket的监听地址，该socket会监听在/var/run/docker.sock上，即Docker守护程序使用的默认sock文件。\n# - SocketMode：指定了socket文件的权限模式，此处为0660，即用户和用户组有读写权限，其他用户无权限。\n# - SocketUser：指定了socket文件的所有者，此处为root用户。\n# - SocketGroup：指定了socket文件的所属用户组，此处为docker用户组。\n# \n# [Install]\n# - WantedBy：指定了该socket被启用时的目标，此处为sockets.target，表示当sockets.target启动时启用该socket。\n# \n# 该配置文件的作用是为Docker提供API访问的通道，它监听在/var/run/docker.sock上，具有root用户权限，但只接受docker用户组的成员的连接，并且其他用户无法访问。这样，只有docker用户组的成员可以通过该socket与Docker守护进程进行通信。\n\n```\n\n### 2.2.5 配置加速器\n\n```shell\n# 配置加速器\nmkdir /etc/docker/ -pv\ncat >/etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"registry-mirrors\": [\n    \"https://docker.mirrors.ustc.edu.cn\"\n  ],\n  \"max-concurrent-downloads\": 10,\n  \"log-driver\": \"json-file\",\n  \"log-level\": \"warn\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n    },\n  \"data-root\": \"/var/lib/docker\"\n}\nEOF\n\n\n# 该参数文件中包含以下参数：\n# \n# 1. exec-opts: 用于设置Docker守护进程的选项，native.cgroupdriver=systemd表示使用systemd作为Cgroup驱动程序。\n# 2. registry-mirrors: 用于指定Docker镜像的镜像注册服务器。在这里有三个镜像注册服务器：https://docker.m.daocloud.io、https://docker.mirrors.ustc.edu.cn和http://hub-mirror.c.163.com。\n# 3. max-concurrent-downloads: 用于设置同时下载镜像的最大数量，默认值为3，这里设置为10。\n# 4. log-driver: 用于设置Docker守护进程的日志驱动程序，这里设置为json-file。\n# 5. log-level: 用于设置日志的级别，这里设置为warn。\n# 6. log-opts: 用于设置日志驱动程序的选项，这里有两个选项：max-size和max-file。max-size表示每个日志文件的最大大小，这里设置为10m，max-file表示保存的最大日志文件数量，这里设置为3。\n# 7. data-root: 用于设置Docker守护进程的数据存储根目录，默认为/var/lib/docker，这里设置为/var/lib/docker。\n\n```\n\n### 2.2.6 启动docker\n\n```shell\ngroupadd docker\n#创建docker组\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now docker.socket\n# 启用并立即启动docker.socket单元。docker.socket是一个systemd的socket单元，用于接收来自网络的Docker API请求。\n\nsystemctl enable --now docker.service\n# 启用并立即启动docker.service单元。docker.service是Docker守护进程的systemd服务单元。\n\nsystemctl stop docker.service\n# 停止运行中的docker.service单元，即停止Docker守护进程。\n\nsystemctl start docker.service\n# 启动docker.service单元，即启动Docker守护进程。\n\nsystemctl restart docker.service\n# 重启docker.service单元，即重新启动Docker守护进程。\n\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n\ndocker info\n#验证\n```\n\n### 2.2.7 解压cri-docker\n\n```shell\n# 由于1.24以及更高版本不支持docker所以安装cri-docker\n# 下载cri-docker \n# https://github.com/Mirantis/cri-dockerd/releases/\n# wget  https://mirrors.chenby.cn/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd-0.3.10.amd64.tgz\n\n# 解压cri-docker\ntar xvf cri-dockerd-*.amd64.tgz \ncp -r cri-dockerd/  /usr/bin/\nchmod +x /usr/bin/cri-dockerd/cri-dockerd\n```\n\n### 2.2.8 写入启动cri-docker配置文件\n\n```shell\n# 写入启动配置文件\ncat >  /usr/lib/systemd/system/cri-docker.service <<EOF\n[Unit]\nDescription=CRI Interface for Docker Application Container Engine\nDocumentation=https://docs.mirantis.com\nAfter=network-online.target firewalld.service docker.service\nWants=network-online.target\nRequires=cri-docker.socket\n\n[Service]\nType=notify\nExecStart=/usr/bin/cri-dockerd/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n# [Unit]\n# - Description：该参数用于描述该单元的功能，这里描述的是CRI与Docker应用容器引擎的接口。\n# - Documentation：该参数指定了相关文档的网址，供用户参考。\n# - After：该参数指定了此单元应该在哪些其他单元之后启动，确保在网络在线、防火墙和Docker服务启动之后再启动此单元。\n# - Wants：该参数指定了此单元希望也启动的所有单元，此处是希望在网络在线之后启动。\n# - Requires：该参数指定了此单元需要依赖的单元，此处是cri-docker.socket单元。\n# \n# [Service]\n# - Type：该参数指定了服务的类型，这里是notify，表示当服务启动完成时向系统发送通知。\n# - ExecStart：该参数指定了将要运行的命令和参数，此处是执行/usr/bin/cri-dockerd/cri-dockerd命令，并指定了网络插件为cni和Pod基础设施容器的镜像为registry.aliyuncs.com/google_containers/pause:3.7。\n# - ExecReload：该参数指定在服务重载时运行的命令，此处是发送HUP信号给主进程。\n# - TimeoutSec：该参数指定了服务启动的超时时间，此处为0，表示无限制。\n# - RestartSec：该参数指定了自动重启服务的时间间隔，此处为2秒。\n# - Restart：该参数指定了在服务发生错误时自动重启，此处是始终重启。\n# - StartLimitBurst：该参数指定了在给定时间间隔内允许的启动失败次数，此处为3次。\n# - StartLimitInterval：该参数指定启动失败的时间间隔，此处为60秒。\n# - LimitNOFILE：该参数指定了允许打开文件的最大数量，此处为无限制。\n# - LimitNPROC：该参数指定了允许同时运行的最大进程数，此处为无限制。\n# - LimitCORE：该参数指定了允许生成的core文件的最大大小，此处为无限制。\n# - TasksMax：该参数指定了此服务的最大任务数，此处为无限制。\n# - Delegate：该参数指定了是否将控制权委托给指定服务，此处为是。\n# - KillMode：该参数指定了在终止服务时如何处理进程，此处是通过终止进程来终止服务。\n# \n# [Install]\n# - WantedBy：该参数指定了希望这个单元启动的多用户目标。在这里，这个单元希望在multi-user.target启动。\n```\n\n### 2.2.9 写入cri-docker的socket配置文件\n\n```shell\n# 写入socket配置文件\ncat > /usr/lib/systemd/system/cri-docker.socket <<EOF\n[Unit]\nDescription=CRI Docker Socket for the API\nPartOf=cri-docker.service\n\n[Socket]\nListenStream=%t/cri-dockerd.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n\n[Install]\nWantedBy=sockets.target\nEOF\n\n\n# 该配置文件是用于systemd的单元配置文件(unit file)，用于定义一个socket单元。\n# \n# [Unit]\n# - Description：表示该单元的描述信息。\n# - PartOf：表示该单元是cri-docker.service的一部分。\n# \n# [Socket]\n# - ListenStream：指定了该socket要监听的地址和端口，这里使用了%t占位符，表示根据单元的类型来决定路径。%t/cri-dockerd.sock表示将监听Unix域套接字cri-dockerd.sock。Unix域套接字用于在同一台主机上的进程之间通信。\n# - SocketMode：指定了socket文件的权限模式，此处为0660，即用户和用户组有读写权限，其他用户无权限。\n# - SocketUser：指定了socket文件的所有者，此处为root用户。\n# - SocketGroup：指定了socket文件的所属用户组，此处为docker用户组。\n# \n# [Install]\n# - WantedBy：部分定义了该单元的安装配置信息。WantedBy=sockets.target表示当sockets.target单元启动时，自动启动该socket单元。sockets.target是一个系统服务，用于管理所有的socket单元。\n```\n\n### 2.2.10 启动cri-docker\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now cri-docker.service\n# 启用并立即启动cri-docker.service单元。cri-docker.service是cri-docker守护进程的systemd服务单元。\n\nsystemctl restart cri-docker.service\n# 重启cri-docker.service单元，即重新启动cri-docker守护进程。\n\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n\n\n## 2.3.k8s与etcd下载及安装（仅在master01操作）\n\n### 2.3.1解压k8s安装包\n\n```shell\n# 下载安装包\n# wget https://mirrors.chenby.cn/https://github.com/etcd-io/etcd/releases/download/v3.5.12/etcd-v3.5.12-linux-amd64.tar.gz\n# wget https://dl.k8s.io/v1.29.2/kubernetes-server-linux-amd64.tar.gz\n\n# 解压k8s安装文件\ncd cby\ntar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}\n\n# 这是一个tar命令，用于解压指定的kubernetes-server-linux-amd64.tar.gz文件，并将其中的特定文件提取到/usr/local/bin目录下。\n# \n# 命令的解释如下：\n# - tar：用于处理tar压缩文件的命令。\n# - -xf：表示解压操作。\n# - kubernetes-server-linux-amd64.tar.gz：要解压的文件名。\n# - --strip-components=3：表示解压时忽略压缩文件中的前3级目录结构，提取文件时直接放到目标目录中。\n# - -C /usr/local/bin：指定提取文件的目标目录为/usr/local/bin。\n# - kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}：要解压和提取的文件名模式，用花括号括起来表示模式中的多个可能的文件名。\n# \n# 总的来说，这个命令的作用是将kubernetes-server-linux-amd64.tar.gz文件中的kubelet、kubectl、kube-apiserver、kube-controller-manager、kube-scheduler和kube-proxy六个文件提取到/usr/local/bin目录下，同时忽略文件路径中的前三级目录结构。\n\n\n# 解压etcd安装文件\ntar -xf etcd*.tar.gz && mv etcd-*/etcd /usr/local/bin/ && mv etcd-*/etcdctl /usr/local/bin/\n\n# 这是一个将文件解压并移动到特定目录的命令。这是一个用于 Linux 系统中的命令。\n# \n# - tar -xf etcd*.tar.gz：这个命令将解压以 etcd 开头并以.tar.gz 结尾的文件。`-xf` 是使用 `tar` 命令的选项，它表示解压文件并展开其中的内容。\n# - mv etcd-*/etcd /usr/local/bin/：这个命令将 etcd 文件移动到 /usr/local/bin 目录。`mv` 是移动命令，它将 etcd-*/etcd 路径下的 etcd 文件移动到了 /usr/local/bin 目录。\n# - mv etcd-*/etcdctl /usr/local/bin/：这个命令将 etcdctl 文件移动到 /usr/local/bin 目录，和上一条命令类似。\n# \n# 总结起来，以上命令将从名为 etcd*.tar.gz 的压缩文件中解压出 etcd 和 etcdctl 文件，并将它们移动到 /usr/local/bin 目录中。\n\n# 查看/usr/local/bin下内容\nls /usr/local/bin/\ncontainerd               crictl       etcdctl                  kube-proxy\ncontainerd-shim          critest      kube-apiserver           kube-scheduler\ncontainerd-shim-runc-v1  ctd-decoder  kube-controller-manager\ncontainerd-shim-runc-v2  ctr          kubectl\ncontainerd-stress        etcd         kubelet\n```\n\n### 2.3.2查看版本\n\n```shell\n[root@k8s-master01 ~]#  kubelet --version\nKubernetes v1.29.2\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.12\nAPI version: 3.5\n[root@k8s-master01 ~]# \n```\n\n### 2.3.3将组件发送至其他k8s节点\n\n```shell\nMaster='k8s-master02 k8s-master03'\nWork='k8s-node01 k8s-node02'\n\n# 拷贝master组件\nfor NODE in $Master; do echo $NODE; scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\n\n# 该命令是一个for循环，对于在$Master变量中的每个节点，执行以下操作：\n# \n# 1. 打印出节点的名称。\n# 2. 使用scp命令将/usr/local/bin/kubelet、kubectl、kube-apiserver、kube-controller-manager、kube-scheduler和kube-proxy文件复制到节点的/usr/local/bin/目录下。\n# 3. 使用scp命令将/usr/local/bin/etcd*文件复制到节点的/usr/local/bin/目录下。\n\n\n# 拷贝work组件\nfor NODE in $Work; do echo $NODE; scp /usr/local/bin/kube{let,-proxy} $NODE:/usr/local/bin/ ; done\n# 该命令是一个for循环，对于在$Master变量中的每个节点，执行以下操作：\n# \n# 1. 打印出节点的名称。\n# 2. 使用scp命令将/usr/local/bin/kubelet和kube-proxy文件复制到节点的/usr/local/bin/目录下。\n\n# 所有节点执行\nmkdir -p /opt/cni/bin\n```\n\n## 2.3创建证书相关文件\n\n```shell\n# 请查看Github仓库 或者进行获取已经打好的包\nhttps://github.com/cby-chen/Kubernetes/\nhttps://github.com/cby-chen/Kubernetes/tags\nhttps://github.com/cby-chen/Kubernetes/releases/download/v1.29.2/kubernetes-v1.29.2.tar\n```\n\n# 3.相关证书生成\n\n```shell\n# master01节点下载证书生成工具\n# wget \"https://mirrors.chenby.cn/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssl_1.6.4_linux_amd64\" -O /usr/local/bin/cfssl\n# wget \"https://mirrors.chenby.cn/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssljson_1.6.4_linux_amd64\" -O /usr/local/bin/cfssljson\n\n# 软件包内有\ncp cfssl_*_linux_amd64 /usr/local/bin/cfssl\ncp cfssljson_*_linux_amd64 /usr/local/bin/cfssljson\n\n# 添加执行权限\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n```\n\n## 3.1.生成etcd证书\n\n特别说明除外，以下操作在所有master节点操作\n\n### 3.1.1所有master节点创建证书存放目录\n\n```shell\nmkdir /etc/etcd/ssl -p\n```\n\n### 3.1.2master01节点生成etcd证书\n\n```shell\n# 写入生成证书所需的配置文件\ncat > ca-config.json << EOF \n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"876000h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"876000h\"\n      }\n    }\n  }\n}\nEOF\n# 这段配置文件是用于配置加密和认证签名的一些参数。\n# \n# 在这里，有两个部分：`signing`和`profiles`。\n# \n# `signing`包含了默认签名配置和配置文件。\n# 默认签名配置`default`指定了证书的过期时间为`876000h`。`876000h`表示证书有效期为100年。\n# \n# `profiles`部分定义了不同的证书配置文件。\n# 在这里，只有一个配置文件`kubernetes`。它包含了以下`usages`和过期时间`expiry`：\n# \n# 1. `signing`：用于对其他证书进行签名\n# 2. `key encipherment`：用于加密和解密传输数据\n# 3. `server auth`：用于服务器身份验证\n# 4. `client auth`：用于客户端身份验证\n# \n# 对于`kubernetes`配置文件，证书的过期时间也是`876000h`，即100年。\n\ncat > etcd-ca-csr.json  << EOF \n{\n  \"CN\": \"etcd\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"etcd\",\n      \"OU\": \"Etcd Security\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n# 这是一个用于生成证书签名请求（Certificate Signing Request，CSR）的JSON配置文件。JSON配置文件指定了生成证书签名请求所需的数据。\n# \n# - \"CN\": \"etcd\" 指定了希望生成的证书的CN字段（Common Name），即证书的主题，通常是该证书标识的实体的名称。\n# - \"key\": {} 指定了生成证书所使用的密钥的配置信息。\"algo\": \"rsa\" 指定了密钥的算法为RSA，\"size\": 2048 指定了密钥的长度为2048位。\n# - \"names\": [] 包含了生成证书时所需的实体信息。在这个例子中，只包含了一个实体，其相关信息如下：\n#   - \"C\": \"CN\" 指定了实体的国家/地区代码，这里是中国。\n#   - \"ST\": \"Beijing\" 指定了实体所在的省/州。\n#   - \"L\": \"Beijing\" 指定了实体所在的城市。\n#   - \"O\": \"etcd\" 指定了实体的组织名称。\n#   - \"OU\": \"Etcd Security\" 指定了实体所属的组织单位。\n# - \"ca\": {} 指定了生成证书时所需的CA（Certificate Authority）配置信息。\n#   - \"expiry\": \"876000h\" 指定了证书的有效期，这里是876000小时。\n# \n# 生成证书签名请求时，可以使用这个JSON配置文件作为输入，根据配置文件中的信息生成相应的CSR文件。然后，可以将CSR文件发送给CA进行签名，以获得有效的证书。\n\n# 生成etcd证书和etcd证书的key（如果你觉得以后可能会扩容，可以在ip那多写几个预留出来）\n# 若没有IPv6 可删除可保留 \n\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。etcd-ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/etcd/ssl/etcd-ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/etcd/ssl/etcd-ca路径下。\n\ncat > etcd-csr.json << EOF \n{\n  \"CN\": \"etcd\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"etcd\",\n      \"OU\": \"Etcd Security\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于生成一个证书签名请求（Certificate Signing Request，CSR）。\n# \n# 首先，\"CN\"字段指定了该证书的通用名称（Common Name），这里设为\"etcd\"。\n# \n# 接下来，\"key\"字段指定了密钥的算法（\"algo\"字段）和长度（\"size\"字段），此处使用的是RSA算法，密钥长度为2048位。\n# \n# 最后，\"names\"字段是一个数组，其中包含了一个名字对象，用于指定证书中的一些其他信息。这个名字对象包含了以下字段：\n# - \"C\"字段指定了国家代码（Country），这里设置为\"CN\"。\n# - \"ST\"字段指定了省份（State）或地区，这里设置为\"Beijing\"。\n# - \"L\"字段指定了城市（Locality），这里设置为\"Beijing\"。\n# - \"O\"字段指定了组织（Organization），这里设置为\"etcd\"。\n# - \"OU\"字段指定了组织单元（Organizational Unit），这里设置为\"Etcd Security\"。\n# \n# 这些字段将作为证书的一部分，用于标识和验证证书的使用范围和颁发者等信息。\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.31,192.168.1.32,192.168.1.33,fc00:43f4:1eea:1::10,fc00:43f4:1eea:1::20,fc00:43f4:1eea:1::30,::1 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n# 这是一条使用cfssl生成etcd证书的命令，下面是各个参数的解释：\n# \n# -ca=/etc/etcd/ssl/etcd-ca.pem：指定用于签名etcd证书的CA文件的路径。\n# -ca-key=/etc/etcd/ssl/etcd-ca-key.pem：指定用于签名etcd证书的CA私钥文件的路径。\n# -config=ca-config.json：指定CA配置文件的路径，该文件定义了证书的有效期、加密算法等设置。\n# -hostname=xxxx：指定要为etcd生成证书的主机名和IP地址列表。\n# -profile=kubernetes：指定使用的证书配置文件，该文件定义了证书的用途和扩展属性。\n# etcd-csr.json：指定etcd证书请求的JSON文件的路径，该文件包含了证书请求的详细信息。\n# | cfssljson -bare /etc/etcd/ssl/etcd：通过管道将cfssl命令的输出传递给cfssljson命令，并使用-bare参数指定输出文件的前缀路径，这里将生成etcd证书的.pem和-key.pem文件。\n# \n# 这条命令的作用是使用指定的CA证书和私钥，根据证书请求的JSON文件和配置文件生成etcd的证书文件。\n```\n\n### 3.1.3将证书复制到其他节点\n\n```shell\nMaster='k8s-master02 k8s-master03'\nfor NODE in $Master; do ssh $NODE \"mkdir -p /etc/etcd/ssl\"; for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do scp /etc/etcd/ssl/${FILE} $NODE:/etc/etcd/ssl/${FILE}; done; done\n\n# 这个命令是一个简单的for循环，在一个由`$Master`存储的主机列表中迭代执行。对于每个主机，它使用`ssh`命令登录到主机，并在远程主机上创建一个名为`/etc/etcd/ssl`的目录（如果不存在）。接下来，它使用`scp`将本地主机上`/etc/etcd/ssl`目录中的四个文件（`etcd-ca-key.pem`，`etcd-ca.pem`，`etcd-key.pem`和`etcd.pem`）复制到远程主机的`/etc/etcd/ssl`目录中。最终的结果是，远程主机上的`/etc/etcd/ssl`目录中包含与本地主机上相同的四个文件的副本。\n```\n\n## 3.2.生成k8s相关证书\n\n特别说明除外，以下操作在所有master节点操作\n\n### 3.2.1 所有k8s节点创建证书存放目录\n\n```shell\nmkdir -p /etc/kubernetes/pki\n```\n\n### 3.2.2 master01节点生成k8s证书\n\n```shell\n# 写入生成证书所需的配置文件\ncat > ca-csr.json   << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n# 这是一个用于生成 Kubernetes 相关证书的配置文件。该配置文件中包含以下信息：\n# \n# - CN：CommonName，即用于标识证书的通用名称。在此配置中，CN 设置为 \"kubernetes\"，表示该证书是用于 Kubernetes。\n# - key：用于生成证书的算法和大小。在此配置中，使用的算法是 RSA，大小是 2048 位。\n# - names：用于证书中的名称字段的详细信息。在此配置中，有以下字段信息：\n#   - C：Country，即国家。在此配置中，设置为 \"CN\"。\n#   - ST：State，即省/州。在此配置中，设置为 \"Beijing\"。\n#   - L：Locality，即城市。在此配置中，设置为 \"Beijing\"。\n#   - O：Organization，即组织。在此配置中，设置为 \"Kubernetes\"。\n#   - OU：Organization Unit，即组织单位。在此配置中，设置为 \"Kubernetes-manual\"。\n# - ca：用于证书签名的证书颁发机构（CA）的配置信息。在此配置中，设置了证书的有效期为 876000 小时。\n# \n# 这个配置文件可以用于生成 Kubernetes 相关的证书，以确保集群中的通信安全性。\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n\n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/kubernetes/pki/ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/kubernetes/pki/ca路径下。\n\ncat > apiserver-csr.json << EOF \n{\n  \"CN\": \"kube-apiserver\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n# 这是一个用于生成 Kubernetes 相关证书的配置文件。该配置文件中包含以下信息：\n# \n# - `CN` 字段指定了证书的通用名称 (Common Name)，这里设置为 \"kube-apiserver\"，表示该证书用于 Kubernetes API Server。\n# - `key` 字段指定了生成证书时所选用的加密算法和密钥长度。这里选用了 RSA 算法，密钥长度为 2048 位。\n# - `names` 字段包含了一组有关证书持有者信息的项。这里使用了以下信息：\n#   - `C` 表示国家代码 (Country)，这里设置为 \"CN\" 表示中国。\n#   - `ST` 表示州或省份 (State)，这里设置为 \"Beijing\" 表示北京市。\n#   - `L` 表示城市或地区 (Location)，这里设置为 \"Beijing\" 表示北京市。\n#   - `O` 表示组织名称 (Organization)，这里设置为 \"Kubernetes\" 表示 Kubernetes。\n#   - `OU` 表示组织单位 (Organizational Unit)，这里设置为 \"Kubernetes-manual\" 表示手动管理的 Kubernetes 集群。\n# \n# 这个配置文件可以用于生成 Kubernetes 相关的证书，以确保集群中的通信安全性。\n\n\n# 生成一个根证书 ，多写了一些IP作为预留IP，为将来添加node做准备\n# 10.96.0.1是service网段的第一个地址，需要计算，192.168.1.36为高可用vip地址\n# 若没有IPv6 可删除可保留 \n\ncfssl gencert   \\\n-ca=/etc/kubernetes/pki/ca.pem   \\\n-ca-key=/etc/kubernetes/pki/ca-key.pem   \\\n-config=ca-config.json   \\\n-hostname=10.96.0.1,192.168.1.36,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,x.oiox.cn,k.oiox.cn,l.oiox.cn,o.oiox.cn,192.168.1.31,192.168.1.32,192.168.1.33,192.168.1.34,192.168.1.35,192.168.1.36,192.168.0.37,192.168.0.38,192.168.0.39,192.168.1.70,fc00:43f4:1eea:1::10,fc00:43f4:1eea:1::20,fc00:43f4:1eea:1::30,fc00:43f4:1eea:1::40,fc00:43f4:1eea:1::50,fc00:43f4:1eea:1::60,fc00:43f4:1eea:1::70,fc00:43f4:1eea:1::80,fc00:43f4:1eea:1::90,fc00:43f4:1eea:1::100,::1   \\\n-profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n\n# 这个命令是使用cfssl工具生成Kubernetes API Server的证书。\n# \n# 命令的参数解释如下：\n# - `-ca=/etc/kubernetes/pki/ca.pem`：指定证书的颁发机构（CA）文件路径。\n# - `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定证书的颁发机构（CA）私钥文件路径。\n# - `-config=ca-config.json`：指定证书生成的配置文件路径，配置文件中包含了证书的有效期、加密算法等信息。\n# - `-hostname=10.96.0.1,192.168.1.36,127.0.0.1,fc00:43f4:1eea:1::10`：指定证书的主机名或IP地址列表。\n# - `-profile=kubernetes`：指定证书生成的配置文件中的配置文件名。\n# - `apiserver-csr.json`：API Server的证书签名请求配置文件路径。\n# - `| cfssljson -bare /etc/kubernetes/pki/apiserver`：通过管道将生成的证书输出到cfssljson工具，将其转换为PEM编码格式，并保存到 `/etc/kubernetes/pki/apiserver.pem` 和 `/etc/kubernetes/pki/apiserver-key.pem` 文件中。\n# \n# 最终，这个命令将会生成API Server的证书和私钥，并保存到指定的文件中。\n\n```\n\n### 3.2.3 生成apiserver聚合证书\n\n```shell\ncat > front-proxy-ca-csr.json  << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  },\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n\n# 这个JSON文件表示了生成一个名为\"kubernetes\"的证书的配置信息。这个证书是用来进行Kubernetes集群的身份验证和安全通信。\n# \n# 配置信息包括以下几个部分：\n# \n# 1. \"CN\": \"kubernetes\"：这表示了证书的通用名称（Common Name），也就是证书所代表的实体的名称。在这里，证书的通用名称被设置为\"kubernetes\"，表示这个证书是用来代表Kubernetes集群。\n# \n# 2. \"key\"：这是用来生成证书的密钥相关的配置。在这里，配置使用了RSA算法，并且设置了密钥的大小为2048位。\n# \n# 3. \"ca\"：这个字段指定了证书的颁发机构（Certificate Authority）相关的配置。在这里，配置指定了证书的有效期为876000小时，即100年。这意味着该证书在100年内将被视为有效，过期后需要重新生成。\n# \n# 总之，这个JSON文件中的配置信息描述了如何生成一个用于Kubernetes集群的证书，包括证书的通用名称、密钥算法和大小以及证书的有效期。\n\ncfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。front-proxy-ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/kubernetes/pki/front-proxy-ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/kubernetes/pki/front-proxy-ca路径下。\n\ncat > front-proxy-client-csr.json  << EOF \n{\n  \"CN\": \"front-proxy-client\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  }\n}\nEOF\n\n# 这是一个JSON格式的配置文件，用于描述一个名为\"front-proxy-client\"的配置。配置包括两个字段：CN和key。\n# \n# - CN（Common Name）字段表示证书的通用名称，这里为\"front-proxy-client\"。\n# - key字段描述了密钥的算法和大小。\"algo\"表示使用RSA算法，\"size\"表示密钥大小为2048位。\n# \n# 该配置文件用于生成一个SSL证书，用于在前端代理客户端进行认证和数据传输的加密。这个证书中的通用名称是\"front-proxy-client\"，使用RSA算法生成，密钥大小为2048位。\n\ncfssl gencert  \\\n-ca=/etc/kubernetes/pki/front-proxy-ca.pem   \\\n-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   \\\n-config=ca-config.json   \\\n-profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n\n# 这个命令使用cfssl工具生成一个用于Kubernetes的front-proxy-client证书。\n# \n# 主要参数解释如下：\n# - `-ca=/etc/kubernetes/pki/front-proxy-ca.pem`: 指定用于签署证书的根证书文件路径。\n# - `-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem`: 指定用于签署证书的根证书的私钥文件路径。\n# - `-config=ca-config.json`: 指定用于配置证书签署的配置文件路径。该配置文件描述了证书生成的一些规则，如加密算法和有效期等。\n# - `-profile=kubernetes`: 指定生成证书时使用的配置文件中定义的profile，其中包含了一些默认的参数。\n# - `front-proxy-client-csr.json`: 指定用于生成证书的CSR文件路径，该文件包含了证书请求的相关信息。\n# - `| cfssljson -bare /etc/kubernetes/pki/front-proxy-client`: 通过管道将生成的证书输出到cfssljson工具进行解析，并通过`-bare`参数将证书和私钥分别保存到指定路径。\n# \n# 这个命令的作用是根据提供的CSR文件和配置信息，使用指定的根证书和私钥生成一个前端代理客户端的证书，并将证书和私钥分别保存到`/etc/kubernetes/pki/front-proxy-client.pem`和`/etc/kubernetes/pki/front-proxy-client-key.pem`文件中。\n```\n\n### 3.2.4 生成controller-manage的证书\n\n在《5.高可用配置》选择使用那种高可用方案\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```shell\ncat > manager-csr.json << EOF \n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这是一个用于生成密钥对（公钥和私钥）的JSON配置文件。下面是针对该文件中每个字段的详细解释：\n# \n# - \"CN\": 值为\"system:kube-controller-manager\"，代表通用名称（Common Name），是此密钥对的主题（subject）。\n# - \"key\": 这个字段用来定义密钥算法和大小。\n#   - \"algo\": 值为\"rsa\"，表示使用RSA算法。\n#   - \"size\": 值为2048，表示生成的密钥大小为2048位。\n# - \"names\": 这个字段用来定义密钥对的各个名称字段。\n#   - \"C\": 值为\"CN\"，表示国家（Country）名称是\"CN\"（中国）。\n#   - \"ST\": 值为\"Beijing\"，表示省/州（State/Province）名称是\"Beijing\"（北京）。\n#   - \"L\": 值为\"Beijing\"，表示城市（Locality）名称是\"Beijing\"（北京）。\n#   - \"O\": 值为\"system:kube-controller-manager\"，表示组织（Organization）名称是\"system:kube-controller-manager\"。\n#   - \"OU\": 值为\"Kubernetes-manual\"，表示组织单位（Organizational Unit）名称是\"Kubernetes-manual\"。\n# \n# 这个JSON配置文件基本上是告诉生成密钥对的工具，生成一个带有特定名称和属性的密钥对。\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n# 这是一个命令行操作，使用cfssl工具生成证书。\n# \n# 1. `cfssl gencert` 是cfssl工具的命令，用于生成证书。\n# 2. `-ca` 指定根证书的路径和文件名，这里是`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key` 指定根证书的私钥的路径和文件名，这里是`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config` 指定配置文件的路径和文件名，这里是`ca-config.json`。\n# 5. `-profile` 指定证书使用的配置文件中的配置模板，这里是`kubernetes`。\n# 6. `manager-csr.json` 是证书签发请求的配置文件，用于生成证书签发请求。\n# 7. `|` 管道操作符，将前一条命令的输出作为后一条命令的输入。\n# 8. `cfssljson -bare` 是 cfssl 工具的命令，作用是将证书签发请求的输出转换为PKCS＃1、PKCS＃8和x509 PEM文件。\n# 9. `/etc/kubernetes/pki/controller-manager` 是转换后的 PEM 文件的存储位置和文件名。\n# \n# 这个命令的作用是根据根证书和私钥、配置文件以及证书签发请求的配置文件，生成经过签发的控制器管理器证书和私钥，并将转换后的 PEM 文件保存到指定的位置。\n\n\n# 设置一个集群项\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# kubectl config set-cluster命令用于配置集群信息。\n# --certificate-authority选项指定了集群的证书颁发机构（CA）的路径，这个CA会验证kube-apiserver提供的证书是否合法。\n# --embed-certs选项用于将证书嵌入到生成的kubeconfig文件中，这样就不需要在kubeconfig文件中单独指定证书文件路径。\n# --server选项指定了kube-apiserver的地址，这里使用的是127.0.0.1:8443，表示使用本地主机上的kube-apiserver，默认端口为8443。\n# --kubeconfig选项指定了生成的kubeconfig文件的路径和名称，这里指定为/etc/kubernetes/controller-manager.kubeconfig。\n# 综上所述，kubectl config set-cluster命令的作用是在kubeconfig文件中设置集群信息，包括证书颁发机构、证书、kube-apiserver地址等。\n\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 这个命令用于配置 Kubernetes 控制器管理器的上下文信息。下面是各个参数的详细解释：\n# 1. `kubectl config set-context system:kube-controller-manager@kubernetes`: 设置上下文的名称为 `system:kube-controller-manager@kubernetes`，这是一个标识符，用于唯一标识该上下文。\n# 2. `--cluster=kubernetes`: 指定集群的名称为 `kubernetes`，这是一个现有集群的标识符，表示要管理的 Kubernetes 集群。\n# 3. `--user=system:kube-controller-manager`: 指定使用的用户身份为 `system:kube-controller-manager`。这是一个特殊的用户身份，具有控制 Kubernetes 控制器管理器的权限。\n# 4. `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`: 指定 kubeconfig 文件的路径为 `/etc/kubernetes/controller-manager.kubeconfig`。kubeconfig 文件是一个用于管理 Kubernetes 配置的文件，包含了集群、用户和上下文的相关信息。\n# 通过运行这个命令，可以将这些配置信息保存到 `/etc/kubernetes/controller-manager.kubeconfig` 文件中，以便在后续的操作中使用。\n\n\n\n  # 设置一个用户项\n  kubectl config set-credentials system:kube-controller-manager \\\n       --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n       --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n       --embed-certs=true \\\n       --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 上述命令是用于设置 Kubernetes 的 controller-manager 组件的客户端凭据。下面是每个参数的详细解释：\n# \n# - `kubectl config`: 是使用 kubectl 命令行工具的配置子命令。\n# - `set-credentials`: 是定义一个新的用户凭据配置的子命令。\n# - `system:kube-controller-manager`: 是设置用户凭据的名称，`system:` 是 Kubernetes API Server 内置的身份验证器使用的用户标识符前缀，它表示是一个系统用户，在本例中是 kube-controller-manager 组件使用的身份。\n# - `--client-certificate=/etc/kubernetes/pki/controller-manager.pem`: 指定 controller-manager.pem 客户端证书的路径。\n# - `--client-key=/etc/kubernetes/pki/controller-manager-key.pem`: 指定 controller-manager-key.pem 客户端私钥的路径。\n# - `--embed-certs=true`: 表示将证书和私钥直接嵌入到生成的 kubeconfig 文件中，而不是通过引用外部文件。\n# - `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`: 指定生成的 kubeconfig 文件的路径和文件名，即 controller-manager.kubeconfig。\n# \n# 通过运行上述命令，将根据提供的证书和私钥信息，为 kube-controller-manager 创建一个 kubeconfig 文件，以便后续使用该文件进行身份验证和访问 Kubernetes API。\n\n\n# 设置默认环境\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 这个命令是用来指定kubectl使用指定的上下文环境来执行操作。上下文环境是kubectl用来确定要连接到哪个Kubernetes集群以及使用哪个身份验证信息的配置。\n# \n# 在这个命令中，`kubectl config use-context`是用来设置当前上下文环境的命令。 `system:kube-controller-manager@kubernetes`是指定的上下文名称，它告诉kubectl要使用的Kubernetes集群和身份验证信息。 \n# `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`是用来指定使用的kubeconfig文件的路径。kubeconfig文件是存储集群连接和身份验证信息的配置文件。\n# 通过执行这个命令，kubectl将使用指定的上下文来执行后续的操作，包括部署和管理Kubernetes资源。\n```\n\n### 3.2.5 生成kube-scheduler的证书\n\n```shell\ncat > scheduler-csr.json << EOF \n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这个命令是用来创建一个叫做scheduler-csr.json的文件，并将其中的内容赋值给该文件。\n# \n# 文件内容是一个JSON格式的文本，包含了一个描述证书请求的结构。\n# \n# 具体内容如下：\n# \n# - \"CN\": \"system:kube-scheduler\"：Common Name字段，表示该证书的名称为system:kube-scheduler。\n# - \"key\": {\"algo\": \"rsa\", \"size\": 2048}：key字段指定生成证书时使用的加密算法是RSA，并且密钥的长度为2048位。\n# - \"names\": [...]：names字段定义了证书中的另外一些标识信息。\n# - \"C\": \"CN\"：Country字段，表示国家/地区为中国。\n# - \"ST\": \"Beijing\"：State字段，表示省/市为北京。\n# - \"L\": \"Beijing\"：Locality字段，表示所在城市为北京。\n# - \"O\": \"system:kube-scheduler\"：Organization字段，表示组织为system:kube-scheduler。\n# - \"OU\": \"Kubernetes-manual\"：Organizational Unit字段，表示组织单元为Kubernetes-manual。\n# \n# 而EOF是一个占位符，用于标记开始和结束的位置。在开始的EOF之后到结束的EOF之间的内容将会被写入到scheduler-csr.json文件中。\n# \n# 总体来说，这个命令用于生成一个描述kube-scheduler证书请求的JSON文件。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n# 上述命令是使用cfssl工具生成Kubernetes Scheduler的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `scheduler-csr.json`：指定Scheduler的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`scheduler-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/scheduler`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/scheduler.pem`（包含了证书）、`/etc/kubernetes/pki/scheduler-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n\n\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/scheduler.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 这段命令是用于设置 kube-scheduler 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials system:kube-scheduler`：设置 `system:kube-scheduler` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/scheduler.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 kube-scheduler 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/scheduler-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 kube-scheduler 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 该命令用于设置一个名为\"system:kube-scheduler@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=system:kube-scheduler: 指定用户的名称为\"system:kube-scheduler\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权kube-scheduler组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/scheduler.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/scheduler.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `system:kube-scheduler@kubernetes`是一个上下文名称。它指定了使用`kube-scheduler`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n### 3.2.6 生成admin的证书配置\n\n```shell\ncat > admin-csr.json << EOF \n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于创建和配置一个名为\"admin\"的Kubernetes凭证。\n# \n# 这个凭证包含以下字段：\n# \n# - \"CN\": \"admin\": 这是凭证的通用名称，表示这是一个管理员凭证。\n# - \"key\": 这是一个包含证书密钥相关信息的对象。\n#   - \"algo\": \"rsa\"：这是使用的加密算法类型，这里是RSA加密算法。\n#   - \"size\": 2048：这是密钥的大小，这里是2048位。\n# - \"names\": 这是一个包含证书名称信息的数组。\n#   - \"C\": \"CN\"：这是证书的国家/地区字段，这里是中国。\n#   - \"ST\": \"Beijing\"：这是证书的省/州字段，这里是北京。\n#   - \"L\": \"Beijing\"：这是证书的城市字段，这里是北京。\n#   - \"O\": \"system:masters\"：这是证书的组织字段，这里是system:masters，表示系统的管理员组。\n#   - \"OU\": \"Kubernetes-manual\"：这是证书的部门字段，这里是Kubernetes-manual。\n# \n# 通过这个配置文件创建的凭证将具有管理员权限，并且可以用于管理Kubernetes集群。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n# 上述命令是使用cfssl工具生成Kubernetes admin的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `admin-csr.json`：指定admin的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`admin-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/admin`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/admin.pem`（包含了证书）、`/etc/kubernetes/pki/admin-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/scheduler.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials kubernetes-admin  \\\n  --client-certificate=/etc/kubernetes/pki/admin.pem     \\\n  --client-key=/etc/kubernetes/pki/admin-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 这段命令是用于设置 kubernetes-admin 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials kubernetes-admin`：设置 `kubernetes-admin` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/admin.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 admin 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/admin-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 admin 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\n\nkubectl config set-context kubernetes-admin@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kubernetes-admin     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 该命令用于设置一个名为\"kubernetes-admin@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=kubernetes-admin: 指定用户的名称为\"kubernetes-admin\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权admin组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/admin.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/admin.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\n\nkubectl config use-context kubernetes-admin@kubernetes  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `kubernetes-admin@kubernetes`是一个上下文名称。它指定了使用`kubernetes-admin`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n### 3.2.7 创建kube-proxy证书\n\n在《5.高可用配置》选择使用那种高可用方案\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```shell\ncat > kube-proxy-csr.json  << EOF \n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-proxy\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于创建和配置一个名为\"kube-proxy-csr\"的Kubernetes凭证。\n# \n# 这个凭证包含以下字段：\n# \n# - \"CN\": \"system:kube-proxy\": 这是凭证的通用名称，表示这是一个管理员凭证。\n# - \"key\": 这是一个包含证书密钥相关信息的对象。\n#   - \"algo\": \"rsa\"：这是使用的加密算法类型，这里是RSA加密算法。\n#   - \"size\": 2048：这是密钥的大小，这里是2048位。\n# - \"names\": 这是一个包含证书名称信息的数组。\n#   - \"C\": \"CN\"：这是证书的国家/地区字段，这里是中国。\n#   - \"ST\": \"Beijing\"：这是证书的省/州字段，这里是北京。\n#   - \"L\": \"Beijing\"：这是证书的城市字段，这里是北京。\n#   - \"O\": \"system:kube-proxy\"：这是证书的组织字段，这里是system:kube-proxy。\n#   - \"OU\": \"Kubernetes-manual\"：这是证书的部门字段，这里是Kubernetes-manual。\n# \n# 通过这个配置文件创建的凭证将具有管理员权限，并且可以用于管理Kubernetes集群。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n# 上述命令是使用cfssl工具生成Kubernetes admin的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `kube-proxy-csr.json`：指定admin的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`kube-proxy-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/kube-proxy`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/kube-proxy.pem`（包含了证书）、`/etc/kubernetes/pki/kube-proxy-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n   \n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/kube-proxy.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials kube-proxy  \\\n  --client-certificate=/etc/kubernetes/pki/kube-proxy.pem     \\\n  --client-key=/etc/kubernetes/pki/kube-proxy-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 这段命令是用于设置 kube-proxy 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials kube-proxy`：设置 `kube-proxy` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/kube-proxy.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 kube-proxy 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/kube-proxy-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 kube-proxy 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\nkubectl config set-context kube-proxy@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kube-proxy     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 该命令用于设置一个名为\"kube-proxy@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=kube-proxy: 指定用户的名称为\"kube-proxy\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权kube-proxy组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/kube-proxy.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\nkubectl config use-context kube-proxy@kubernetes  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `kube-proxy@kubernetes`是一个上下文名称。它指定了使用`kube-proxy`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n\n\n### 3.2.8 创建ServiceAccount Key ——secret\n\n```shell\nopenssl genrsa -out /etc/kubernetes/pki/sa.key 2048\nopenssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n\n# 这两个命令是使用OpenSSL工具生成RSA密钥对。\n# \n# 命令1：openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n# 该命令用于生成私钥文件。具体解释如下：\n# - openssl：openssl命令行工具。\n# - genrsa：生成RSA密钥对。\n# - -out /etc/kubernetes/pki/sa.key：指定输出私钥文件的路径和文件名。\n# - 2048：指定密钥长度为2048位。\n# \n# 命令2：openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n# 该命令用于从私钥中导出公钥。具体解释如下：\n# - openssl：openssl命令行工具。\n# - rsa：与私钥相关的RSA操作。\n# - -in /etc/kubernetes/pki/sa.key：指定输入私钥文件的路径和文件名。\n# - -pubout：指定输出公钥。\n# - -out /etc/kubernetes/pki/sa.pub：指定输出公钥文件的路径和文件名。\n# \n# 总结：通过以上两个命令，我们可以使用OpenSSL工具生成一个RSA密钥对，并将私钥保存在/etc/kubernetes/pki/sa.key文件中，将公钥保存在/etc/kubernetes/pki/sa.pub文件中。\n```\n\n### 3.2.9 将证书发送到其他master节点\n\n```shell\n#其他节点创建目录\n# mkdir  /etc/kubernetes/pki/ -p\n\nfor NODE in k8s-master02 k8s-master03; do  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do  scp /etc/kubernetes/pki/${FILE} $NODE:/etc/kubernetes/pki/${FILE}; done;  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do  scp /etc/kubernetes/${FILE} $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n### 3.2.10 查看证书\n\n```shell\nls /etc/kubernetes/pki/\nadmin.csr          controller-manager.csr      kube-proxy.csr\nadmin-key.pem      controller-manager-key.pem  kube-proxy-key.pem\nadmin.pem          controller-manager.pem      kube-proxy.pem\napiserver.csr      front-proxy-ca.csr          sa.key\napiserver-key.pem  front-proxy-ca-key.pem      sa.pub\napiserver.pem      front-proxy-ca.pem          scheduler.csr\nca.csr             front-proxy-client.csr      scheduler-key.pem\nca-key.pem         front-proxy-client-key.pem  scheduler.pem\nca.pem             front-proxy-client.pem\n\n# 一共26个就对了\nls /etc/kubernetes/pki/ |wc -l\n26\n```\n\n# 4.k8s系统组件配置\n\n## 4.1.etcd配置\n\n```shell\n这个配置文件是用于 etcd 集群的配置，其中包含了一些重要的参数和选项：\n\n- `name`：指定了当前节点的名称，用于集群中区分不同的节点。\n- `data-dir`：指定了 etcd 数据的存储目录。\n- `wal-dir`：指定了 etcd 数据写入磁盘的目录。\n- `snapshot-count`：指定了触发快照的事务数量。\n- `heartbeat-interval`：指定了 etcd 集群中节点之间的心跳间隔。\n- `election-timeout`：指定了选举超时时间。\n- `quota-backend-bytes`：指定了存储的限额，0 表示无限制。\n- `listen-peer-urls`：指定了节点之间通信的 URL，使用 HTTPS 协议。\n- `listen-client-urls`：指定了客户端访问 etcd 集群的 URL，同时提供了本地访问的 URL。\n- `max-snapshots`：指定了快照保留的数量。\n- `max-wals`：指定了日志保留的数量。\n- `initial-advertise-peer-urls`：指定了节点之间通信的初始 URL。\n- `advertise-client-urls`：指定了客户端访问 etcd 集群的初始 URL。\n- `discovery`：定义了 etcd 集群发现相关的选项。\n- `initial-cluster`：指定了 etcd 集群的初始成员。\n- `initial-cluster-token`：指定了集群的 token。\n- `initial-cluster-state`：指定了集群的初始状态。\n- `strict-reconfig-check`：指定了严格的重新配置检查选项。\n- `enable-v2`：启用了 v2 API。\n- `enable-pprof`：启用了性能分析。\n- `proxy`：设置了代理模式。\n- `client-transport-security`：客户端的传输安全配置。\n- `peer-transport-security`：节点之间的传输安全配置。\n- `debug`：是否启用调试模式。\n- `log-package-levels`：日志的输出级别。\n- `log-outputs`：指定了日志的输出类型。\n- `force-new-cluster`：是否强制创建一个新的集群。\n\n这些参数和选项可以根据实际需求进行调整和配置。\n```\n\n### 4.1.1master01配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master01'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.31:2380'\nlisten-client-urls: 'https://192.168.1.31:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.31:2380'\nadvertise-client-urls: 'https://192.168.1.31:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n### 4.1.2master02配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master02'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.32:2380'\nlisten-client-urls: 'https://192.168.1.32:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.32:2380'\nadvertise-client-urls: 'https://192.168.1.32:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n### 4.1.3master03配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master03'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.33:2380'\nlisten-client-urls: 'https://192.168.1.33:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.33:2380'\nadvertise-client-urls: 'https://192.168.1.33:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n## 4.2.创建service（所有master节点操作）\n\n### 4.2.1创建etcd.service并启动\n\n```shell\ncat > /usr/lib/systemd/system/etcd.service << EOF\n\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n\nEOF\n# 这是一个系统服务配置文件，用于启动和管理Etcd服务。\n# \n# [Unit] 部分包含了服务的一些基本信息，它定义了服务的描述和文档链接，并指定了服务应在网络连接之后启动。\n# \n# [Service] 部分定义了服务的具体配置。在这里，服务的类型被设置为notify，意味着当服务成功启动时，它将通知系统。ExecStart 指定了启动服务时要执行的命令，这里是运行 /usr/local/bin/etcd 命令并传递一个配置文件 /etc/etcd/etcd.config.yml。Restart 设置为 on-failure，意味着当服务失败时将自动重启，并且在10秒后进行重启。LimitNOFILE 指定了服务的最大文件打开数。\n# \n# [Install] 部分定义了服务的安装配置。WantedBy 指定了服务应该被启动的目标，这里是 multi-user.target，表示在系统进入多用户模式时启动。Alias 定义了一个别名，可以通过etcd3.service来引用这个服务。\n# \n# 这个配置文件描述了如何启动和管理Etcd服务，并将其安装到系统中。通过这个配置文件，可以确保Etcd服务在系统启动后自动启动，并在出现问题时进行重启。\n```\n\n### 4.2.2创建etcd证书目录\n\n```shell\nmkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now etcd.service\n# 启用并立即启动etcd.service单元。etcd.service是etcd守护进程的systemd服务单元。\n\nsystemctl restart etcd.service\n# 重启etcd.service单元，即重新启动etcd守护进程。\n\nsystemctl status etcd.service\n# etcd.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 4.2.3查看etcd状态\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\nexport ETCDCTL_API=3\netcdctl --endpoints=\"192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| 192.168.1.33:2379 |  8065f2e59c8d68c |  3.5.12 |   20 kB |     false |      false |         4 |         14 |                 14 |        |\n| 192.168.1.32:2379 | b7b7ad6bf4db3f28 |  3.5.12 |   20 kB |      true |      false |         4 |         14 |                 14 |        |\n| 192.168.1.31:2379 | bf047bcfe3b9bf27 |  3.5.12 |   20 kB |     false |      false |         4 |         14 |                 14 |        |\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n\n# 这个命令是使用etcdctl工具，用于查看指定etcd集群的健康状态。下面是每个参数的详细解释：\n# \n# - `--endpoints`：指定要连接的etcd集群节点的地址和端口。在这个例子中，指定了3个节点的地址和端口，分别是`192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379`。\n# - `--cacert`：指定用于验证etcd服务器证书的CA证书的路径。在这个例子中，指定了CA证书的路径为`/etc/kubernetes/pki/etcd/etcd-ca.pem`。CA证书用于验证etcd服务器证书的有效性。\n# - `--cert`：指定用于与etcd服务器进行通信的客户端证书的路径。在这个例子中，指定了客户端证书的路径为`/etc/kubernetes/pki/etcd/etcd.pem`。客户端证书用于在与etcd服务器建立安全通信时进行身份验证。\n# - `--key`：指定与客户端证书配对的私钥的路径。在这个例子中，指定了私钥的路径为`/etc/kubernetes/pki/etcd/etcd-key.pem`。私钥用于对通信进行加密解密和签名验证。\n# - `endpoint status`：子命令，用于检查etcd集群节点的健康状态。\n# - `--write-out`：指定输出的格式。在这个例子中，指定以表格形式输出。\n# \n# 通过执行这个命令，可以获取到etcd集群节点的健康状态，并以表格形式展示。\n```\n\n# 5.高可用配置（在Master服务器上操作）\n\n**注意* 5.1.1 和5.1.2 二选一即可**\n\n选择使用那种高可用方案，同时可以俩种都选用，实现内外兼顾的效果，比如：\n5.1 的 NGINX方案实现集群内的高可用\n5.2 的 haproxy、keepalived 方案实现集群外访问\n\n在《3.2.生成k8s相关证书》\n\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n\n## 5.1 NGINX高可用方案 \n### 5.1.1 进行编译\n```shell\n# 安装编译环境\nyum install gcc -y\n\n# 下载解压nginx二进制文件\n# wget http://nginx.org/download/nginx-1.25.3.tar.gz\ntar xvf nginx-*.tar.gz\ncd nginx-*\n\n# 进行编译\n./configure --with-stream --without-http --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module\nmake && make install \n\n# 拷贝编译好的nginx\nnode='k8s-master02 k8s-master03 k8s-node01 k8s-node02'\nfor NODE in $node; do scp -r /usr/local/nginx/ $NODE:/usr/local/nginx/; done\n\n# 这是一系列命令行指令，用于编译和安装软件。\n# \n# 1. `./configure` 是用于配置软件的命令。在这个例子中，配置的软件是一个Web服务器，指定了一些选项来启用流模块，并禁用了HTTP、uwsgi、scgi和fastcgi模块。\n# 2. `--with-stream` 指定启用流模块。流模块通常用于代理TCP和UDP流量。\n# 3. `--without-http` 指定禁用HTTP模块。这意味着编译的软件将没有HTTP服务器功能。\n# 4. `--without-http_uwsgi_module` 指定禁用uwsgi模块。uwsgi是一种Web服务器和应用服务器之间的通信协议。\n# 5. `--without-http_scgi_module` 指定禁用scgi模块。scgi是一种用于将Web服务器请求传递到应用服务器的协议。\n# 6. `--without-http_fastcgi_module` 指定禁用fastcgi模块。fastcgi是一种用于在Web服务器和应用服务器之间交换数据的协议。\n# 7. `make` 是用于编译软件的命令。该命令将根据之前的配置生成可执行文件。\n# 8. `make install` 用于安装软件。该命令将生成的可执行文件和其他必要文件复制到系统的适当位置，以便可以使用该软件。\n# \n# 总之，这个命令序列用于编译一个配置了特定选项的Web服务器，并将其安装到系统中。\n```\n\n### 5.1.2 写入启动配置\n在所有主机上执行\n```shell\n# 写入nginx配置文件\ncat > /usr/local/nginx/conf/kube-nginx.conf <<EOF\nworker_processes 1;\nevents {\n    worker_connections  1024;\n}\nstream {\n    upstream backend {\n    \tleast_conn;\n        hash $remote_addr consistent;\n        server 192.168.1.31:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.32:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.33:6443        max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 127.0.0.1:8443;\n        proxy_connect_timeout 1s;\n        proxy_pass backend;\n    }\n}\nEOF\n# 这段配置是一个nginx的stream模块的配置，用于代理TCP和UDP流量。\n# \n# 首先，`worker_processes 1;`表示启动一个worker进程用于处理流量。\n# 接下来，`events { worker_connections 1024; }`表示每个worker进程可以同时处理最多1024个连接。\n# 在stream块里面，定义了一个名为`backend`的upstream，用于负载均衡和故障转移。\n# `least_conn`表示使用最少连接算法进行负载均衡。\n# `hash $remote_addr consistent`表示用客户端的IP地址进行哈希分配请求，保持相同IP的请求始终访问同一台服务器。\n# `server`指令用于定义后端的服务器，每个服务器都有一个IP地址和端口号，以及一些可选的参数。\n# `max_fails=3`表示当一个服务器连续失败3次时将其标记为不可用。\n# `fail_timeout=30s`表示如果一个服务器被标记为不可用，nginx将在30秒后重新尝试。\n# 在server块内部，定义了一个监听地址为127.0.0.1:8443的服务器。\n# `proxy_connect_timeout 1s`表示与后端服务器建立连接的超时时间为1秒。\n# `proxy_pass backend`表示将流量代理到名为backend的上游服务器组。\n# \n# 总结起来，这段配置将流量代理到一个包含3个后端服务器的上游服务器组中，使用最少连接算法进行负载均衡，并根据客户端的IP地址进行哈希分配请求。如果一个服务器连续失败3次，则将其标记为不可用，并在30秒后重新尝试。\n\n\n# 写入启动配置文件\ncat > /etc/systemd/system/kube-nginx.service <<EOF\n[Unit]\nDescription=kube-apiserver nginx proxy\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=forking\nExecStartPre=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -t\nExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx\nExecReload=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -s reload\nPrivateTmp=true\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nLimitNOFILE=65536\n \n[Install]\nWantedBy=multi-user.target\nEOF\n# 这是一个用于kube-apiserver的NGINX代理的systemd单位文件。\n# \n# [Unit]部分包含了单位的描述和依赖关系。它指定了在network.target和network-online.target之后启动，并且需要network-online.target。\n# \n# [Service]部分定义了如何运行该服务。Type指定了服务进程的类型（forking表示主进程会派生一个子进程）。ExecStartPre指定了在服务启动之前需要运行的命令，用于检查NGINX配置文件的语法是否正确。ExecStart指定了启动服务所需的命令。ExecReload指定了在重新加载配置文件时运行的命令。PrivateTmp设置为true表示将为服务创建一个私有的临时文件系统。Restart和RestartSec用于设置服务的自动重启机制。StartLimitInterval设置为0表示无需等待，可以立即重启服务。LimitNOFILE指定了服务的文件描述符的限制。\n# \n# [Install]部分指定了在哪些target下该单位应该被启用。\n# \n# 综上所述，此单位文件用于启动和管理kube-apiserver的NGINX代理服务。它通过NGINX来反向代理和负载均衡kube-apiserver的请求。该服务会在系统启动时自动启动，并具有自动重启的机制。\n\n\n# 设置开机自启\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now kube-nginx.service\n# 启用并立即启动kube-nginx.service单元。kube-nginx.service是kube-nginx守护进程的systemd服务单元。\nsystemctl restart kube-nginx.service\n# 重启kube-nginx.service单元，即重新启动kube-nginx守护进程。\nsystemctl status kube-nginx.service\n# kube-nginx.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 5.2 keepalived和haproxy 高可用方案 \n\n### 5.2.1安装keepalived和haproxy服务\n\n```shell\nsystemctl disable --now firewalld\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nyum -y install keepalived haproxy\n```\n\n### 5.2.2修改haproxy配置文件（配置文件一样）\n\n```shell\n# cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n\ncat >/etc/haproxy/haproxy.cfg<<\"EOF\"\nglobal\n maxconn 2000\n ulimit-n 16384\n log 127.0.0.1 local0 err\n stats timeout 30s\n\ndefaults\n log global\n mode http\n option httplog\n timeout connect 5000\n timeout client 50000\n timeout server 50000\n timeout http-request 15s\n timeout http-keep-alive 15s\n\n\nfrontend monitor-in\n bind *:33305\n mode http\n option httplog\n monitor-uri /monitor\n\nfrontend k8s-master\n bind 0.0.0.0:9443\n bind 127.0.0.1:9443\n mode tcp\n option tcplog\n tcp-request inspect-delay 5s\n default_backend k8s-master\n\n\nbackend k8s-master\n mode tcp\n option tcplog\n option tcp-check\n balance roundrobin\n default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n server  k8s-master01  192.168.1.31:6443 check\n server  k8s-master02  192.168.1.32:6443 check\n server  k8s-master03  192.168.1.33:6443 check\nEOF\n```\n参数\n\n```shell\n这段配置代码是指定了一个HAProxy负载均衡器的配置。下面对各部分进行详细解释：\n1. global:\n   - maxconn 2000: 设置每个进程的最大连接数为2000。\n   - ulimit-n 16384: 设置每个进程的最大文件描述符数为16384。\n   - log 127.0.0.1 local0 err: 指定日志的输出地址为本地主机的127.0.0.1，并且只记录错误级别的日志。\n   - stats timeout 30s: 设置查看负载均衡器统计信息的超时时间为30秒。\n\n2. defaults:\n   - log global: 使默认日志与global部分相同。\n   - mode http: 设定负载均衡器的工作模式为HTTP模式。\n   - option httplog: 使负载均衡器记录HTTP协议的日志。\n   - timeout connect 5000: 设置与后端服务器建立连接的超时时间为5秒。\n   - timeout client 50000: 设置与客户端的连接超时时间为50秒。\n   - timeout server 50000: 设置与后端服务器连接的超时时间为50秒。\n   - timeout http-request 15s: 设置处理HTTP请求的超时时间为15秒。\n   - timeout http-keep-alive 15s: 设置保持HTTP连接的超时时间为15秒。\n\n3. frontend monitor-in:\n   - bind *:33305: 监听所有IP地址的33305端口。\n   - mode http: 设定frontend的工作模式为HTTP模式。\n   - option httplog: 记录HTTP协议的日志。\n   - monitor-uri /monitor: 设置监控URI为/monitor。\n\n4. frontend k8s-master:\n   - bind 0.0.0.0:9443: 监听所有IP地址的9443端口。\n   - bind 127.0.0.1:9443: 监听本地主机的9443端口。\n   - mode tcp: 设定frontend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - tcp-request inspect-delay 5s: 设置在接收到请求后延迟5秒进行检查。\n   - default_backend k8s-master: 设置默认的后端服务器组为k8s-master。\n\n5. backend k8s-master:\n   - mode tcp: 设定backend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - option tcp-check: 启用TCP检查功能。\n   - balance roundrobin: 使用轮询算法进行负载均衡。\n   - default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100: 设置默认的服务器参数。\n   - server k8s-master01 192.168.1.31:6443 check: 增加一个名为k8s-master01的服务器，IP地址为192.168.1.31，端口号为6443，并对其进行健康检查。\n   - server k8s-master02 192.168.1.32:6443 check: 增加一个名为k8s-master02的服务器，IP地址为192.168.1.32，端口号为6443，并对其进行健康检查。\n   - server k8s-master03 192.168.1.33:6443 check: 增加一个名为k8s-master03的服务器，IP地址为192.168.1.33，端口号为6443，并对其进行健康检查。\n\n以上就是这段配置代码的详细解释。它主要定义了全局配置、默认配置、前端监听和后端服务器组的相关参数和设置。通过这些配置，可以实现负载均衡和监控功能。\n```\n\n### 5.2.3Master01配置keepalived master节点\n\n```shell\n#cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n}\nvrrp_instance VI_1 {\n    state MASTER\n    # 注意网卡名\n    interface eth0 \n    mcast_src_ip 192.168.1.31\n    virtual_router_id 51\n    priority 100\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n\n### 5.2.4Master02配置keepalived backup节点\n\n```shell\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.32\n    virtual_router_id 51\n    priority 80\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n\n### 5.2.5Master03配置keepalived backup节点\n\n```shell\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.33\n    virtual_router_id 51\n    priority 50\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n参数\n\n```shell\n这是一个用于配置keepalived的配置文件。下面是对每个部分的详细解释：\n\n- `global_defs`部分定义了全局参数。\n- `router_id`参数指定了当前路由器的标识，这里设置为\"LVS_DEVEL\"。\n\n- `vrrp_script`部分定义了一个VRRP脚本。`chk_apiserver`是脚本的名称，\n    - `script`参数指定了脚本的路径。该脚本每5秒执行一次，返回值为0表示服务正常，返回值为1表示服务异常。\n    - `weight`参数指定了根据脚本返回的值来调整优先级，这里设置为-5。\n    - `fall`参数指定了失败阈值，当连续2次脚本返回值为1时认为服务异常。\n    - `rise`参数指定了恢复阈值，当连续1次脚本返回值为0时认为服务恢复正常。\n\n- `vrrp_instance`部分定义了一个VRRP实例。`VI_1`是实例的名称。\n    - `state`参数指定了当前实例的状态，这里设置为MASTER表示当前实例是主节点。\n    - `interface`参数指定了要监听的网卡，这里设置为eth0。\n    - `mcast_src_ip`参数指定了VRRP报文的源IP地址，这里设置为192.168.1.31。\n    - `virtual_router_id`参数指定了虚拟路由器的ID，这里设置为51。\n    - `priority`参数指定了实例的优先级，优先级越高（数值越大）越有可能被选为主节点。\n    - `nopreempt`参数指定了当主节点失效后不要抢占身份，即不要自动切换为主节点。\n    - `advert_int`参数指定了发送广播的间隔时间，这里设置为2秒。\n    - `authentication`部分指定了认证参数\n    \t- `auth_type`参数指定了认证类型，这里设置为PASS表示使用密码认证，\n    \t- `auth_pass`参数指定了认证密码，这里设置为K8SHA_KA_AUTH。\n    - `virtual_ipaddress`部分指定了虚拟IP地址，这里设置为192.168.1.36。\n    - `track_script`部分指定了要跟踪的脚本，这里跟踪了chk_apiserver脚本。\n```\n\n\n### 5.2.6健康检查脚本配置（lb主机）\n\n```shell\ncat >  /etc/keepalived/check_apiserver.sh << EOF\n#!/bin/bash\n\nerr=0\nfor k in \\$(seq 1 3)\ndo\n    check_code=\\$(pgrep haproxy)\n    if [[ \\$check_code == \"\" ]]; then\n        err=\\$(expr \\$err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ \\$err != \"0\" ]]; then\n    echo \"systemctl stop keepalived\"\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\nEOF\n\n# 给脚本授权\n\nchmod +x /etc/keepalived/check_apiserver.sh\n\n# 这段脚本是一个简单的bash脚本，主要用来检查是否有名为haproxy的进程正在运行。\n# \n# 脚本的主要逻辑如下：\n# 1. 首先设置一个变量err为0，用来记录错误次数。\n# 2. 使用一个循环，在循环内部执行以下操作：\n#    a. 使用pgrep命令检查是否有名为haproxy的进程在运行。如果不存在该进程，将err加1，并暂停1秒钟，然后继续下一次循环。\n#    b. 如果存在haproxy进程，将err重置为0，并跳出循环。\n# 3. 检查err的值，如果不为0，表示检查失败，输出一条错误信息并执行“systemctl stop keepalived”命令停止keepalived进程，并退出脚本返回1。\n# 4. 如果err的值为0，表示检查成功，退出脚本返回0。\n# \n# 该脚本的主要作用是检查是否存在运行中的haproxy进程，如果无法检测到haproxy进程，将停止keepalived进程并返回错误状态。如果haproxy进程存在，则返回成功状态。这个脚本可能是作为一个健康检查脚本的一部分，在确保haproxy服务可用的情况下，才继续运行其他操作。\n```\n\n### 5.2.7启动服务\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now haproxy.service\n# 启用并立即启动haproxy.service单元。haproxy.service是haproxy守护进程的systemd服务单元。\nsystemctl enable --now keepalived.service\n# 启用并立即启动keepalived.service单元。keepalived.service是keepalived守护进程的systemd服务单元。\nsystemctl status haproxy.service\n# haproxy.service单元的当前状态，包括运行状态、是否启用等信息。\nsystemctl status keepalived.service\n# keepalived.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 5.2.8测试高可用\n\n```shell\n# 能ping同\n[root@k8s-node02 ~]# ping 192.168.1.36\n\n# 能telnet访问\n[root@k8s-node02 ~]# telnet 192.168.1.36 9443\n\n# 关闭主节点，看vip是否漂移到备节点\n```\n\n# 6.k8s组件配置\n\n所有k8s节点创建以下目录\n\n```shell\nmkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes\n```\n\n## 6.1.创建apiserver（所有master节点）\n\n### 6.1.1master01节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.31 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n### 6.1.2master02节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.32 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n### 6.1.3master03节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service  << EOF\n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.33 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n该配置文件是用于定义Kubernetes API Server的systemd服务的配置。systemd是一个用于启动和管理Linux系统服务的守护进程。\n\n[Unit]\n- Description: 服务的描述信息，用于显示在日志和系统管理工具中。\n- Documentation: 提供关于服务的文档链接。\n- After: 规定服务依赖于哪些其他服务或单元。在这个例子中，API Server服务在网络目标启动之后启动。\n\n[Service]\n- ExecStart: 定义服务的命令行参数和命令。这里指定了API Server的启动命令，包括各种参数选项。\n- Restart: 指定当服务退出时应该如何重新启动。在这个例子中，服务在失败时将被重新启动。\n- RestartSec: 指定两次重新启动之间的等待时间。\n- LimitNOFILE: 指定进程可以打开的文件描述符的最大数量。\n\n[Install]\n- WantedBy: 指定服务应该安装到哪个系统目标。在这个例子中，服务将被安装到multi-user.target目标，以便在多用户模式下启动。\n\n上述配置文件中定义的kube-apiserver服务将以指定的参数运行，这些参数包括：\n\n- `--v=2` 指定日志级别为2，打印详细的API Server日志。\n- `--allow-privileged=true` 允许特权容器运行。\n- `--bind-address=0.0.0.0` 绑定API Server监听的IP地址。\n- `--secure-port=6443` 指定API Server监听的安全端口。\n- `--advertise-address=192.168.1.31` 广告API Server的地址。\n- `--service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112` 指定服务CIDR范围。\n- `--service-node-port-range=30000-32767` 指定NodePort的范围。\n- `--etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379` 指定etcd服务器的地址。\n- `--etcd-cafile` 指定etcd服务器的CA证书。\n- `--etcd-certfile` 指定etcd服务器的证书。\n- `--etcd-keyfile` 指定etcd服务器的私钥。\n- `--client-ca-file` 指定客户端CA证书。\n- `--tls-cert-file` 指定服务的证书。\n- `--tls-private-key-file` 指定服务的私钥。\n- `--kubelet-client-certificate` 和 `--kubelet-client-key` 指定与kubelet通信的客户端证书和私钥。\n- `--service-account-key-file` 指定服务账户公钥文件。\n- `--service-account-signing-key-file` 指定服务账户签名密钥文件。\n- `--service-account-issuer` 指定服务账户的发布者。\n- `--kubelet-preferred-address-types` 指定kubelet通信时的首选地址类型。\n- `--enable-admission-plugins` 启用一系列准入插件。\n- `--authorization-mode` 指定授权模式。\n- `--enable-bootstrap-token-auth` 启用引导令牌认证。\n- `--requestheader-client-ca-file` 指定请求头中的客户端CA证书。\n- `--proxy-client-cert-file` 和 `--proxy-client-key-file` 指定代理客户端的证书和私钥。\n- `--requestheader-allowed-names` 指定请求头中允许的名字。\n- `--requestheader-group-headers` 指定请求头中的组头。\n- `--requestheader-extra-headers-prefix` 指定请求头中的额外头前缀。\n- `--requestheader-username-headers` 指定请求头中的用户名头。\n- `--enable-aggregator-routing` 启用聚合路由。\n\n整个配置文件为Kubernetes API Server提供了必要的参数，以便正确地启动和运行。\n```\n\n\n### 6.1.4启动apiserver（所有master节点）\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-apiserver.service\n# 启用并立即启动kube-apiserver.service单元。kube-apiserver.service是kube-apiserver守护进程的systemd服务单元。\n\nsystemctl restart kube-apiserver.service\n# 重启kube-apiserver.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-apiserver.service\n# kube-apiserver.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 6.2.配置kube-controller-manager service\n```shell\n# 所有master节点配置，且配置相同\n# 172.16.0.0/12为pod网段，按需求设置你自己的网段\n\ncat > /usr/lib/systemd/system/kube-controller-manager.service << EOF\n\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\\\n      --leader-elect=true \\\\\n      --use-service-account-credentials=true \\\\\n      --node-monitor-grace-period=40s \\\\\n      --node-monitor-period=5s \\\\\n      --controllers=*,bootstrapsigner,tokencleaner \\\\\n      --allocate-node-cidrs=true \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112 \\\\\n      --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n      --node-cidr-mask-size-ipv4=24 \\\\\n      --node-cidr-mask-size-ipv6=120 \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n这是一个用于启动 Kubernetes 控制器管理器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 控制器管理器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-controller-manager，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--root-ca-file：根证书文件的路径，用于验证其他组件的证书。\n--cluster-signing-cert-file：用于签名集群证书的证书文件路径。\n--cluster-signing-key-file：用于签名集群证书的私钥文件路径。\n--service-account-private-key-file：用于签名服务账户令牌的私钥文件路径。\n--kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个控制器管理器作为 leader 在运行。\n--use-service-account-credentials=true：使用服务账户的凭据进行认证和授权。\n--node-monitor-grace-period=40s：节点监控的优雅退出时间，节点长时间不响应时会触发节点驱逐。\n--node-monitor-period=5s：节点监控的检测周期，用于检测节点是否正常运行。\n--controllers：指定要运行的控制器类型，在这里使用了通配符 *，表示运行所有的控制器，同时还包括了 bootstrapsigner 和 tokencleaner 控制器。\n--allocate-node-cidrs=true：为节点分配 CIDR 子网，用于分配 Pod 网络地址。\n--service-cluster-ip-range：定义 Service 的 IP 范围，这里设置为 10.96.0.0/12 和 fd00::/108。\n--cluster-cidr：定义集群的 CIDR 范围，这里设置为 172.16.0.0/12 和 fc00::/48。\n--node-cidr-mask-size-ipv4：分配给每个节点的 IPv4 子网掩码大小，默认是 24。\n--node-cidr-mask-size-ipv6：分配给每个节点的 IPv6 子网掩码大小，默认是 120。\n--requestheader-client-ca-file：设置请求头中客户端 CA 的证书文件路径，用于认证请求头中的 CA 证书。\n\n这个服务单元文件描述了 Kubernetes 控制器管理器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 控制器管理器组件。\n```\n### 6.2.1启动kube-controller-manager，并查看状态\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-controller-manager.service\n# 启用并立即启动kube-controller-manager.service单元。kube-controller-manager.service是kube-controller-manager守护进程的systemd服务单元。\n\nsystemctl restart kube-controller-manager.service\n# 重启kube-controller-manager.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-controller-manager.service\n# kube-controller-manager.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 6.3.配置kube-scheduler service\n\n### 6.3.1所有master节点配置，且配置相同\n\n```shell\ncat > /usr/lib/systemd/system/kube-scheduler.service << EOF\n\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --leader-elect=true \\\\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n这是一个用于启动 Kubernetes 调度器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 调度器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-scheduler，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个调度器作为 leader 在运行。\n--kubeconfig=/etc/kubernetes/scheduler.kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n\n这个服务单元文件描述了 Kubernetes 调度器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 调度器组件。\n```\n### 6.3.2启动并查看服务状态\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-scheduler.service\n# 启用并立即启动kube-scheduler.service单元。kube-scheduler.service是kube-scheduler守护进程的systemd服务单元。\n\nsystemctl restart kube-scheduler.service\n# 重启kube-scheduler.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-scheduler.service\n# kube-scheduler.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 7.TLS Bootstrapping配置\n\n## 7.1在master01上配置\n\n```shell\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:8443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\ncd bootstrap\n\nkubectl config set-cluster kubernetes     \\\n--certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n--embed-certs=true     --server=https://127.0.0.1:8443     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置 Kubernetes 集群配置的命令示例。下面是对每个选项的详细解释：\n# \n# config set-cluster kubernetes：指定要设置的集群名称为 \"kubernetes\"，表示要修改名为 \"kubernetes\" 的集群配置。\n# --certificate-authority=/etc/kubernetes/pki/ca.pem：指定证书颁发机构（CA）的证书文件路径，用于验证服务器证书的有效性。\n# --embed-certs=true：将证书文件嵌入到生成的 kubeconfig 文件中。这样可以避免在 kubeconfig 文件中引用外部证书文件。\n# --server=https://127.0.0.1:8443：指定 Kubernetes API 服务器的地址和端口，这里使用的是 https 协议和本地地址（127.0.0.1），端口号为 8443。你可以根据实际环境修改该参数。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"kubernetes\" 的集群配置，并提供 CA 证书、API 服务器地址和端口，并将这些配置信息嵌入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-credentials tls-bootstrap-token-user     \\\n--token=c8ad9c.2e4d610cf3e7426e \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置凭证信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-credentials tls-bootstrap-token-user：指定要设置的凭证名称为 \"tls-bootstrap-token-user\"，表示要修改名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --token=c8ad9c.2e4d610cf3e7426e：指定用户的身份验证令牌（token）。在这个示例中，令牌是 c8ad9c.2e4d610cf3e7426e。你可以根据实际情况修改该令牌。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user\" 的用户凭证，并将令牌信息加入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-context tls-bootstrap-token-user@kubernetes     \\\n--cluster=kubernetes     \\\n--user=tls-bootstrap-token-user     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置上下文信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-context tls-bootstrap-token-user@kubernetes：指定要设置的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要修改名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文配置。\n# --cluster=kubernetes：指定上下文关联的集群名称为 \"kubernetes\"，表示使用名为 \"kubernetes\" 的集群配置。\n# --user=tls-bootstrap-token-user：指定上下文关联的用户凭证名称为 \"tls-bootstrap-token-user\"，表示使用名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文，并将其关联到名为 \"kubernetes\" 的集群配置和名为 \"tls-bootstrap-token-user\" 的用户凭证配置。这样，bootstrap-kubelet.kubeconfig 文件就包含了完整的上下文信息，可以用于指定与 Kubernetes 集群建立连接时要使用的集群和凭证。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config use-context tls-bootstrap-token-user@kubernetes     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置当前上下文的命令示例。下面是对每个选项的详细解释：\n# \n# config use-context tls-bootstrap-token-user@kubernetes：指定要使用的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要将当前上下文切换为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以将当前上下文设置为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。这样，当你执行其他 kubectl 命令时，它们将使用该上下文与 Kubernetes 集群进行交互。请确保路径和文件名与实际环境中的配置相匹配。\n\n\n# token的位置在bootstrap.secret.yaml，如果修改的话到这个文件修改\nmkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n```\n\n## 7.2查看集群状态，没问题的话继续后续操作\n\n```shell\n# 1.28 版本只能查看到一个etcd 属于正常现象\n# export ETCDCTL_API=3\n# etcdctl --endpoints=\"192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n\nkubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\nscheduler            Healthy   ok        \ncontroller-manager   Healthy   ok        \netcd-0               Healthy   ok \n\n# 切记执行，别忘记！！！\nkubectl create -f bootstrap.secret.yaml\n```\n\n# 8.node节点配置\n\n## 8.1.在master01上将证书复制到node节点\n\n```shell\ncd /etc/kubernetes/\n \nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $NODE mkdir -p /etc/kubernetes/pki; for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig kube-proxy.kubeconfig; do scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n## 8.2.kubelet配置\n\n**注意 ： 8.2.1 和 8.2.2 需要和 上方 2.1 和 2.2 对应起来**\n\n### 8.2.1当使用docker作为Runtime\n\n```shell\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node= \n\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。下面是对每个节（[Unit]、[Service]、[Install]）的详细解释：\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数。这里使用的是 /usr/local/bin/kubelet 命令，并传递了一系列参数来配置 Kubelet 的运行。这些参数包括：\n# --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定了用于引导 kubelet 的 kubeconfig 文件的路径和名称。\n# --kubeconfig=/etc/kubernetes/kubelet.kubeconfig：指定了 kubelet 的 kubeconfig 文件的路径和名称。\n# --config=/etc/kubernetes/kubelet-conf.yml：指定了 kubelet 的配置文件的路径和名称。\n# --container-runtime-endpoint=unix:///run/cri-dockerd.sock：指定了容器运行时接口的端点地址，这里使用的是 Docker 运行时（cri-dockerd）的 UNIX 套接字。\n# --node-labels=node.kubernetes.io/node=：指定了节点的标签。这里的示例只给节点添加了一个简单的标签 node.kubernetes.io/node=。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，指定相关的配置文件和凭证文件，以及定义节点的标签。请确认路径和文件名与你的实际环境中的配置相匹配。\n\n\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=   \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 8.2.2当使用Containerd作为Runtime （推荐）\n\n```shell\nmkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n\n# 所有k8s节点配置kubelet service\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。与之前相比，添加了 After 和 Requires 字段来指定依赖关系。\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数，与之前的示例相同。\n# --container-runtime-endpoint=unix:///run/containerd/containerd.sock：修改了容器运行时接口的端点地址，将其更改为使用 containerd 运行时（通过 UNIX 套接字）。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，并指定了它依赖的 containerd 服务。确保路径和文件名与你实际环境中的配置相匹配。\n\n\n\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=  \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n\n### 8.2.3所有k8s节点创建kubelet的配置文件\n\n```shell\ncat > /etc/kubernetes/kubelet-conf.yml <<EOF\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\nEOF\n\n# 这是一个Kubelet的配置文件，用于配置Kubelet的各项参数。\n# \n# - apiVersion: kubelet.config.k8s.io/v1beta1：指定了配置文件的API版本为kubelet.config.k8s.io/v1beta1。\n# - kind: KubeletConfiguration：指定了配置类别为KubeletConfiguration。\n# - address: 0.0.0.0：指定了Kubelet监听的地址为0.0.0.0。\n# - port: 10250：指定了Kubelet监听的端口为10250。\n# - readOnlyPort: 10255：指定了只读端口为10255，用于提供只读的状态信息。\n# - authentication：指定了认证相关的配置信息。\n#   - anonymous.enabled: false：禁用了匿名认证。\n#   - webhook.enabled: true：启用了Webhook认证。\n#   - x509.clientCAFile: /etc/kubernetes/pki/ca.pem：指定了X509证书的客户端CA文件路径。\n# - authorization：指定了授权相关的配置信息。\n#   - mode: Webhook：指定了授权模式为Webhook。\n#   - webhook.cacheAuthorizedTTL: 5m0s：指定了授权缓存时间段为5分钟。\n#   - webhook.cacheUnauthorizedTTL: 30s：指定了未授权缓存时间段为30秒。\n# - cgroupDriver: systemd：指定了Cgroup驱动为systemd。\n# - cgroupsPerQOS: true：启用了每个QoS类别一个Cgroup的设置。\n# - clusterDNS: 指定了集群的DNS服务器地址列表。\n#   - 10.96.0.10：指定了DNS服务器地址为10.96.0.10。\n# - clusterDomain: cluster.local：指定了集群的域名后缀为cluster.local。\n# - containerLogMaxFiles: 5：指定了容器日志文件保留的最大数量为5个。\n# - containerLogMaxSize: 10Mi：指定了容器日志文件的最大大小为10Mi。\n# - contentType: application/vnd.kubernetes.protobuf：指定了内容类型为protobuf。\n# - cpuCFSQuota: true：启用了CPU CFS Quota。\n# - cpuManagerPolicy: none：禁用了CPU Manager。\n# - cpuManagerReconcilePeriod: 10s：指定了CPU管理器的调整周期为10秒。\n# - enableControllerAttachDetach: true：启用了控制器的挂载和拆卸。\n# - enableDebuggingHandlers: true：启用了调试处理程序。\n# - enforceNodeAllocatable: 指定了强制节点可分配资源的列表。\n#   - pods：强制节点可分配pods资源。\n# - eventBurst: 10：指定了事件突发的最大数量为10。\n# - eventRecordQPS: 5：指定了事件记录的最大请求量为5。\n# - evictionHard: 指定了驱逐硬性限制参数的配置信息。\n#   - imagefs.available: 15%：指定了镜像文件系统可用空间的限制为15%。\n#   - memory.available: 100Mi：指定了可用内存的限制为100Mi。\n#   - nodefs.available: 10%：指定了节点文件系统可用空间的限制为10%。\n#   - nodefs.inodesFree: 5%：指定了节点文件系统可用inode的限制为5%。\n# - evictionPressureTransitionPeriod: 5m0s：指定了驱逐压力转换的时间段为5分钟。\n# - failSwapOn: true：指定了在发生OOM时禁用交换分区。\n# - fileCheckFrequency: 20s：指定了文件检查频率为20秒。\n# - hairpinMode: promiscuous-bridge：设置了Hairpin Mode为\"promiscuous-bridge\"。\n# - healthzBindAddress: 127.0.0.1：指定了健康检查的绑定地址为127.0.0.1。\n# - healthzPort: 10248：指定了健康检查的端口为10248。\n# - httpCheckFrequency: 20s：指定了HTTP检查的频率为20秒。\n# - imageGCHighThresholdPercent: 85：指定了镜像垃圾回收的上阈值为85%。\n# - imageGCLowThresholdPercent: 80：指定了镜像垃圾回收的下阈值为80%。\n# - imageMinimumGCAge: 2m0s：指定了镜像垃圾回收的最小时间为2分钟。\n# - iptablesDropBit: 15：指定了iptables的Drop Bit为15。\n# - iptablesMasqueradeBit: 14：指定了iptables的Masquerade Bit为14。\n# - kubeAPIBurst: 10：指定了KubeAPI的突发请求数量为10个。\n# - kubeAPIQPS: 5：指定了KubeAPI的每秒请求频率为5个。\n# - makeIPTablesUtilChains: true：指定了是否使用iptables工具链。\n# - maxOpenFiles: 1000000：指定了最大打开文件数为1000000。\n# - maxPods: 110：指定了最大的Pod数量为110。\n# - nodeStatusUpdateFrequency: 10s：指定了节点状态更新的频率为10秒。\n# - oomScoreAdj: -999：指定了OOM Score Adjustment为-999。\n# - podPidsLimit: -1：指定了Pod的PID限制为-1，表示无限制。\n# - registryBurst: 10：指定了Registry的突发请求数量为10个。\n# - registryPullQPS: 5：指定了Registry的每秒拉取请求数量为5个。\n# - resolvConf: /etc/resolv.conf：指定了resolv.conf的文件路径。\n# - rotateCertificates: true：指定了是否轮转证书。\n# - runtimeRequestTimeout: 2m0s：指定了运行时请求的超时时间为2分钟。\n# - serializeImagePulls: true：指定了是否序列化镜像拉取。\n# - staticPodPath: /etc/kubernetes/manifests：指定了静态Pod的路径。\n# - streamingConnectionIdleTimeout: 4h0m0s：指定了流式连接的空闲超时时间为4小时。\n# - syncFrequency: 1m0s：指定了同步频率为1分钟。\n# - volumeStatsAggPeriod: 1m0s：指定了卷统计聚合周期为1分钟。\n```\n\n### 8.2.4启动kubelet\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kubelet.service\n# 启用并立即启动kubelet.service单元。kubelet.service是kubelet守护进程的systemd服务单元。\n\nsystemctl restart kubelet.service\n# 重启kubelet.service单元，即重新启动kubelet守护进程。\n\nsystemctl status kubelet.service\n# kubelet.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 8.2.5查看集群\n\n```shell\n[root@k8s-master01 ~]# kubectl  get node\nNAME           STATUS   ROLES    AGE   VERSION\nk8s-master01   Ready    <none>   16s   v1.29.2\nk8s-master02   Ready    <none>   13s   v1.29.2\nk8s-master03   Ready    <none>   12s   v1.29.2\nk8s-node01     Ready    <none>   10s   v1.29.2\nk8s-node02     Ready    <none>   9s    v1.29.2\n[root@k8s-master01 ~]#\n```\n\n### 8.2.6查看容器运行时\n\n```shell\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n\n```\n\n\n## 8.3.kube-proxy配置\n\n### 8.3.1将kubeconfig发送至其他节点\n\n```shell\n# master-1执行\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig; done\n```\n\n### 8.3.2所有k8s节点添加kube-proxy的service文件\n\n```shell\ncat >  /usr/lib/systemd/system/kube-proxy.service << EOF\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\\\n  --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n  --v=2\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n\n# 这是一个 systemd 服务单元文件的示例，用于配置 Kubernetes Kube Proxy 服务。下面是对其中一些字段的详细解释：\n# \n# [Unit]\n# \n# Description: 描述了该服务单元的用途，这里是 Kubernetes Kube Proxy。\n# Documentation: 指定了该服务单元的文档地址，即 https://github.com/kubernetes/kubernetes。\n# After: 指定该服务单元应在 network.target（网络目标）之后启动。\n# [Service]\n# \n# ExecStart: 指定了启动 Kube Proxy 服务的命令。通过 /usr/local/bin/kube-proxy 命令启动，并指定了配置文件的路径为 /etc/kubernetes/kube-proxy.yaml，同时指定了日志级别为 2。\n# Restart: 配置了服务在失败或退出后自动重启。\n# RestartSec: 配置了重启间隔，这里是每次重启之间的等待时间为 10 秒。\n# [Install]\n# \n# WantedBy: 指定了该服务单元的安装目标为 multi-user.target（多用户目标），表示该服务将在多用户模式下启动。\n# 通过配置这些字段，你可以启动和管理 Kubernetes Kube Proxy 服务。请注意，你需要根据实际情况修改 ExecStart 中的路径和文件名，确保与你的环境一致。另外，可以根据需求修改其他字段的值，以满足你的特定要求。\n```\n\n### 8.3.3所有k8s节点添加kube-proxy的配置\n```shell\ncat > /etc/kubernetes/kube-proxy.yaml << EOF\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: \"\"\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/12,fc00:2222::/112\nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: \"\"\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: \"rr\"\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: \"ipvs\"\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: \"\"\nudpIdleTimeout: 250ms\nEOF\n\n# 这是一个Kubernetes的kube-proxy组件配置文件示例。以下是每个配置项的详细解释：\n# \n# 1. apiVersion: kubeproxy.config.k8s.io/v1alpha1\n#    - 指定该配置文件的API版本。\n# \n# 2. bindAddress: 0.0.0.0\n#    - 指定kube-proxy使用的监听地址。0.0.0.0表示监听所有网络接口。\n# \n# 3. clientConnection:\n#    - 客户端连接配置项。\n# \n#    a. acceptContentTypes: \"\"\n#       - 指定接受的内容类型。\n# \n#    b. burst: 10\n#       - 客户端请求超出qps设置时的最大突发请求数。\n# \n#    c. contentType: application/vnd.kubernetes.protobuf\n#       - 指定客户端请求的内容类型。\n# \n#    d. kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n#       - kube-proxy使用的kubeconfig文件路径。\n# \n#    e. qps: 5\n#       - 每秒向API服务器发送的请求数量。\n# \n# 4. clusterCIDR: 172.16.0.0/12,fc00:2222::/112\n#    - 指定集群使用的CIDR范围，用于自动分配Pod IP。\n# \n# 5. configSyncPeriod: 15m0s\n#    - 指定kube-proxy配置同步到节点的频率。\n# \n# 6. conntrack:\n#    - 连接跟踪设置。\n# \n#    a. max: null\n#       - 指定连接跟踪的最大值。\n# \n#    b. maxPerCore: 32768\n#       - 指定每个核心的最大连接跟踪数。\n# \n#    c. min: 131072\n#       - 指定最小的连接跟踪数。\n# \n#    d. tcpCloseWaitTimeout: 1h0m0s\n#       - 指定处于CLOSE_WAIT状态的TCP连接的超时时间。\n# \n#    e. tcpEstablishedTimeout: 24h0m0s\n#       - 指定已建立的TCP连接的超时时间。\n# \n# 7. enableProfiling: false\n#    - 是否启用性能分析。\n# \n# 8. healthzBindAddress: 0.0.0.0:10256\n#    - 指定健康检查监听地址和端口。\n# \n# 9. hostnameOverride: \"\"\n#    - 指定覆盖默认主机名的值。\n# \n# 10. iptables:\n#     - iptables设置。\n# \n#     a. masqueradeAll: false\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. masqueradeBit: 14\n#        - 指定伪装的Bit标记。\n# \n#     c. minSyncPeriod: 0s\n#        - 指定同步iptables规则的最小间隔。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步iptables规则的时间间隔。\n# \n# 11. ipvs:\n#     - ipvs设置。\n# \n#     a. masqueradeAll: true\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. minSyncPeriod: 5s\n#        - 指定同步ipvs规则的最小间隔。\n# \n#     c. scheduler: \"rr\"\n#        - 指定ipvs默认使用的调度算法。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步ipvs规则的时间间隔。\n# \n# 12. kind: KubeProxyConfiguration\n#     - 指定该配置文件的类型。\n# \n# 13. metricsBindAddress: 127.0.0.1:10249\n#     - 指定指标绑定的地址和端口。\n# \n# 14. mode: \"ipvs\"\n#     - 指定kube-proxy的模式。这里指定为ipvs，使用IPVS代理模式。\n# \n# 15. nodePortAddresses: null\n#     - 指定可用于NodePort的网络地址。\n# \n# 16. oomScoreAdj: -999\n#     - 指定kube-proxy的OOM优先级。\n# \n# 17. portRange: \"\"\n#     - 指定可用于服务端口范围。\n# \n# 18. udpIdleTimeout: 250ms\n#     - 指定UDP连接的空闲超时时间。\n```\n\n### 8.3.4启动kube-proxy\n\n```shell\n systemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-proxy.service\n# 启用并立即启动kube-proxy.service单元。kube-proxy.service是kube-proxy守护进程的systemd服务单元。\n\nsystemctl restart kube-proxy.service\n# 重启kube-proxy.service单元，即重新启动kube-proxy守护进程。\n\nsystemctl status kube-proxy.service\n# kube-proxy.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 9.安装网络插件\n\n**注意 9.1 和 9.2 二选其一即可，建议在此处创建好快照后在进行操作，后续出问题可以回滚**\n\n** centos7 要升级libseccomp 不然 无法安装网络插件**\n\n```shell\n# https://github.com/opencontainers/runc/releases\n# 升级runc\n# wget https://mirrors.chenby.cn/https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\n\ninstall -m 755 runc.amd64 /usr/local/sbin/runc\ncp -p /usr/local/sbin/runc  /usr/local/bin/runc\ncp -p /usr/local/sbin/runc  /usr/bin/runc\n\n#下载高于2.4以上的包\nyum -y install http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n# 清华源\nyum -y install https://mirrors.tuna.tsinghua.edu.cn/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n\n#查看当前版本\n[root@k8s-master-1 ~]# rpm -qa | grep libseccomp\nlibseccomp-2.5.1-1.el8.x86_64\n```\n\n## 9.1安装Calico\n\n### 9.1.1更改calico网段\n\n```shell\nwget https://mirrors.chenby.cn/https://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yaml\n\ncp calico-typha.yaml calico.yaml\ncp calico-typha.yaml calico-ipv6.yaml\n\nvim calico.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n# vim calico-ipv6.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n        \"assign_ipv4\": \"true\",\n        \"assign_ipv6\": \"true\"\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: IP6\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n    - name: CALICO_IPV6POOL_CIDR\n      value: \"fc00:2222::/112\"\n\n    - name: FELIX_IPV6SUPPORT\n      value: \"true\"\n\n\n# 若docker镜像拉不下来，可以使用国内的仓库\nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico.yaml \nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico-ipv6.yaml\n\nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico.yaml \nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico-ipv6.yaml\n\n# 本地没有公网 IPv6 使用 calico.yaml\nkubectl apply -f calico.yaml\n\n# 本地有公网 IPv6 使用 calico-ipv6.yaml \n# kubectl apply -f calico-ipv6.yaml \n\n```\n\n### 9.1.2查看容器状态\n\n```shell\n# calico 初始化会很慢 需要耐心等待一下，大约十分钟左右\n[root@k8s-master01 ~]# kubectl  get pod -A\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-6747f75cdc-fbvvc   1/1     Running   0          61s\nkube-system   calico-node-fs7hl                          1/1     Running   0          61s\nkube-system   calico-node-jqz58                          1/1     Running   0          61s\nkube-system   calico-node-khjlg                          1/1     Running   0          61s\nkube-system   calico-node-wmf8q                          1/1     Running   0          61s\nkube-system   calico-node-xc6gn                          1/1     Running   0          61s\nkube-system   calico-typha-6cdc4b4fbc-57snb              1/1     Running   0          61s\n```\n\n## 9.2 安装cilium\n\n### 9.2.1 安装helm\n\n```shell\n# [root@k8s-master01 ~]# curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n# [root@k8s-master01 ~]# chmod 700 get_helm.sh\n# [root@k8s-master01 ~]# ./get_helm.sh\n\n# wget https://mirrors.huaweicloud.com/helm/v3.13.2/helm-v3.13.2-linux-amd64.tar.gz\ntar xvf helm-*-linux-amd64.tar.gz\ncp linux-amd64/helm /usr/local/bin/\n```\n\n### 9.2.2 安装cilium\n\n```shell\n# 添加源\nhelm repo add cilium https://helm.cilium.io\n\n# 修改为国内源\nhelm pull cilium/cilium\ntar xvf cilium-*.tgz\ncd cilium/\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  cilium ./cilium/ -n kube-system\n\n# 启用ipv6\n# helm install cilium cilium/cilium --namespace kube-system --set ipv6.enabled=true\n\n# 启用路由信息和监控插件\n# helm install cilium cilium/cilium --namespace kube-system --set hubble.relay.enabled=true --set hubble.ui.enabled=true --set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.enabled=true --set hubble.metrics.enabled=\"{dns,drop,tcp,flow,port-distribution,icmp,http}\" \n\n```\n\n### 9.2.3 查看\n\n```shell\n[root@k8s-master01 ~]# kubectl  get pod -A | grep cil\nkube-system   cilium-gmr6c                       1/1     Running       0             5m3s\nkube-system   cilium-kzgdj                       1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-6pw4k   1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-xzzdk   1/1     Running       0             5m3s\nkube-system   cilium-q2rnr                       1/1     Running       0             5m3s\nkube-system   cilium-smx5v                       1/1     Running       0             5m3s\nkube-system   cilium-tdjq4                       1/1     Running       0             5m3s\n[root@k8s-master01 ~]#\n```\n\n### 9.2.4 下载专属监控面板\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# wget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/1.12.1/examples/kubernetes/addons/prometheus/monitoring-example.yaml\n\n[root@k8s-master01 yaml]# sed -i \"s#docker.io/#m.daocloud.io/docker.io/#g\" monitoring-example.yaml\n\n[root@k8s-master01 yaml]# kubectl  apply -f monitoring-example.yaml\nnamespace/cilium-monitoring created\nserviceaccount/prometheus-k8s created\nconfigmap/grafana-config created\nconfigmap/grafana-cilium-dashboard created\nconfigmap/grafana-cilium-operator-dashboard created\nconfigmap/grafana-hubble-dashboard created\nconfigmap/prometheus created\nclusterrole.rbac.authorization.k8s.io/prometheus created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus created\nservice/grafana created\nservice/prometheus created\ndeployment.apps/grafana created\ndeployment.apps/prometheus created\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.5 下载部署测试用例\n\n说明 测试用例 需要在 安装CoreDNS 之后即可完成\n\n```shell\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/connectivity-check/connectivity-check.yaml\n\nsed -i \"s#google.com#baidu.cn#g\" connectivity-check.yaml\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" connectivity-check.yaml\n\nkubectl  apply -f connectivity-check.yaml\n```\n\n### 9.2.6 查看pod\n\n```shell\n[root@k8s-master01 yaml]# kubectl  get pod -A\nNAMESPACE           NAME                                                     READY   STATUS    RESTARTS      AGE\ncilium-monitoring   grafana-59957b9549-6zzqh                                 1/1     Running   0             10m\ncilium-monitoring   prometheus-7c8c9684bb-4v9cl                              1/1     Running   0             10m\ndefault             chenby-75b5d7fbfb-7zjsr                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-hbvr8                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-ppbzg                                  1/1     Running   0             27h\ndefault             echo-a-6799dff547-pnx6w                                  1/1     Running   0             10m\ndefault             echo-b-fc47b659c-4bdg9                                   1/1     Running   0             10m\ndefault             echo-b-host-67fcfd59b7-28r9s                             1/1     Running   0             10m\ndefault             host-to-b-multi-node-clusterip-69c57975d6-z4j2z          1/1     Running   0             10m\ndefault             host-to-b-multi-node-headless-865899f7bb-frrmc           1/1     Running   0             10m\ndefault             pod-to-a-allowed-cnp-5f9d7d4b9d-hcd8x                    1/1     Running   0             10m\ndefault             pod-to-a-denied-cnp-65cc5ff97b-2rzb8                     1/1     Running   0             10m\ndefault             pod-to-a-dfc64f564-p7xcn                                 1/1     Running   0             10m\ndefault             pod-to-b-intra-node-nodeport-677868746b-trk2l            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-clusterip-76bbbc677b-knfq2           1/1     Running   0             10m\ndefault             pod-to-b-multi-node-headless-698c6579fd-mmvd7            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-nodeport-5dc4b8cfd6-8dxmz            1/1     Running   0             10m\ndefault             pod-to-external-1111-8459965778-pjt9b                    1/1     Running   0             10m\ndefault             pod-to-external-fqdn-allow-google-cnp-64df9fb89b-l9l4q   1/1     Running   0             10m\nkube-system         cilium-7rfj6                                             1/1     Running   0             56s\nkube-system         cilium-d4cch                                             1/1     Running   0             56s\nkube-system         cilium-h5x8r                                             1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-flpl5                         1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-gcznc                         1/1     Running   0             56s\nkube-system         cilium-t2xlz                                             1/1     Running   0             56s\nkube-system         cilium-z65z7                                             1/1     Running   0             56s\nkube-system         coredns-665475b9f8-jkqn8                                 1/1     Running   1 (36h ago)   36h\nkube-system         hubble-relay-59d8575-9pl9z                               1/1     Running   0             56s\nkube-system         hubble-ui-64d4995d57-nsv9j                               2/2     Running   0             56s\nkube-system         metrics-server-776f58c94b-c6zgs                          1/1     Running   1 (36h ago)   37h\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.7 修改为NodePort\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# kubectl  edit svc  -n kube-system hubble-ui\nservice/hubble-ui edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring grafana\nservice/grafana edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring prometheus\nservice/prometheus edited\n[root@k8s-master01 yaml]#\n\ntype: NodePort\n```\n\n### 9.2.8 查看端口\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# kubectl get svc -A | grep monit\ncilium-monitoring   grafana                NodePort    10.100.250.17    <none>        3000:30707/TCP           15m\ncilium-monitoring   prometheus             NodePort    10.100.131.243   <none>        9090:31155/TCP           15m\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl get svc -A | grep hubble\nkube-system         hubble-metrics         ClusterIP   None             <none>        9965/TCP                 5m12s\nkube-system         hubble-peer            ClusterIP   10.100.150.29    <none>        443/TCP                  5m12s\nkube-system         hubble-relay           ClusterIP   10.109.251.34    <none>        80/TCP                   5m12s\nkube-system         hubble-ui              NodePort    10.102.253.59    <none>        80:31219/TCP             5m12s\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.9 访问\n\n安装时候没有创建 监控可以忽略\n\n```shell\nhttp://192.168.1.31:30707\nhttp://192.168.1.31:31155\nhttp://192.168.1.31:31219\n```\n\n# 10.安装CoreDNS\n\n## 10.1以下步骤只在master01操作\n\n### 10.1.1修改文件\n\n```shell\n# 下载tgz包\nhelm repo add coredns https://coredns.github.io/helm\nhelm pull coredns/coredns\ntar xvf coredns-*.tgz\ncd coredns/\n\n# 修改IP地址\nvim values.yaml\ncat values.yaml | grep clusterIP:\nclusterIP: \"10.96.0.10\"\n\n# 示例\n---\nservice:\n# clusterIP: \"\"\n# clusterIPs: []\n# loadBalancerIP: \"\"\n# externalIPs: []\n# externalTrafficPolicy: \"\"\n# ipFamilyPolicy: \"\"\n  # The name of the Service\n  # If not set, a name is generated using the fullname template\n  clusterIP: \"10.96.0.10\"\n  name: \"\"\n  annotations: {}\n---\n\n# 修改为国内源 docker源可选\nsed -i \"s#coredns/#m.daocloud.io/docker.io/coredns/#g\" values.yaml\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  coredns ./coredns/ -n kube-system\n```\n\n# 11.安装Metrics Server\n\n## 11.1以下步骤只在master01操作\n\n### 11.1.1安装Metrics-server\n\n在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率\n\n```shell\n# 下载 \nwget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# 修改配置\nvim components.yaml\n\n---\n# 1\n\t\t\t- args:\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\n        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n        - --requestheader-username-headers=X-Remote-User\n        - --requestheader-group-headers=X-Remote-Group\n        - --requestheader-extra-headers-prefix=X-Remote-Extra-\n\n# 2\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-dir\n        - name: ca-ssl\n          mountPath: /etc/kubernetes/pki\n\n# 3\n      volumes:\n      - emptyDir: {}\n        name: tmp-dir\n      - name: ca-ssl\n        hostPath:\n          path: /etc/kubernetes/pki\n---\n\n\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n\n# 执行部署\nkubectl apply -f components.yaml\n```\n\n### 11.1.2稍等片刻查看状态\n\n```shell\nkubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   268m         6%     2318Mi          60%       \nk8s-master02   147m         3%     1802Mi          47%       \nk8s-master03   147m         3%     1820Mi          47%       \nk8s-node01     62m          1%     1152Mi          30%       \nk8s-node02     63m          1%     1114Mi          29%  \n```\n\n# 12.集群验证\n\n## 12.1部署pod资源\n\n```shell\ncat<<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: docker.io/library/busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\nEOF\n\n# 查看\nkubectl  get pod\nNAME      READY   STATUS    RESTARTS   AGE\nbusybox   1/1     Running   0          17s\n```\n\n## 12.2用pod解析默认命名空间中的kubernetes\n\n```shell\n# 查看name\nkubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   17h\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup kubernetes\n3Server:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes\nAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n```\n\n## 12.3测试跨命名空间是否可以解析\n\n```shell\n# 查看有那些name\nkubectl  get svc -A\nNAMESPACE     NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndefault       kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP         76m\nkube-system   calico-typha      ClusterIP   10.105.100.82   <none>        5473/TCP        35m\nkube-system   coredns-coredns   ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP   8m14s\nkube-system   metrics-server    ClusterIP   10.105.60.31    <none>        443/TCP         109s\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup coredns-coredns.kube-system\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n\nName:      coredns-coredns.kube-system\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n[root@k8s-master01 metrics-server]# \n```\n\n## 12.4每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53\n\n```shell\ntelnet 10.96.0.1 443\nTrying 10.96.0.1...\nConnected to 10.96.0.1.\nEscape character is '^]'.\n\n telnet 10.96.0.10 53\nTrying 10.96.0.10...\nConnected to 10.96.0.10.\nEscape character is '^]'.\n\ncurl 10.96.0.10:53\ncurl: (52) Empty reply from server\n```\n\n## 12.5Pod和Pod之前要能通\n\n```shell\nkubectl get po -owide\nNAME      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nbusybox   1/1     Running   0          17m   172.27.14.193   k8s-node02   <none>           <none>\n\nkubectl get po -n kube-system -owide\nNAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\ncalico-kube-controllers-76754ff848-pw4xg   1/1     Running   0          38m     172.25.244.193   k8s-master01   <none>           <none>\ncalico-node-97m55                          1/1     Running   0          38m     192.168.1.34     k8s-node01     <none>           <none>\ncalico-node-hlz7j                          1/1     Running   0          38m     192.168.1.32     k8s-master02   <none>           <none>\ncalico-node-jtlck                          1/1     Running   0          38m     192.168.1.33     k8s-master03   <none>           <none>\ncalico-node-lxfkf                          1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncalico-node-t667x                          1/1     Running   0          38m     192.168.1.31     k8s-master01   <none>           <none>\ncalico-typha-59d75c5dd4-gbhfp              1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncoredns-coredns-c5c6d4d9b-bd829            1/1     Running   0          10m     172.25.92.65     k8s-master02   <none>           <none>\nmetrics-server-7c8b55c754-w7q8v            1/1     Running   0          3m56s   172.17.125.3     k8s-node01     <none>           <none>\n\n# 进入busybox ping其他节点上的pod\n\nkubectl exec -ti busybox -- sh\n/ # ping 192.168.1.34\nPING 192.168.1.34 (192.168.1.34): 56 data bytes\n64 bytes from 192.168.1.34: seq=0 ttl=63 time=0.358 ms\n64 bytes from 192.168.1.34: seq=1 ttl=63 time=0.668 ms\n64 bytes from 192.168.1.34: seq=2 ttl=63 time=0.637 ms\n64 bytes from 192.168.1.34: seq=3 ttl=63 time=0.624 ms\n64 bytes from 192.168.1.34: seq=4 ttl=63 time=0.907 ms\n\n# 可以连通证明这个pod是可以跨命名空间和跨主机通信的\n```\n\n## 12.6创建三个副本，可以看到3个副本分布在不同的节点上（用完可以删了）\n\n```shell\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl  get pod \nNAME                               READY   STATUS    RESTARTS   AGE\nbusybox                            1/1     Running   0          6m25s\nnginx-deployment-9456bbbf9-4bmvk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-9rcdk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-dqv8s   1/1     Running   0          8s\n\n# 删除nginx\n[root@k8s-master01 ~]# kubectl delete deployments nginx-deployment \n```\n\n# 13.安装dashboard\n\n```shell\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\nhelm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kube-system\n```\n\n## 13.1更改dashboard的svc为NodePort，如果已是请忽略\n\n```shell\nkubectl edit svc kubernetes-dashboard -n kube-system\n  type: NodePort\n```\n\n## 13.2查看端口号\n\n```shell\nkubectl get svc kubernetes-dashboard -n kube-system\nNAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.108.120.110   <none>        443:30034/TCP   34s\n```\n\n## 13.3创建token\n\n```shell\ncat > dashboard-user.yaml << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\nEOF\n\nkubectl  apply -f dashboard-user.yaml\n\n# 创建token\nkubectl -n kube-system create token admin-user\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Im5vZExpNi1tTERLb09ONVM2cEE0SWNCUnA4eTZieE81RnVGb1IwSk5QVFEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzA4MjQ4NjM4LCJpYXQiOjE3MDgyNDUwMzgsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiMTQ1YTdmZTktMTQ0YS00NDZmLWI1M2QtNDk4OGM3YjIyZjgyIn19LCJuYmYiOjE3MDgyNDUwMzgsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.H2Oxxrb5BVLH1iDOA-Uo1I7aiAUZX1wK-xBiV9NJXQ32EDyQvss95yQbCNHtPMhQZ8jFE3NRhyjkgZMZmX7kR9J-89QXLqKhE8Qnihd1mq5HOEVQ8tjZ6ix8ymxs5QkfSvd_OUzILKBtfYAMb4Fer67Dyf14oBHWVKU9LQkCdtFaLxerK--N7gLWeGXzavqzOlEPZR5UZWUPwP5dJmAQtvSToPVMaKiA49LjaGJid0F5Pxnutr80oZRsLfKr0MpoEG6jrow1QeJ2PgVksDTcqMTpye-M6jmIbuxabsRSskTT_zEDT0J86BiLYIHnh79D-P7IUUq6GOp8DgG-wXhICQ\n```\n\n## 13.3登录dashboard\n\nhttps://192.168.1.31:30034/\n\n# 14.ingress安装\n\n## 14.1执行部署\n\n```shell\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml\n\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n\ncat > backend.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: registry.cn-hangzhou.aliyuncs.com/chenby/defaultbackend-amd64:1.5 \n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\nEOF\n\nkubectl  apply -f deploy.yaml \nkubectl  apply -f backend.yaml \n\n\ncat > ingress-demo-app.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello-server\n  template:\n    metadata:\n      labels:\n        app: hello-server\n    spec:\n      containers:\n      - name: hello-server\n        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server\n        ports:\n        - containerPort: 9000\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-demo\n  template:\n    metadata:\n      labels:\n        app: nginx-demo\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  selector:\n    app: nginx-demo\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello-server\n  name: hello-server\nspec:\n  selector:\n    app: hello-server\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 9000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress  \nmetadata:\n  name: ingress-host-bar\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: \"hello.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: hello-server\n            port:\n              number: 8000\n  - host: \"demo.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/nginx\"  \n        backend:\n          service:\n            name: nginx-demo\n            port:\n              number: 8000\nEOF\n\n# 等创建完成后在执行：\nkubectl  apply -f ingress-demo-app.yaml \n\nkubectl  get ingress\nNAME               CLASS   HOSTS                            ADDRESS     PORTS   AGE\ningress-host-bar   nginx   hello.chenby.cn,demo.chenby.cn   192.168.1.32   80      7s\n\n```\n\n## 14.2过滤查看ingress端口\n\n```shell\n# 修改为nodeport\nkubectl edit svc -n ingress-nginx   ingress-nginx-controller\ntype: NodePort\n\n[root@hello ~/yaml]# kubectl  get svc -A | grep ingress\ningress-nginx          ingress-nginx-controller             NodePort    10.104.231.36    <none>        80:32636/TCP,443:30579/TCP   104s\ningress-nginx          ingress-nginx-controller-admission   ClusterIP   10.101.85.88     <none>        443/TCP                      105s\n[root@hello ~/yaml]#\n```\n\n# 15.IPv6测试\n\n\n\n```shell\n#部署应用\n\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chenby\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: chenby\n  template:\n    metadata:\n      labels:\n        app: chenby\n    spec:\n      hostNetwork: true\n      containers:\n      - name: chenby\n        image: docker.io/library/nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: chenby\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n  - IPv6\n  - IPv4\n  type: NodePort\n  selector:\n    app: chenby\n  ports:\n  - port: 80\n    targetPort: 80\nEOF\n\n\n#查看端口\n[root@k8s-master01 ~]# kubectl  get svc\nNAME           TYPE        CLUSTER-IP            EXTERNAL-IP   PORT(S)        AGE\nchenby         NodePort    fd00:1111::bc86       <none>        80:31540/TCP   5s\n[root@k8s-master01 ~]# \n\n[root@localhost yaml]# curl -I http://192.168.1.31:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:59 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n\n[root@localhost yaml]# \n\n[root@localhost yaml]# curl -I http://[2409:8a10:9e18:9020::10]:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:54 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n```\n\n# 16.安装命令行自动补全功能\n\n```shell\nyum install bash-completion -y\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\n```\n\n# 附录\n```shell\n# 镜像加速器可以使用DaoCloud仓库，替换规则如下\ncr.l5d.io/  ===> m.daocloud.io/cr.l5d.io/\ndocker.elastic.co/  ===> m.daocloud.io/docker.elastic.co/\ndocker.io/  ===> m.daocloud.io/docker.io/\ngcr.io/  ===> m.daocloud.io/gcr.io/\nghcr.io/  ===> m.daocloud.io/ghcr.io/\nk8s.gcr.io/  ===> m.daocloud.io/k8s.gcr.io/\nmcr.microsoft.com/  ===> m.daocloud.io/mcr.microsoft.com/\nnvcr.io/  ===> m.daocloud.io/nvcr.io/\nquay.io/  ===> m.daocloud.io/quay.io/\nregistry.jujucharms.com/  ===> m.daocloud.io/registry.jujucharms.com/\nregistry.k8s.io/  ===> m.daocloud.io/registry.k8s.io/\nregistry.opensource.zalan.do/  ===> m.daocloud.io/registry.opensource.zalan.do/\nrocks.canonical.com/  ===> m.daocloud.io/rocks.canonical.com/\n\n\n\n\n# 镜像版本要自行查看，因为镜像版本是随时更新的，文档无法做到实时更新\n\n# docker pull 镜像\n\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/cni:master \ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/node:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/typha:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2\ndocker pull kubernetesui/dashboard:v2.7.0\ndocker pull kubernetesui/metrics-scraper:v1.0.8\ndocker pull quay.io/cilium/cilium:v1.12.6\ndocker pull quay.io/cilium/certgen:v0.1.8\ndocker pull quay.io/cilium/hubble-relay:v1.12.6\ndocker pull quay.io/cilium/hubble-ui-backend:v0.9.2\ndocker pull quay.io/cilium/hubble-ui:v0.9.2\ndocker pull quay.io/cilium/cilium-etcd-operator:v2.0.7\ndocker pull quay.io/cilium/operator:v1.12.6\ndocker pull quay.io/cilium/clustermesh-apiserver:v1.12.6\ndocker pull quay.io/coreos/etcd:v3.5.4\ndocker pull quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26\n\n# docker 保存镜像\ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/cni:master -o cni.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/node:master -o node.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/typha:master -o typha.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master -o kube-controllers.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0 -o coredns.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6 -o pause.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2 -o metrics-server.tar \ndocker save kubernetesui/dashboard:v2.7.0 -o dashboard.tar \ndocker save kubernetesui/metrics-scraper:v1.0.8 -o metrics-scraper.tar \ndocker save quay.io/cilium/cilium:v1.12.6 -o cilium.tar \ndocker save quay.io/cilium/certgen:v0.1.8 -o certgen.tar \ndocker save quay.io/cilium/hubble-relay:v1.12.6 -o hubble-relay.tar \ndocker save quay.io/cilium/hubble-ui-backend:v0.9.2 -o hubble-ui-backend.tar \ndocker save quay.io/cilium/hubble-ui:v0.9.2 -o hubble-ui.tar \ndocker save quay.io/cilium/cilium-etcd-operator:v2.0.7 -o cilium-etcd-operator.tar \ndocker save quay.io/cilium/operator:v1.12.6 -o operator.tar \ndocker save quay.io/cilium/clustermesh-apiserver:v1.12.6 -o clustermesh-apiserver.tar \ndocker save quay.io/coreos/etcd:v3.5.4 -o etcd.tar \ndocker save quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26 -o startup-script.tar \n\n# 传输到各个节点\nfor NODE in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp -r images/  $NODE:/root/ ; done\n\n# 创建命名空间\nctr ns create k8s.io\n\n# 导入镜像\nctr --namespace k8s.io image import images/cni.tar\nctr --namespace k8s.io image import images/node.tar\nctr --namespace k8s.io image import images/typha.tar\nctr --namespace k8s.io image import images/kube-controllers.tar \nctr --namespace k8s.io image import images/coredns.tar \nctr --namespace k8s.io image import images/pause.tar \nctr --namespace k8s.io image import images/metrics-server.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/cilium.tar \nctr --namespace k8s.io image import images/certgen.tar \nctr --namespace k8s.io image import images/hubble-relay.tar \nctr --namespace k8s.io image import images/hubble-ui-backend.tar \nctr --namespace k8s.io image import images/hubble-ui.tar \nctr --namespace k8s.io image import images/cilium-etcd-operator.tar \nctr --namespace k8s.io image import images/operator.tar \nctr --namespace k8s.io image import images/clustermesh-apiserver.tar \nctr --namespace k8s.io image import images/etcd.tar \nctr --namespace k8s.io image import images/startup-script.tar \n\n# pull tar包 解压后\nhelm pull cilium/cilium\n\n# 查看镜像版本\nroot@hello:~/cilium# cat values.yaml| grep tag: -C1\n  repository: \"quay.io/cilium/cilium\"\n  tag: \"v1.12.6\"\n  pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/certgen\"\n    tag: \"v0.1.8@sha256:4a456552a5f192992a6edcec2febb1c54870d665173a33dc7d876129b199ddbd\"\n    pullPolicy: \"IfNotPresent\"\n--\n      repository: \"quay.io/cilium/hubble-relay\"\n      tag: \"v1.12.6\"\n       # hubble-relay-digest\n--\n        repository: \"quay.io/cilium/hubble-ui-backend\"\n        tag: \"v0.9.2@sha256:a3ac4d5b87889c9f7cc6323e86d3126b0d382933bd64f44382a92778b0cde5d7\"\n        pullPolicy: \"IfNotPresent\"\n--\n        repository: \"quay.io/cilium/hubble-ui\"\n        tag: \"v0.9.2@sha256:d3596efc94a41c6b772b9afe6fe47c17417658956e04c3e2a28d293f2670663e\"\n        pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium-etcd-operator\"\n    tag: \"v2.0.7@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/operator\"\n    tag: \"v1.12.6\"\n    # operator-generic-digest\n--\n    repository: \"quay.io/cilium/startup-script\"\n    tag: \"d69851597ea019af980891a4628fb36b7880ec26\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium\"\n    tag: \"v1.12.6\"\n    # cilium-digest\n--\n      repository: \"quay.io/cilium/clustermesh-apiserver\"\n      tag: \"v1.12.6\"\n      # clustermesh-apiserver-digest\n--\n        repository: \"quay.io/coreos/etcd\"\n        tag: \"v3.5.4@sha256:795d8660c48c439a7c3764c2330ed9222ab5db5bb524d8d0607cac76f7ba82a3\"\n        pullPolicy: \"IfNotPresent\"\n\n```\n","tags":["kubernetes"],"categories":["云原生"]},{"title":"kubernetes集群升级ipv4ipv6双栈","url":"/2024/04/25/kubernetes集群升级ipv4ipv6双栈/","content":"\n## kubernetes集群升级ipv4/ipv6双栈\n\n### 前提条件\n\n- 节点配置ipv4/ipv6地址\n- k8s集群<=1.23\n- 安装calico插件>=3.23\n\n#### 地址规划\n\n| 网络         | ipv4          | ipv6       |\n| ------------ | ------------- | ---------- |\n| pod-cidr     | 10.244.0.0/16 | 2000::/112 |\n| service-cidr | 10.96.0.0/12  | 3000::/112 |\n\n`注意：ipv6掩码位需要>=108，否则kube-apiserver会报错`\n\n### 网卡同时配置ipv4/ipv6\n\n```\nvim /etc/sysconfig/network-scripts/ifcfg-XXX\n```\n\n```\nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=none\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=yes\nIPV6_ADDR_GEN_MODE=stable-privacy\nIPV6ADDR=2031:0:130c::90         # 改为实际的ipv6地址\nIPV6_DEFAULTGW=2031:0:130c::1    # 改为实际的ipv6网关地址\nIPV6_DEFROUTE=yes\nDEVICE=ens192\nONBOOT=yes\nIPADDR=10.20.13.90\nPREFIX=24\nGATEWAY=10.20.13.1\nDNS1=218.2.2.2\nDNS2=2001:4860:4860::8888        # ipv6 dns 不用动\nIPV6_PRIVACY=no\n```\n\n```\nsystemctl restart network\n```\n\n#### 测试ipv4\n\n```\nping 10.20.13.90\n```\n\n#### 测试ipv6\n\n```\nping -6 2031:0:130c::90\n```\n\n\n\n### 节点开启ipv6（所有节点）\n\n```\nvim /etc/sysctl.d/k8s.conf\n```\n\n```\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv6.conf.all.disable_ipv6 = 0\nnet.ipv6.conf.default.disable_ipv6 = 0\nnet.ipv6.conf.lo.disable_ipv6 = 0\nnet.ipv6.conf.all.forwarding = 1\n```\n\n```\nsysctl --system\n```\n\n### k8s开启双栈（主节点）\n\n#### 修改kube-apiserver\n\n```\n vim /etc/kubernetes/manifests/kube-apiserver.yaml\n```\n\n```text\n--service-cluster-ip-range=10.96.0.0/12,3000::/112\n```\n\n![image-20240424154319890](/images/image-vbud.png)\n\n#### 修改kube-controller-manager\n\n```\nvim /etc/kubernetes/manifests/kube-controller-manager.yaml\n```\n\n![image-20240424154354671](/images/image-vthx.png)\n\n#### 修改kube-proxy\n\n```\nkubectl edit cm -n kube-system kube-proxy\n```\n\n![image-20240424155114938](/images/image-veqq.png)\n\n重启kube-proxy容器\n\n```text\nkubectl get pod -n kube-system | grep kube-proxy | awk '{print $1}' | xargs kubectl delete pod -n kube-system\n```\n\n#### calico 配置双栈\n\n```\nkubectl edit cm -n kube-system calico-config\n```\n\n```\n\"ipam\": {\n    \"type\": \"calico-ipam\",\n    \"assign_ipv4\": \"true\",\n    \"assign_ipv6\": \"true\"\n},\n```\n\n![image-20240424155614010](/images/image-zwor.png)\n\n```\nkubectl edit ds -n kube-system calico-node\n```\n\n```\n# 设置自动获取ipv6地址\n- name: IP6\n  value: autodetect\n# felix启用ipv6支持\n- name: FELIX_IPV6SUPPORT\n  value: \"true\"\n# 设置ipv6地址池，与pod-cidr一致，官方说使用kubeadm安装的集群不需要配置，但是没试过。\n- name: CALICO_IPV6POOL_CIDR\n  value: 2000::/112\n# pod通信使用vxlan封装，如果节点跨子网，使容器之间可以访问\n- name: CALICO_IPV6POOL_VXLAN\n  value: CrossSubnet\n# 开启ipv6 NAT功能，使容器可以访问外部的ipv6地址\n- name: CALICO_IPV6POOL_NAT_OUTGOING\n  value: \"true\"\n```\n\n![image-20240424160040807](/images/image-zktz.png)\n\n等待calico重启\n\n#### 修改kubeadm-config\n\n上方修改的apiserver、controller-manager是静态文件，如果集群升级kubeadm-config会重置静态文件，导致ipv6的配置消失，所以将这个配置文件同时修改\n\n```\nkubectl edit cm -n kube-system kubeadm-config\n```\n\n![image-20240425091454939](/images/image-bitu.png)\n\n### 验证\n\n#### 查看ipv6地址池\n\n```\nkubectl get ippool\n```\n\n![image-20240424160631893](/images/image-rkbg.png)\n\n#### pod获取ipv6地址\n\n重启pod，查看获取的ipv6地址\n\n```\nkubectl delete pod -n xxxx xxxxxx-xxx-xxx --force\n\nkubectl get pod - n xxxx xxxxxx-xxx-xxx -o jsonpath={.status.podIPs}\n```\n\n![image-20240424161315923](/images/image-tbnz.png)\n\n#### services配置双栈\n\n```\nkubectl edit svc -n xxxx xxxxx-svc\n```\n\n![image-20240424161359493](/images/image-yoqx.png)\n\n- `SingleStack`：单栈 Service。控制面使用第一个配置的服务集群 IP 范围为 Service 分配集群 IP\n\n- `PreferDualStack`：为 Service 分配 IPv4 和 IPv6 集群 IP 地址。\n\n- `RequireDualStack`：从 IPv4 和 IPv6 的地址范围分配 Service 的`ClusterIPs`。\n\n  从 `ipFamilies` 数组中第一个ip地址的 `ClusterIPs`作为`ClusterIP`\n\n`注意：ipFamilies 字段修改是有条件的：你可以添加或删除第二个 IP 地址族， 但你不能更改现有 Service 的主要 IP 地址。`\n\n```\nkubectl describe svc -n xxxx xxxxxx-svc\n```\n\n![image-20240424162335499](/images/image-unvc.png)\n\n### service示例\n\n#### 新增service单栈ipv6\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  ipFamilyPolicy: SingleStack\n  ports:\n  - nodePort: 30080\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n#### service双栈\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  - IPv4\n  ipFamilyPolicy: PreferDualStack\n  ports:\n  - nodePort: 30081\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n参考链接：[IPv4/IPv6 双协议栈 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/services-networking/dual-stack/)、[自定义 Calico 配置 |Calico 文档 (tigera.io)](https://docs.tigera.io/calico/latest/networking/ipam/ipv6#enable-dual-stack)","tags":["kubernetes"],"categories":["云原生"]},{"title":"etcd 入门 - 二进制部署etcd集群","url":"/2024/04/25/etcd 入门 - 二进制部署etcd集群/","content":"\n# etcd 入门 - 二进制部署etcd集群\n\n### 环境信息\n\n最少三个及以上奇数个节点\n\n| 名称   | IP          |\n| ------ | ----------- |\n| etcd-1 | 10.20.13.10 |\n| etcd-2 | 10.20.13.11 |\n| etcd-3 | 10.20.13.12 |\n\n### etcd集群节点数量的相关说明：\n\netcd 是基于 raft算法的分布式键值数据库，生来就为集群化而设计的，由于Raft算法在做决策时需要超半数节点的投票，所以etcd集群一般推荐奇数节点，如3、5或者7个节点构成一个集群。\n\netcd官方推荐3、5、7个节点，虽然raft算法也是半数以上投票才能有 leader，但奇数只是推荐，其实偶数也是可以的。如 2、4、8个节点。下面分情况说明：\n\n- 1 个节点：就是单实例，没有集群概念，不做讨论\n- 2 个节点：是集群，但没人会这么配，这里说点废话：双节点的etcd能启动，启动时也能有主，可以正常提供服务，但是一台挂掉之后，就选不出主了，因为他只能拿到1票，剩下的那台也无法提供服务，也就是双节点无容错能力，不要使用。\n- 3 节点：标准的3 节点etcd 集群只能容忍1台机器宕机，挂掉 1 台此时等于2个节点的情况，如果再挂 1 台，就和 2节点的情形一致了，一直选，一直增加任期，但就是选不出来，服务也就不可用了\n- 4 节点：最大容忍1 台 服务器宕机\n- 5 节点：最大容忍 2 台 服务器宕机\n- 6 节点：最大容忍 2 台 服务器宕机\n- 7和8个节点，最大容忍3台 服务器宕机\n\n以此类推，9和10个节点，最大容忍4台 服务器宕机，总结以上可以得出结论：偶数节点虽然多了一台机器，但是容错能力是一样的，也就是说，你可以设置偶数节点，但没增加什么能力，还浪费了一台机器。同时etcd 是通过复制数据给所有节点来达到一致性，因此偶数的多一台机器增加不了性能，反而会拉低写入速度。\n\n## 部署etcd集群（三台节点）\n\n### 部署etcd\n\n下载地址：[Releases · etcd-io/etcd (github.com)](https://github.com/etcd-io/etcd/releases/)\n\n##### 解压\n\n```\ntar zxvf etcd-v3.5.13-linux-amd64.tar.gz\n```\n\n##### 添加到环境变量目录\n\n```\ncd etcd-v3.5.13-linux-amd64\nmv etcd etcdctl etcdutl /usr/local/bin/\n```\n\n##### 创建配置文件目录与数据目录\n\n```\nmkdir -p /etc/etcd\nmkdir -p /data/etcd\nmkdir -p /etc/etcd/pki\n```\n\n##### 注册系统服务\n\n```\nvim /etc/systemd/system/etcd.service\n```\n\n```\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=notify\nEnvironmentFile=/etc/etcd/etcd.conf\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.conf\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n##### etcd-1节点\n\n```\n# 节点名称\nname: \"etcd-1\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.10:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.10:2379,http://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.10:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.10:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n##### etcd-2节点\n\n```\n# 节点名称\nname: \"etcd-2\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.11:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.11:2379,https://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.11:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.11:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n##### etcd-3节点\n\n```\n# 节点名称\nname: \"etcd-3\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.12:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.12:2379,https://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.12:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.12:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n先不启动，准备证书文件，如果不需要证书认证，在配置文件删除上方加密证书的配置启动。跳转到最下方测试即可。\n\n### 部署TLS加密集群\n\n#### 安装cfssl\n\n下载地址：[Releases · cloudflare/cfssl (github.com)](https://github.com/cloudflare/cfssl/releases)\n\n```\nmv cfssl_1.6.5_linux_amd64 /usr/bin/cfssl\nmv cfssljson_1.6.5_linux_amd64 /usr/bin/cfssljson\nchmod +x /usr/bin/{cfssl,cfssljson}\n```\n\n#### 创建默认配置文件（可选，下一步会替换）\n\n```\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n#### 创建CA证书\n\n##### 修改ca-config\n\n各个组件都需要配置证书，并且依赖 CA 证书来签发证书，所以首先要生成好 CA 证书以及后续的签发配置文件\n\n```\ncat > ca-config.json <<EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"Etcd\": {\n        \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"87600h\"\n      }\n    }\n  }\n}\nEOF\n```\n\n>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；\n>signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；\n>server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；\n>client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；\n\n##### 配置证书请求\n\n```\ncat > ca-csr.json <<EOF\n{\n    \"CN\": \"Etcd\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"BeiJing\",\n            \"ST\": \"BeiJing\",\n            \"O\": \"Etcd\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n```\n\n#### 生成ca证书\n\n```\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n```\n\n>CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法；\n>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；\n>kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识；\n\n#### 创建客户端与对等证书请求文件\n\n`注意hosts字段需要加上etcd全部节点的IP/主机名信息及127.0.0.1`\n\n```\ncat > etcd-csr.json <<EOF\n{\n    \"CN\": \"etcd\",\n    \"hosts\": [\n      \"127.0.0.1\",\n      \"10.20.13.10\",\n      \"10.20.13.11\",\n      \"10.20.13.12\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"beijing\",\n            \"ST\": \"BeiJing\",\n            \"O\": \"ETCD\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n```\n\n#####  生成证书和私钥\n\n```\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=Etcd etcd-csr.json | cfssljson -bare etcd\n```\n\n#### 拷贝密钥到etcd节点\n\n```\ncp *.pem /etc/etcd/pki/\nscp /etc/etcd/pki/* 10.20.13.11:/etc/etcd/pki/\nscp /etc/etcd/pki/* 10.20.13.12:/etc/etcd/pki/\n```\n\n\n\n#### 设置开机自启并启动\n\n```\nsystemctl daemon-reload\nsystemctl enable etcd --now\n```\n\n### 测试\n\n```\netcdctl --endpoints=https://10.20.13.10:2379 \\\n--cacert=/etc/etcd/pki/ca.pem \\\n--cert=/etc/etcd/pki/etcd.pem \\\n--key=/etc/etcd/pki/etcd-key.pem \\\nendpoint status --cluster -w table\n```\n\n![image-20240423142609585](/images/image-vvcc.png)\n\n如果没有使用证书，用下方命令测试\n\n```\netcdctl endpoint status --cluster -w table\n```\n\n![image-20240422164147285](/images/image-tlzm.png)\n\n测试读写\n\n在第一台节点上写入\n\n![image-20240423142708565](/images/image-ontn.png)\n\n第二台节点上读取\n\n![image-20240423142805685](/images/image-ugmd.png)\n\n第三台节点上读取\n\n![image-20240423142821737](/images/image-onxo.png)","tags":["etcd"],"categories":["云原生"]},{"title":"etcd 入门 - 二进制部署单节点etcd","url":"/2024/04/25/etcd 入门 - 二进制部署单节点etcd/","content":"\n# etcd 入门 - 二进制部署单节点etcd\n\n下载地址：[Releases · etcd-io/etcd (github.com)](https://github.com/etcd-io/etcd/releases/)\n\n#### 解压\n\n```\ntar zxvf etcd-v3.5.13-linux-amd64.tar.gz\n```\n\n#### 添加到环境变量目录\n\n```\ncd etcd-v3.5.13-linux-amd64\nmv etcd etcdctl etcdutl /usr/local/bin/\n```\n\n#### 创建配置文件目录与数据目录\n\n```\nmkdir -p /etc/etcd\nmkdir -p /data/etcd\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/etcd.service\n```\n\n```\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=notify\nEnvironmentFile=/etc/etcd/etcd.conf\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.conf\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n```\n# 节点名称\nname: 'etcd-1'\n# 指定节点的数据存储目录\ndata-dir: '/data/etcd'\n# 对外提供服务的地址，客户端会连接到这里和 etcd 交互\nlisten-client-urls: 'http://10.20.13.10:2379,http://127.0.0.1:2379'\n```\n\n#### 设置开机自启并启动\n\n```\nsystemctl daemon-reload\nsystemctl enable etcd --now\n```\n\n#### 测试\n\n```\netcdctl endpoint status --cluster -w table\n```\n\n![image-20240422160908837](/images/image-ozlm.png)\n\n![image-20240422160429784](images//image-dgcp.png)","tags":["etcd"],"categories":["云原生"]},{"title":"prometheus通过alertmanager集成钉钉告警","url":"/2024/04/25/prometheus通过alertmanager集成钉钉告警/","content":"\n# prometheus通过alertmanager集成钉钉告警\n\n下载二进制包：[Download | Prometheus](https://prometheus.io/download/)\n\n## 前提条件\n\n- Prometheus\n- alertmanager\n- 设置告警规则\n- 已有监控节点/服务\n\n### 创建告警机器人\n\n#### 创建群聊\n\n![image-20240418114025103](/images/image-hpxf.png)\n\n#### 添加机器人\n\n![image-20240418114122619](/images/image-rddd.png)\n\n#### 配置安全设置为加签，并记录Webhook和加签密钥\n\n![image-20240419143554146](/images/image-jmzx.png)\n\n\n\n### 安装dingtalk-webhook\n\n下载地址：[Releases · timonwong/prometheus-webhook-dingtalk (github.com)](https://github.com/timonwong/prometheus-webhook-dingtalk/releases)\n\n#### 安装\n\n```\ntar zxvf prometheus-webhook-dingtalk-2.1.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/prometheus-webhook-dingtalk-2.1.0.linux-amd64 /usr/local/prometheus/dingtalk\n```\n\n#### 修改配置文件\n\n##### 配置告警消息\n\n```\nvim /usr/local/prometheus/dingtalk/default.tmpl\n```\n\n```\n{{ define \"__subject\" }}\n[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}]\n{{ end }}\n\n\n{{ define \"__alert_list\" }}{{ range . }}\n---\n\n{{ if .Labels.owner }}@{{ .Labels.owner }}{{ end }}\n\n**告警主题**: {{ .Annotations.summary }}\n\n**告警类型**: {{ .Labels.alertname }}\n\n**告警级别**: {{ .Labels.severity }} \n\n**告警主机**: {{ .Labels.instance }} \n\n**告警信息**: {{ index .Annotations \"description\" }}\n\n**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}\n{{ end }}{{ end }}\n\n{{ define \"__resolved_list\" }}{{ range . }}\n---\n\n{{ if .Labels.owner }}@{{ .Labels.owner }}{{ end }}\n\n**告警主题**: {{ .Annotations.summary }}\n\n**告警类型**: {{ .Labels.alertname }} \n\n**告警级别**: {{ .Labels.severity }}\n\n**告警主机**: {{ .Labels.instance }}\n\n**告警信息**: {{ index .Annotations \"description\" }}\n\n**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}\n\n**恢复时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.EndsAt) \"Asia/Shanghai\" }}\n{{ end }}{{ end }}\n\n\n{{ define \"default.title\" }}\n{{ template \"__subject\" . }}\n{{ end }}\n\n{{ define \"default.content\" }}\n{{ if gt (len .Alerts.Firing) 0 }}\n**====侦测到{{ .Alerts.Firing | len  }}个故障====**\n\n{{ template \"__alert_list\" .Alerts.Firing }}\n---\n\n{{ end }}\n\n{{ if gt (len .Alerts.Resolved) 0 }}\n**====恢复{{ .Alerts.Resolved | len  }}个故障====**\n{{ template \"__resolved_list\" .Alerts.Resolved }}\n{{ end }}\n{{ end }}\n\n\n{{ define \"ding.link.title\" }}{{ template \"default.title\" . }}{{ end }}\n{{ define \"ding.link.content\" }}{{ template \"default.content\" . }}{{ end }}\n{{ template \"default.title\" . }}\n{{ template \"default.content\" . }}\n```\n\n##### 钉钉机器人集成\n\n```\nvim /usr/local/prometheus/dingtalk/config.yml\n```\n\n```\n## Request timeout\n# timeout: 5s\n\n## Uncomment following line in order to write template from scratch (be careful!)\n#no_builtin_template: true\n\n## Customizable templates path\ntemplates:\n  - /usr/local/prometheus/dingtalk/default.tmpl\n\n## You can also override default template using `default_message`\n## The following example to use the 'legacy' template from v0.3.0\n#default_message:\n#  title: '{{ template \"legacy.title\" . }}'\n#  text: '{{ template \"legacy.content\" . }}'\n\n## Targets, previously was known as \"profiles\"\ntargets:\n  webhook1:\n    # token\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxxxxxxxxxxx\n    # 加签密钥\n    secret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n\n\n\n\n### 修改alertmanager配置文件\n\n```\nvim /usr/local/prometheus/alertmanager/alertmanager.yml\n```\n\n```\nroute:\n  group_by: ['dingtalk']\n  group_wait: 1s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'dingtalk.webhook1'\n  routes:\n  - receiver: \"dingtalk.webhook1\"\n    match_re:\n      altername: \".*\"\nreceivers:\n  - name: 'dingtalk.webhook1'\n    webhook_configs:\n      - url: 'http://localhost:8060/dingtalk/webhook1/send'\n        send_resolved: true\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n```\n\n#### 重启alertmanager\n\n```\nsystemctl restart alertmanager\n```\n\n#### 验证\n\n访问alertmanager地址：`http://ip:9093/#/status`，验证配置生效。\n\n![image-20240419143915353](/images/image-vzdl.png)\n\n\n\n#### 测试\n\n找一个节点或服务，我这里停掉当前节点的node_exporter服务\n\n```\nsystemctl stop node_exporter\n```\n\n触发以下告警规则\n\n```\n  - alert: 服务器宕机\n    expr: up == 0\n    for: 1s\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 服务器宕机, 请尽快处理!\"\n      description: \"{{$labels.instance}} 服务器node_exporter服务被关闭,当前状态{{ $value }}. \"\n```\n\n稍等一下，收到\n\n![image-20240419140237250](/images/image-tkco.png)\n","tags":["prometheus"],"categories":["监控"]},{"title":"zabbix 监控接入钉钉告警","url":"/2024/04/25/zabbix 监控接入钉钉告警/","content":"\n## zabbix 监控接入钉钉告警\n\n### 创建群聊\n\n![image-20240418114025103](/images/image-zlhb.png)\n\n### 添加机器人\n\n![image-20240418114122619](/images/image-vxfw.png)\n\n#### 配置安全设置为加签，并记录Webhook和加签密钥\n\n![image-20240418114101147](/images/image-qete.png)\n\n### 编写告警脚本\n\n在alertscripts目录下添加以下脚本，修改Webhook与加签密钥\n\n```\n#!/usr/bin/python3\n# coding:utf-8 \n\nimport time\nimport hmac\nimport hashlib\nimport base64\nimport urllib.parse\nimport requests\nimport json\nimport sys\n\n\napi_url = 'https://oapi.dingtalk.com/robot/send?access_token=' \\\n              'xxxxxxxxxxxxxxxxxxxxxxxxxxxxx'   # webhook token粘贴在这里\n\n\n# 计算签名\ndef get_timestamp_sign():\n    \"\"\"\n    official url https://open.dingtalk.com/document/robots/customize-robot-security-settings\n    :return: timestamp, sign\n    \"\"\"\n    timestamp = str(round(time.time() * 1000))\n    secret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'  # 加签密钥粘贴在这里\n    secret_enc = secret.encode('utf-8')\n    string_to_sign = '{}\\n{}'.format(timestamp, secret)\n    string_to_sign_enc = string_to_sign.encode('utf-8')\n    hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()\n    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))\n    return timestamp, sign\n\n\n# 获取签名计算后的链接\ndef get_signed_url():\n    \"\"\"\n    :拼接url format --> https://oapi.dingtalk.com/robot/send?access_token=XXXXXX&timestamp=XXX&sign=XXX\n    :return:  webhook\n    \"\"\"\n    timestamp, sign = get_timestamp_sign()\n    webhook = api_url + \"&timestamp=\" + timestamp + \"&sign=\" + sign\n    return webhook\n\n\n# 定义webhook消息模式\ndef get_webhook(mode):\n    if mode == 0:                 # 仅关键字\n        webhook = api_url\n    elif mode == 1 or mode == 2:  # 关键字+加签 或 关键字+加签+ip\n        webhook = get_signed_url()\n    else:\n        webhook = \"\"\n        print(\"error! mode:   \", mode, \"  webhook :  \", webhook)\n    print(webhook)\n    return webhook\n\n\ndef get_message(text, user_info):\n    message = {\n        \"msgtype\": \"text\",  # 有text, \"markdown\"、link、整体跳转ActionCard 、独立跳转ActionCard、FeedCard类型等\n        \"text\": {\n            \"content\": text  # 消息内容\n        },\n        \"at\": {\n            \"atMobiles\": [\n                user_info,\n            ],\n            \"isAtAll\": False  # 是否是发送群中全体成员\n        }\n    }\n    return message\n\n\ndef send_ding_message(text, user_info):\n    \"\"\"\n    model: 0 --> 关键字; 1 --> 关键字 +加签; 2 --> 关键字+加签+IP\n    :param text:\n    :param user_info:\n    :return:\n    \"\"\"\n    webhook = get_webhook(1)\n    # 构建请求头部\n    header = {\n        \"Content-Type\": \"application/json\",\n        \"Charset\": \"UTF-8\"\n    }\n    message = get_message(text, user_info)\n    message_json = json.dumps(message)\n    info = requests.post(url=webhook, data=message_json, headers=header).json()\n    code = info[\"errcode\"]\n    errmsg = info[\"errmsg\"]\n    now_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n    if code == 0:\n        print(now_time, ':Message sent successfully, return info:{} {}\\n'.format(code, errmsg))\n    else:\n        print(now_time + \":Message sending failed, return info:{} {}\\n\".format(code, errmsg))\n        exit(3)\n\n\nif __name__ == \"__main__\":\n    t = sys.argv[3]\n    u = sys.argv[1]\n    send_ding_message(t, u)\n```\n\n![image-20240418114455247](/images/image-vkbg.png)\n\n#### 测试发送信息\n\n```shell\n./dingding.py all subject \"test message\"\n```\n\n![image-20240418115015785](/images/image-lvop.png)\n\n### 添加告警媒介\n\n![image-20240418114558534](/images/image-bjvx.png)\n\n![image-20240418114614275](/images/image-cmfp.png)\n\n![image-20240418114625802](/images/image-yzri.png)\n\n### 添加动作\n\n触发器触发后告警给用户群组\n\n添加用户告警媒介，收件人为钉钉注册的手机号，如果发送的是具体人会@。\n\n![image-20240418140140808](/images/image-pihp.png)\n\n![image-20240418115407582](/images/image-mewc.png)\n\n![image-20240418115426344](/images/image-ecmr.png)\n\n#### 配置告警消息\n\n```\n当前状态: {TRIGGER.STATUS}\n告警主机: {HOST.NAME}\n告警 IP: {HOST.IP}\n告警时间: {EVENT.DATE}-{EVENT.TIME}\n告警等级: {TRIGGER.SEVERITY}\n告警信息: {TRIGGER.NAME}\n告警项目: {TRIGGER.KEY1}\n```\n\n![image-20240418115452935](/images/image-uunn.png)\n\n### 测试触发告警\n\n占用cpu单核心负载，根据核数开对应的终端，每个终端运行以下命令。\n\n```\nwhile : ; do  openssl speed; done\n```\n\n当前是4核主机，开四个线程\n\n![image-20240418135821593](/images/image-gttg.png)\n\n![image-20240418141834590](/images/image-bxsn.png)","tags":["zabbix"],"categories":["监控"]},{"title":"二进制部署prometheus+alertmanager+grafana监控平台","url":"/2024/04/25/二进制部署prometheus+alertmanager+grafana监控平台/","content":"\n# 二进制部署prometheus+alertmanager+grafana监控平台\n\n下载二进制包：[Download | Prometheus](https://prometheus.io/download/)\n\n### 安装prometheus\n\n本文档使用LTS长期支持版：2.45.4\n\n```\nmkdir /usr/local/prometheus  # 新建存放prometheus组件目录\n# 安装\ntar zxvf prometheus-2.45.4.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/prometheus-2.45.4.linux-amd64 /usr/local/prometheus/prometheus\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/prometheus.service\n```\n\n```\n[Unit]\nDescription=Prometheus Server\nDocumentation=https://prometheus.io/docs/introduction/overview/\nAfter=network-online.target\n\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nRestart=on-failure\nExecStart=/usr/local/prometheus/prometheus/prometheus \\\n  --config.file=/usr/local/prometheus/prometheus/prometheus.yml \\\n  --storage.tsdb.path=/usr/local/prometheus/prometheus/data \\\n  --storage.tsdb.retention.time=60d \\\n  --web.enable-lifecycle\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable prometheus --now\n```\n\n#### 检查状态\n\n```\nsystemctl status prometheus\n```\n\n#### 验证\n\n访问`http://ip:9090`，访问到即可。\n\n![image-20240419141231845](/images/image-lmaq.png)\n\n### 安装alertmanager\n\n```\ntar zxvf alertmanager-0.27.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/alertmanager-0.27.0.linux-amd64 /usr/local/prometheus/alertmanager\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/alertmanager.service\n```\n\n```\n[Unit]\nDescription=Alert Manager\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nExecStart=/usr/local/prometheus/alertmanager/alertmanager \\\n  --config.file=/usr/local/prometheus/alertmanager/alertmanager.yml \\\n  --storage.path=/usr/local/prometheus/alertmanager/data\n\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n```\nvim /usr/local/prometheus/alertmanager/alertmanager.yml\n```\n\n```\nroute:\n  group_by: ['dingtalk']\n  group_wait: 1s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'dingtalk.webhook1'\n  routes:\n  - receiver: \"dingtalk.webhook1\"\n    match_re:\n      altername: \".*\"\nreceivers:\n  - name: 'dingtalk.webhook1'\n    webhook_configs:\n      - url: 'http://localhost:8060/dingtalk/webhook1/send'\n        send_resolved: true\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable alertmanager --now\n```\n\n#### 检查状态\n\n```\nsystemctl status alertmanager\n```\n\n#### 验证\n\n![image-20240419140922190](/images/image-ltls.png)\n\n\n\n### 安装node_exporter\n\n```\ntar zxvf node_exporter-1.7.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/node_exporter-1.7.0.linux-amd64 /usr/local/prometheus/node_exporter\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/node_exporter.service\n```\n\n```\n[Unit]\nDescription=node_exporter\nDocumentation=https://prometheus.io/\nAfter=network.target\n[Service]\n#User=prometheus\n#Group=prometheus\nExecStart=/usr/local/prometheus/node_exporter/node_exporter\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable node_exporter --now\n```\n\n#### 检查状态\n\n```\nsystemctl status node_exporter\n```\n\n#### 验证\n\n访问`http://ip:9100/metrics`，监控数据存在即可。\n\n![image-20240419141314354](/images/image-oktg.png)\n\n### 安装grafana\n\n下载地址：[Grafana get started | Cloud, Self-managed, Enterprise](https://grafana.com/get/?tab=self-managed)\n\n```\ntar zxvf node_exporter-1.7.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/node_exporter-1.7.0.linux-amd64 /usr/local/prometheus/node_exporter\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/grafana-server.service\n```\n\n```\n[Unit]\nDescription=Grafana server\nDocumentation=http://docs.grafana.org\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nRestart=on-failure\nExecStart=/usr/local/prometheus/grafana/bin/grafana-server \\\n  --config=/usr/local/prometheus/grafana/conf/defaults.ini \\\n  --homepath=/usr/local/prometheus/grafana\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable grafana-server --now\n```\n\n#### 检查状态\n\n```\nsystemctl status grafana-server\n```\n\n浏览器访问`http://ip:3000`,账号admin，密码admin，首次根据提示修改密码\n\n![image-20240419141413034](/images/image-hlhn.png)\n\n\n\n### 添加监控\n\n#### 创建告警规则\n\n```\nmkdir -p /usr/local/prometheus/prometheus/rules\n```\n\n```\nvim node.yaml\n```\n\n```yaml\ngroups:\n- name: 服务器资源监控\n  rules:\n  - alert: 内存使用率过高\n    expr: 100 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 80\n    for: 3m \n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{ $labels.instance }} 内存使用率过高, 请尽快处理！\"\n      description: \"{{ $labels.instance }}内存使用率超过80%,当前使用率{{ $value }}%.\"\n          \n  - alert: 服务器宕机\n    expr: up == 0\n    for: 1s\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 服务器宕机, 请尽快处理!\"\n      description: \"{{$labels.instance}} 服务器node_exporter服务被关闭,当前状态{{ $value }}. \"\n \n  - alert: CPU高负荷\n    expr: 100 - (avg by (instance,job)(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 90\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} CPU使用率过高,请尽快处理！\"\n      description: \"{{$labels.instance}} CPU使用大于90%,当前使用率{{ $value }}%. \"\n      \n  - alert: 磁盘IO性能\n    expr: avg(irate(node_disk_io_time_seconds_total[1m])) by(instance,job)* 100 > 90\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流入磁盘IO使用率过高,请尽快处理！\"\n      description: \"{{$labels.instance}} 流入磁盘IO大于90%,当前使用率{{ $value }}%.\"\n \n \n  - alert: 网络流入\n    expr: ((sum(rate (node_network_receive_bytes_total{device!~'tap.*|veth.*|br.*|docker.*|virbr*|lo*'}[5m])) by (instance,job)) / 100) > 102400\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流入网络带宽过高，请尽快处理！\"\n      description: \"{{$labels.instance}} 流入网络带宽持续5分钟高于100M. RX带宽使用量{{$value}}.\"\n \n  - alert: 网络流出\n    expr: ((sum(rate (node_network_transmit_bytes_total{device!~'tap.*|veth.*|br.*|docker.*|virbr*|lo*'}[5m])) by (instance,job)) / 100) > 102400\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流出网络带宽过高,请尽快处理！\"\n      description: \"{{$labels.instance}} 流出网络带宽持续5分钟高于100M. RX带宽使用量{$value}}.\"\n  \n  - alert: TCP连接数\n    expr: node_netstat_Tcp_CurrEstab > 10000\n    for: 2m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \" TCP_ESTABLISHED过高！\"\n      description: \"{{$labels.instance}} TCP_ESTABLISHED大于100%,当前使用率{{ $value }}%.\"\n \n  - alert: 磁盘容量\n    expr: 100-(node_filesystem_free_bytes{fstype=~\"ext4|xfs\"}/node_filesystem_size_bytes {fstype=~\"ext4|xfs\"}*100) > 90\n    for: 1m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.mountpoint}} 磁盘分区使用率过高，请尽快处理！\"\n      description: \"{{$labels.instance}} 磁盘分区使用大于90%，当前使用率{{ $value }}%.\"\n```\n\n#### 修改配置文件\n\n```\nvim /usr/local/prometheus/prometheus/prometheus.yml\n```\n\n```yaml\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n           - localhost:9093 # alertmanager地址，先写上\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  - \"rules/*.yml\"  # 告警规则\n  # - \"first_rules.yml\"\n  # - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n\n    static_configs:\n      - targets: [\"localhost:9090\"]  # 监控本主机\n\n  - job_name: 'alertmanager'\n    scrape_interval: 10s\n    static_configs:\n    - targets: ['localhost:9093'] # 监控alertmanager 主机\n\n  - job_name: 'node-exporter'\n    scrape_interval: 10s\n    static_configs:\n    - targets: ['localhost:9100'] # 监控节点，节点安装node_exporter\n      labels:\n        instance: Prometheus服务器\n```\n\n#### 验证数据\n\n重启prometheus\n\n```\nsystemctl restart prometheus\n```\n\n![image-20240419141800186](/images/image-bjzo.png)\n\n### grafana集成prometheus\n\n#### 浏览器添加数据源\n\n![image-20240419141921453](/images/image-fnfh.png)\n\n添加Prometheus地址，我这里是同一台安装，使用localhost\n\n![image-20240419142016950](/images/image-kvgk.png)\n\n最后保存即可，不需要其他配置。\n\n#### 添加dashboard\n\n这里使用1860id，节点监控仪表盘，更多仪表盘访问官方获取：[Dashboards | Grafana Labs](https://grafana.com/grafana/dashboards/)\n\n![image-20240419142355115](/images/image-qoik.png)","tags":["prometheus"],"categories":["监控"]},{"title":"阿里云域名使用cert-manager通过acme申请https证书(dns01方式)","url":"/2024/04/25/阿里云域名使用cert-manager通过acme申请https证书(dns01方式)/","content":"\n# 阿里云域名使用cert-manager通过acme申请https证书(dns01方式)\n\n## 前提条件\n\n- k8s集群<1.19\n- cert-manager\n- ingress-nginx\n- 阿里云域名\n\n## 申请AccessKey\n\n登录阿里云域名所在账号，开通AccessKey并记录下来\n\n![image-20240416171250857](/images/image-juep.png)\n\n\n\n### 将access-key和secret-key转换成base64\n\n![image-20240416171702060](/images/image-mmsf.png)\n\n### 将转换后的值保存为k8s secret\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: alidns-secret\n  namespace: cert-manager\ndata:\n  access-key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n  secret-key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n\n\n## 安装alidns-webhook\n\n```\nkubectl apply -f https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml\n```\n\n![image-20240416172108074](/images/image-etnp.png)\n\n![image-20240416172143884](/images/image-jmdd.png)\n\n## 启动一个http服务\n\n测试用，如果有服务可跳过\n\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo\nspec:\n  selector:\n    app: demo\n  ports:\n    - name: http-port\n      port: 80\n      protocol: TCP\n      targetPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo\n  labels:\n    app: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.21.6\n          env:\n            - name: TZ\n              value: Asia/Shanghai\n          ports:\n            - containerPort: 80\n```\n\n![image-20240415164809387](/images/image-ufah.png)\n\n![image-20240415165231793](/images/image-ljcx.png)\n\n## 颁发机构（CA）\n\n```yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: user@vsoul.cn\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    solvers:\n    # An empty 'selector' means that this solver matches all domains\n    - dns01:\n        webhook:\n          # 注意这里要改动，在https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml中也要改动对应的groupName\n          groupName: acme.yourcompany.com\n          solverName: alidns\n          config:\n            region: \"\"\n            accessKeySecretRef:\n              name: alidns-secret\n              key: access-key\n            secretKeySecretRef:\n              name: alidns-secret\n              key: secret-key\n```\n\n![image-20240416172852330](/images/image-tbjn.png)\n\n## 签发证书\n\n```yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\" # 强制转换https\n    cert-manager.io/cluster-issuer: letsencrypt-prod # ClusterIssuer名称\nspec:\n  ingressClassName: nginx\n  tls:\n    - secretName: test-tls # 证书名\n      hosts:\n        - vsoul.cn # 域名\n  rules:\n    - host: vsoul.cn # 域名\n      http:\n        paths:\n          - path: /\n            #pathType: ImplementationSpecific\n            pathType: Prefix\n            backend:\n              service:\n                name: demo # 服务名\n                port:\n                  number: 80 # 服务的端口号 service port，非pod port\n```\n\n![image-20240416173038711](/images/image-bylo.png)\n\n等待certificate状态为True\n\n![image-20240416173455625](/images/image-xjwk.png)\n\n## 测试\n\n浏览器通过https访问域名\n\n![image-20240416173343380](/images/image-styt.png)\n\n## 故障排查\n\nCertificate   =>  CertificateRequest  => Order => Challenge \n\n以此使用kubectl get、describe查看状态和日志\n","tags":["cert-manager"],"categories":["云原生"]},{"title":"yum更新Linux内核","url":"/2023/04/23/yum更新Linux内核/","content":"\n## yum更新Linux内核\n\n```\n#载入ELRepo仓库的公共密钥\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\n#安装ELRepo仓库的yum源\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm\n#查看可用的系统内核包\nyum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available\n#安装最新版本内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y\n```\n\n### 设置默认内核\n\n#### 获取内核索引值\n\n```\nawk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg\n```\n\n![image-20240423150008735](/images/image-zjyd.png)\n\n更换的内核为6.8.7，索引值为0\n\n#### 修改grub配置文件\n\n```\nvim /etc/default/grub\n```\n\n![image-20240423145939049](/images/image-wihy.png)\n\n#### 生成grub配置文件\n\n```\ngrub2-mkconfig -o /boot/grub2/grub.cfg\n```\n\n![image-20240423150440339](/images/image-ssrx.png)\n\n#### 重启验证\n\n```\nreboot\n```\n\n![image-20240423150506336](/images/image-cqpp.png)","tags":["linux"],"categories":["系统"]}]