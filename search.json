[{"title":"【云原生】日志收集方案（二）：基于ECK部署生产级ElasticSearch + Kibana + Logstash","url":"/2025/07/25/【云原生】日志收集方案（二）：基于ECK部署生产级ElasticSearch + Kibana + Logstash/","content":"\n## 介绍\n之前文章写过[基于ECK部署生产级ElasticSearch + Kibana + FileBeat](./【云原生】日志收集方案（一）：基于ECK部署生产级ElasticSearch%20+%20Kibana%20+%20FileBeat.md)，采用filebeat轻量级采集器，本文使用Logstash采集器。\n\n### ELK组件关系\n官方在 2018 年后把“ELK”改名 Elastic Stack，成员扩充为：\nElastic Stack = Elasticsearch + Kibana + Logstash + Beats + 其他（如 Elastic Agent、Fleet、APM、Security 等）\n\n早期的 ELK 只有 Elasticsearch、Logstash、Kibana，后来官方推出 Beats 家族作为轻量采集器，Filebeat 的前身叫 logstash-forwarder，由 Logstash 作者 Jordan Sissel 用 Go 写成，之后被 Elastic 团队重构并纳入 Beats；Logstash 与 Beats 是同级兄弟，而非 Beats 成员。\n\n虽然 Logstash 因 JVM 吃资源被吐槽笨重，但它拥有 200+ 插件、强大 ETL 能力、持久化缓冲以及与 Kafka/Redis 的高可靠链路。\n\n二者在现代架构中互补共存：边缘用 Beats 采集，必要时经 Logstash 深加工后再进 Elasticsearch。\n\n\n## 产品支持列表\nhttps://www.elastic.co/cn/support/matrix/#matrix_kubernetes\n\n\n## 部署ECK crd\n\n官方地址：https://www.elastic.co/docs/deploy-manage/deploy/cloud-on-k8s/install-using-yaml-manifest-quickstart\n```\nkubectl create -f https://download.elastic.co/downloads/eck/3.0.0/crds.yaml\n```\n\n```\nkubectl apply -f https://download.elastic.co/downloads/eck/3.0.0/operator.yaml\n```\n\n###  部署elasticsearch\n\n部署前应设置虚拟内存：https://www.elastic.co/docs/deploy-manage/deploy/cloud-on-k8s/virtual-memory\n\n`elasticsearch.yaml`\n```\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\n  name: elasticsearch\n  namespace: elk\nspec:\n  version: 9.0.3\n  nodeSets:\n  - name: elk\n    count: 3\n    #config:\n    #  node.store.allow_mmap: false\n    volumeClaimTemplates:\n    - metadata:\n        name: elasticsearch-data\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 5Gi\n        storageClassName: nfs-storage\n    podTemplate:\n      spec:\n        initContainers:\n        - name: sysctl\n          securityContext:\n            privileged: true\n            runAsUser: 0\n          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']\n```\n\n### 部署kibana\n\n`kibana.yaml`\n```\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\n  name: kibana\n  namespace: elk\nspec:\n  version: 9.0.3\n  count: 3\n  elasticsearchRef:\n    name: elasticsearch\n  config:\n    # 更多kibana设置：https://www.elastic.co/docs/reference/kibana/configuration-reference/general-settings\n    elasticsearch.requestHeadersWhitelist:\n    - authorization\n  podTemplate:\n    spec:\n      containers:\n      - name: kibana\n        env:\n        - name: I18N_LOCALE\n          value: \"zh-CN\"\n  http:\n    service:\n      spec:\n        type: NodePort\n```\n\n### 部署Logstash\n\n`logstash.yaml`\n```\napiVersion: logstash.k8s.elastic.co/v1alpha1\nkind: Logstash\nmetadata:\n  name: logstash\n  namespace: elk\nspec:\n  count: 3\n  elasticsearchRefs:\n    - name: elasticsearch\n      clusterName: elasticsearch\n  version: 9.0.3\n  pipelines:\n    - pipeline.id: main\n      config.string: |\n        input {\n          beats {\n            port => 5044\n          }\n        }\n        output {\n          elasticsearch {\n            hosts => [ \"${ELASTICSEARCH_ES_HOSTS}\" ]\n            user => \"${ELASTICSEARCH_ES_USER}\"\n            password => \"${ELASTICSEARCH_ES_PASSWORD}\"\n            ssl_certificate_authorities => \"${ELASTICSEARCH_ES_SSL_CERTIFICATE_AUTHORITY}\"\n          }\n        }\n  services:\n    - name: beats\n      service:\n        spec:\n          type: NodePort\n          ports:\n            - port: 5044\n              name: \"filebeat\"\n              protocol: TCP\n              targetPort: 5044\n```","tags":["ELK"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（八）：Kubernetes部署Grafana接入Prometheus","url":"/2025/07/25/【云原生】云原生监控方案（八）：Kubernetes部署Grafana接入Prometheus/","content":"\n`grafana-deployment.yaml`\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - env: []\n        image: grafana/grafana:12.0.2\n        name: grafana\n        ports:\n        - containerPort: 3000\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n          readOnly: false\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n        - mountPath: /etc/grafana/provisioning/dashboards\n          name: grafana-dashboards\n          readOnly: false\n        - mountPath: /tmp\n          name: tmp-plugins\n          readOnly: false\n        - mountPath: /etc/grafana\n          name: grafana-config\n          readOnly: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - emptyDir: {}\n        name: grafana-storage\n      - name: grafana-datasources\n        secret:\n          secretName: grafana-datasources\n      - configMap:\n          name: grafana-dashboards\n        name: grafana-dashboards\n      - emptyDir:\n          medium: Memory\n        name: tmp-plugins\n      - name: grafana-config\n        secret:\n          secretName: grafana-config\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: grafana-config\n  namespace: monitoring\nstringData:\n  grafana.ini: |\n    [date_formats]\n    default_timezone = Asia/Shanghai\ntype: Opaque\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: grafana-datasources\n  namespace: monitoring\ntype: Opaque\nstringData:\n  datasources.yaml: |-\n    {\n        \"apiVersion\": 1,\n        \"datasources\": [\n            {\n                \"access\": \"proxy\",\n                \"editable\": false,\n                \"name\": \"prometheus\",\n                \"orgId\": 1,\n                \"type\": \"prometheus\",\n                \"url\": \"http://prometheus.monitoring.svc:9090\",\n                \"version\": 1\n            }\n        ]\n    }\n```\n\n`grafana-svc.yaml`\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\n  namespace: monitoring\nspec:\n  ports:\n  - name: http\n    port: 3000\n    targetPort: http\n    nodePort: 30000\n  selector:\n    app: grafana\n  type: NodePort\n```\n\n`grafana-cm.yaml`\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboards\n  namespace: monitoring\ndata:\n  dashboards.yaml: |-\n    {\n        \"apiVersion\": 1,\n        \"providers\": [\n            {\n                \"folder\": \"Default\",\n                \"folderUid\": \"\",\n                \"name\": \"0\",\n                \"options\": {\n                    \"path\": \"/grafana-dashboard-definitions/0\"\n                },\n                \"orgId\": 1,\n                \"type\": \"file\"\n            }\n        ]\n    }\n```\n\n`grafana-sm.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  endpoints:\n  - interval: 15s\n    port: http\n  selector:\n    matchLabels:\n      app: grafana\n```\n","tags":["Kubernetes","Grafana"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（七）：AlertManagerConfig集成邮箱告警","url":"/2025/07/10/【云原生】云原生监控方案（七）：AlertManagerConfig集成邮箱告警/","content":"\n## 介绍\n在第二篇中（[原文链接](./【云原生】云原生监控方案（二）：基于Prometheus-Operator部署Prometheus、AlertManager.md)）介绍了alert manager的安装，这篇文章是基于这个内容的扩展\n\n\n### AlertManager Config\n\n使用Alertmanager CRD 创建AlertManager时，有两种加载配置文件方式\n\n官方文档：https://prometheus-operator.dev/docs/developer/alerting/\n```\n  alertmanagerConfiguration:\n    name: alert-global\n  alertmanagerConfigSelector:\n    matchLabels:\n      alert: alert-config\n```\n`alertmanagerConfiguration`：全局配置\n\n`alertmanagerConfigSelector`：基于命名空间的配置\n\n非全局会添加基于命名空间的条件，告警规则必须存在这个lable，以下两者生成alertmanager配置的区别\n```\nroute:\n<<<<<<< 基于命名空间的配置\n  receiver: \"null\"\n  continue: false\n  routes:\n  - receiver: monitoring/config-example/email\n    group_by:\n    - alertname\n    matchers:\n    - namespace=\"monitoring\"\n    continue: true\n    group_wait: 10s\n    group_interval: 5m\n    repeat_interval: 12h\n>>>>>>> 全局配置\n  receiver: monitoring/alert-global/email\n  group_by:\n  - alertname\n  continue: false\n  group_wait: 10s\n  group_interval: 1m\n  repeat_interval: 1m\n\n\ninhibit_rules:\n<<<<<<< 基于命名空间的配置\n- source_matchers:\n  - namespace=\"monitoring\"\n  - severity=\"critical\"\n  target_matchers:\n  - namespace=\"monitoring\"\n  - severity=\"warning\"\n  equal:\n  - instance\n>>>>>>> 全局配置\n- source_matchers:\n  - severity=\"critical\"\n  target_matchers:\n  - severity=\"warning\"\n  equal:\n  - instance\n\n```\n\n## 邮件告警\n1. 创建发件邮箱\n   \n   略\n\n2. 测试邮箱发件（阿里邮箱）\n```\ncurl --url 'smtps://smtp.vsoul.cn' \\\n     --mail-from 'notify@vsoul.cn' \\\n     --mail-rcpt 'lihaoxuan@vsoul.cn' \\\n     --user 'notify@vsoul.cn:YOUR_PASSWORD' \\\n     --ssl-reqd --insecure \\\n     -T <(printf 'From: VSoul 通知 <notify@vsoul.cn>\\nTo: lihaoxuan@vsoul.cn\\nSubject: 测试邮件发送')\n```\n\n\n###  创建AlertManagerConfig\n```\napiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: email\n  namespace: monitoring\n  labels:     # 创建Prometheus时指定alertmanager labels\n    alert: alert-config\nspec:\n  # 路由\n  # 官方API：https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1alpha1.Route\n  route:\n    groupBy: ['alertname']  # 告警分组依据，通常按alertname分组，相同名称的告警会被合并\n    groupWait: 10s         # 初始等待时间，同一组的告警首次触发后等待10秒再发送（用于收集同组其他告警）\n    groupInterval: 5m      # 同一组告警的间隔时间（当有新告警触发时，距离上次发送至少5分钟才会再通知）\n    repeatInterval: 12h    # 重复告警的发送间隔（相同告警未解决时，每隔12小时重复通知一次）\n    receiver: 'email'      # 默认接收器名称（需与receivers中的name对应）\n  # 接收器\n  # 官方API：https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1alpha1.Receiver\n  receivers:\n  - name: 'email'\n    # 邮箱配置\n    emailConfigs:\n    - to: 'lihaoxuan@vsoul.cn'\n      from: 'notify@vsoul.cn'\n      smarthost: 'smtp.qiye.aliyun.com:465'\n      authUsername: 'notify@vsoul.cn'\n      authPassword:\n        name: smtp-secret\n        key: password\n      requireTLS: false\n      sendResolved: true\n      tlsConfig:\n        insecureSkipVerify: false\n  # 抑制规则\n  # 官方API：https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1alpha1.InhibitRule\n  inhibitRules:\n  - sourceMatch:\n    - name: severity\n      value: critical\n    targetMatch:\n    - name: severity\n      value: warning\n    equal:\n    - instance\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: smtp-secret\n  namespace: monitoring\ntype: Opaque\nstringData:\n  password: YOUR_PASSWORD\n\n```\n\n\n###  创建告警规则\n\n测试规则\n\n```\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: test-email-alert\n  namespace: monitoring\nspec:\n  groups:\n  - name: test-email\n    rules:\n    - alert: TestEmailAlert\n      expr: vector(1) # 永远为真\n      for: 0m\n      labels:\n        # 如果时基于命名空间的配置需要加上namespace label，因为测试PromQL的条件永远为真，没有指标，所以获取不到namespace的label\n        namespace: monitor \n        severity: critical\n      annotations:\n        summary: \"测试邮件告警\"\n        description: \"这是一条来自 PrometheusRule 的测试告警\"\n\n```\n\n收到告警即为成功\n","tags":["Kubernetes","AlertManager"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（六）：集成k8s监控组件kube-state-metrics的使用","url":"/2025/07/09/【云原生】云原生监控方案（六）：集成k8s监控组件kube-state-metrics的使用/","content":"\n官方仓库地址：https://github.com/kubernetes/kube-state-metrics\n\n[下载地址](https://github.com/kubernetes/kube-state-metrics/releases)\n\n### 兼容性\nhttps://github.com/kubernetes/kube-state-metrics#versioning\n\n| kube-state-metrics | Kubernetes client-go Version |\n|--------------------|:----------------------------:|\n| **v2.12.0**        | v1.29                        |\n| **v2.13.0**        | v1.30                        |\n| **v2.14.0**        | v1.31                        |\n| **v2.15.0**        | v1.32                        |\n| **v2.16.0**        | v1.32                        |\n| **main**           | v1.32                        |\n\n## 部署\n\n`rbac.yaml`\n\n组件使用的集群权限\n```\napiVersion: v1\nautomountServiceAccountToken: false\nkind: ServiceAccount\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - serviceaccounts\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingressclasses\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - rbac.authorization.k8s.io\n  resources:\n  - clusterrolebindings\n  - clusterroles\n  - rolebindings\n  - roles\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring\n\n```\n\n`kube-state-metrics.yaml`\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      automountServiceAccountToken: true\n      containers:\n        #- image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0\n      - image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: http-metrics\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: telemetry\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: kube-state-metrics\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app: kube-state-metrics\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    honorLabels: true\n    interval: 30s\n    metricRelabelings:\n    - action: drop\n      regex: kube_endpoint_address_not_ready|kube_endpoint_address_available\n      sourceLabels:\n      - __name__\n    port: http-metrics\n    relabelings:\n    - action: labeldrop\n      regex: (pod|service|endpoint|namespace)\n    scheme: http\n    scrapeTimeout: 30s\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    interval: 30s\n    port: telemetry\n    scheme: http\n  jobLabel: app.kubernetes.io/name\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n```\n\n`kubeStateMetrics-prometheusRule.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    prometheus: prometheus\n    role: alert-rules\n  name: kube-state-metrics-rules\n  namespace: monitoring\nspec:\n  groups:\n  - name: kube-state-metrics\n    rules:\n    - alert: 列表操作错误\n      annotations:\n        description: kube-state-metrics 在列表操作中出现错误的频率过高，这可能会导致它无法正确或根本无法暴露 Kubernetes 对象的指标。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors\n        summary: kube-state-metrics 在列表操作中出现错误。\n      expr: |\n        (sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) by (cluster)\n          /\n        sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m])) by (cluster))\n        > 0.01\n      for: 15m\n      labels:\n        severity: critical\n    - alert: 监控操作错误\n      annotations:\n        description: kube-state-metrics 在监控操作中出现错误的频率过高，这可能会导致它无法正确或根本无法暴露 Kubernetes 对象的指标。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors\n        summary: kube-state-metrics 在监控操作中出现错误。\n      expr: |\n        (sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) by (cluster)\n          /\n        sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m])) by (cluster))\n        > 0.01\n      for: 15m\n      labels:\n        severity: critical\n    - alert: 分片配置不一致\n      annotations:\n        description: kube-state-metrics 的分片配置不一致，可能会导致某些 Kubernetes 对象被重复暴露或完全不暴露。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch\n        summary: kube-state-metrics 的分片配置不一致。\n      expr: |\n        stdvar (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) by (cluster) != 0\n      for: 15m\n      labels:\n        severity: critical\n    - alert: 分片缺失\n      annotations:\n        description: kube-state-metrics 的分片缺失，某些 Kubernetes 对象未被暴露。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing\n        summary: kube-state-metrics 的分片缺失。\n      expr: |\n        2^max(kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) by (cluster) - 1\n          -\n        sum( 2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job=\"kube-state-metrics\"}) ) by (cluster)\n        != 0\n      for: 15m\n      labels:\n        severity: critical\n```","tags":["Kubernetes","Prometheus"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（五）：监控主机组件node-exporter的使用","url":"/2025/07/09/【云原生】云原生监控方案（五）：监控主机组件node-exporter的使用/","content":"\n\n`rbac.yaml`\n\n组件使用的集群权限\n```\napiVersion: v1\nautomountServiceAccountToken: false\nkind: ServiceAccount\nmetadata:\n  name: node-exporter\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-exporter\nrules:\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-exporter\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: node-exporter\nsubjects:\n- kind: ServiceAccount\n  name: node-exporter\n  namespace: monitoring\n```\n\n`node-exporter.yaml`\n```\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: node-exporter\n      labels:\n        app: node-exporter\n    spec:\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --no-collector.btrfs\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/k3s/containerd/.+|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*|[a-f0-9]{15})$\n        - --collector.netdev.device-exclude=^(veth.*|[a-f0-9]{15})$\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        name: node-exporter\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - SYS_TIME\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      - args:\n        - --secure-listen-address=[$(IP)]:9100\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:9100/\n        env:\n        - name: IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        image: quay.io/brancz/kube-rbac-proxy:v0.18.1\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: https\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      priorityClassName: system-cluster-critical\n      securityContext:\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate\n```\n`service.yaml`\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: https\n    port: 9100\n    targetPort: https\n  selector:\n    app: node-exporter\n```\n`servicemonitor.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    interval: 15s\n    port: https\n    relabelings:\n    - action: replace\n      regex: (.*)\n      replacement: $1\n      sourceLabels:\n      - __meta_kubernetes_pod_node_name\n      targetLabel: instance\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  jobLabel: app\n  selector:\n    matchLabels:\n      app: node-exporter\n```\n\n\n示例规则\n\n`nodeExporter-prometheusRule.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    prometheus: prometheus\n    role: alert-rules\n  name: node-exporter-rules\n  namespace: monitoring\nspec:\n  groups:\n  - name: node-exporter\n    rules:\n    - alert: 文件系统空间即将耗尽\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用空间，并且正在迅速填满。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup\n        summary: 文件系统预计在未来 24 小时内耗尽空间。\n      expr: |\n        (\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 15\n        and\n          predict_linear(node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"}[6h], 24*60*60) < 0\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: warning\n    - alert: 文件系统空间即将耗尽\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用空间，并且正在迅速填满。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup\n        summary: 文件系统预计在未来 4 小时内耗尽空间。\n      expr: |\n        (\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 10\n        and\n          predict_linear(node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"}[6h], 4*60*60) < 0\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: critical\n    - alert: 文件系统空间不足\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用空间。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace\n        summary: 文件系统可用空间不足 5%。\n      expr: |\n        (\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 5\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 30m\n      labels:\n        severity: warning\n    - alert: 文件系统空间不足\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用空间。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace\n        summary: 文件系统可用空间不足 3%。\n      expr: |\n        (\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 3\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 30m\n      labels:\n        severity: critical\n    - alert: 文件系统 inode 即将耗尽\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用 inode，并且正在迅速填满。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup\n        summary: 文件系统预计在未来 24 小时内耗尽 inode。\n      expr: |\n        (\n          node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 40\n        and\n          predict_linear(node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"}[6h], 24*60*60) < 0\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: warning\n    - alert: 文件系统 inode 即将耗尽\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用 inode，并且正在迅速填满。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup\n        summary: 文件系统预计在未来 4 小时内耗尽 inode。\n      expr: |\n        (\n          node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 20\n        and\n          predict_linear(node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"}[6h], 4*60*60) < 0\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: critical\n    - alert: 文件系统 inode 不足\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用 inode。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles\n        summary: 文件系统可用 inode 不足 5%。\n      expr: |\n        (\n          node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 5\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: warning\n    - alert: 文件系统 inode 不足\n      annotations:\n        description: $labels.device 设备上的文件系统，挂载在 $labels.mountpoint，位于 $labels.instance，仅剩下 $value% 的可用 inode。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles\n        summary: 文件系统可用 inode 不足 3%。\n      expr: |\n        (\n          node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} * 100 < 3\n        and\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!=\"\"} == 0\n        )\n      for: 1h\n      labels:\n        severity: critical\n    - alert: 节点网络接收错误\n      annotations:\n        description: $labels.instance 接口 $labels.device 在过去两分钟内发生了 $value 次接收错误。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs\n        summary: 网络接口报告了大量接收错误。\n      expr: |\n        rate(node_network_receive_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_receive_packets_total{job=\"node-exporter\"}[2m]) > 0.01\n      for: 1h\n      labels:\n        severity: warning\n    - alert: 节点网络发送错误\n      annotations:\n        description: $labels.instance 接口 $labels.device 在过去两分钟内发生了 $value 次发送错误。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs\n        summary: 网络接口报告了大量发送错误。\n      expr: |\n        rate(node_network_transmit_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_transmit_packets_total{job=\"node-exporter\"}[2m]) > 0.01\n      for: 1h\n      labels:\n        severity: warning\n    - alert: 节点 conntrack 条目使用率过高\n      annotations:\n        description: 已使用 $value% 的 conntrack 条目。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused\n        summary: conntrack 条目数量接近限制。\n      expr: |\n        (node_nf_conntrack_entries{job=\"node-exporter\"} / node_nf_conntrack_entries_limit) > 0.75\n      labels:\n        severity: warning\n    - alert: 节点文本文件采集器采集失败\n      annotations:\n        description: Node Exporter 文本文件采集器在 $labels.instance 上采集失败。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror\n        summary: Node Exporter 文本文件采集器采集失败。\n      expr: |\n        node_textfile_scrape_error{job=\"node-exporter\"} == 1\n      labels:\n        severity: warning\n    - alert: 节点时钟偏差检测\n      annotations:\n        description: $labels.instance 的时钟偏差超过 0.05 秒。请确保该主机正确配置了 NTP。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected\n        summary: 检测到时钟偏差。\n      expr: |\n        (\n          node_timex_offset_seconds{job=\"node-exporter\"} > 0.05\n        and\n          deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) >= 0\n        )\n        or\n        (\n          node_timex_offset_seconds{job=\"node-exporter\"} < -0.05\n        and\n          deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) <= 0\n        )\n      for: 10m\n      labels:\n        severity: warning\n    - alert: 节点时钟未同步\n      annotations:\n        description: $labels.instance 的时钟未同步。请确保该主机配置了 NTP。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising\n        summary: 时钟未同步。\n      expr: |\n        min_over_time(node_timex_sync_status{job=\"node-exporter\"}[5m]) == 0\n        and\n        node_timex_maxerror_seconds{job=\"node-exporter\"} >= 16\n      for: 10m\n      labels:\n        severity: warning\n    - alert: 节点 RAID 阵列降级\n      annotations:\n        description: RAID 阵列 $labels.device 在 $labels.instance 上处于降级状态，由于一个或多个磁盘故障。备用磁盘数量不足以自动修复问题。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded\n        summary: RAID 阵列降级。\n      expr: |\n        node_md_disks_required{job=\"node-exporter\",device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\"} - ignoring (state) (node_md_disks{state=\"active\",job=\"node-exporter\",device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\"}) > 0\n      for: 15m\n      labels:\n        severity: critical\n    - alert: 节点 RAID 磁盘故障\n      annotations:\n        description: RAID 阵列在 $labels.instance 上至少有一个设备故障。阵列 $labels.device 需要关注并可能需要更换磁盘。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure\n        summary: RAID 阵列中有设备故障。\n      expr: |\n        node_md_disks{state=\"failed\",job=\"node-exporter\",device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\"} > 0\n      labels:\n        severity: warning\n    - alert: 节点文件描述符限制\n      annotations:\n        description: $labels.instance 的文件描述符限制目前为 $value%。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit\n        summary: 内核即将耗尽文件描述符限制。\n      expr: |\n        (\n          node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 70\n        )\n      for: 15m\n      labels:\n        severity: warning\n    - alert: 节点文件描述符限制\n      annotations:\n        description: $labels.instance 的文件描述符限制目前为 $value%。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit\n        summary: 内核即将耗尽文件描述符限制。\n      expr: |\n        (\n          node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 90\n        )\n      for: 15m\n      labels:\n        severity: critical\n    - alert: 节点 CPU 使用率过高\n      annotations:\n        description: |\n          $labels.instance 的 CPU 使用率在过去 15 分钟内一直高于 90%，目前为 $value%。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage\n        summary: CPU 使用率过高。\n      expr: |\n        sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job=\"node-exporter\", mode!=\"idle\"}[2m]))) * 100 > 90\n      for: 15m\n      labels:\n        severity: info\n    - alert: 节点系统饱和\n      annotations:\n        description: |\n          $labels.instance 的系统负载每核在过去 15 分钟内一直高于 2，目前为 $value。\n          这可能表明该实例资源饱和，可能导致实例变得无响应。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation\n        summary: 系统饱和，每核负载过高。\n      expr: |\n        node_load1{job=\"node-exporter\"}\n        / count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\", mode=\"idle\"}) > 2\n      for: 15m\n      labels:\n        severity: warning\n    - alert: 节点内存重大页面错误\n      annotations:\n        description: |\n          $labels.instance 的内存重大页面错误发生率非常高，在过去 15 分钟内每秒发生 500 次重大页面错误，目前为 $value。\n          请检查该实例是否有足够的内存可用。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults\n        summary: 内存重大页面错误发生率非常高。\n      expr: |\n        rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m]) > 500\n      for: 15m\n      labels:\n        severity: warning\n    - alert: 节点内存使用率过高\n      annotations:\n        description: |\n          $labels.instance 的内存已满，在过去 15 分钟内一直高于 90%，目前为 $value%。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization\n        summary: 主机内存不足。\n      expr: |\n        100 - (node_memory_MemAvailable_bytes{job=\"node-exporter\"} / node_memory_MemTotal_bytes{job=\"node-exporter\"} * 100) > 90\n      for: 15m\n      labels:\n        severity: warning\n    - alert: 节点磁盘 IO 饱和\n      annotations:\n        description: |\n          $labels.instance 的磁盘 IO 队列（aqu-sq）在 $labels.device 上非常高，在过去 30 分钟内一直高于 10，目前为 $value。\n          此症状可能表明磁盘饱和。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation\n        summary: 磁盘 IO 队列过高。\n      expr: |\n        rate(node_disk_io_time_weighted_seconds_total{job=\"node-exporter\", device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\"}[5m]) > 10\n      for: 30m\n      labels:\n        severity: warning\n    - alert: 节点 systemd 服务失败\n      annotations:\n        description: systemd 服务 $labels.name 在 $labels.instance 上进入失败状态。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed\n        summary: systemd 服务进入失败状态。\n      expr: |\n        node_systemd_unit_state{job=\"node-exporter\", state=\"failed\"} == 1\n      for: 5m\n      labels:\n        severity: warning\n    - alert: 节点绑定接口降级\n      annotations:\n        description: 绑定接口 $labels.master 在 $labels.instance 上由于一个或多个从设备故障而处于降级状态。\n        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded\n        summary: 绑定接口降级。\n      expr: |\n        (node_bonding_slaves - node_bonding_active) != 0\n      for: 5m\n      labels:\n        severity: warning\n```\n\n","tags":["Kubernetes","Prometheus"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（四）：使用podMonitor监控微服务","url":"/2025/07/09/【云原生】云原生监控方案（四）：使用podMonitor监控微服务/","content":"\n### 介绍\n上一篇使用了ServiceMonitor创建监控指标：[原文地址](./【云原生】云原生监控方案（三）：使用serviceMonitor监控微服务.md)\n\n对于没有service的pod监控，可以使用PodMonitor告诉Prometheus指标的配置\n\n### 部署微服务\n\n采用Prometheus官方提供的基于golang编写的微服务示例\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        jobname: testpodmonitor\n        app: example-app\n    spec:\n      containers:\n      - name: example-app\n        image: quay.io/brancz/prometheus-example-app:v0.5.0\n        ports:\n        - name: web\n          containerPort: 8080\n\n```\n\n\n\n### 部署PodMonitor\n告诉Prometheus基于pod获取\n\n官方api参考：https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.PodMonitor\n\n```\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: example-app   # PodMonitor 的名称\nspec:\n  podMetricsEndpoints:\n  - port: web       # 抓取指标的端口名称（对应pod中的端口名称）\n    interval: 15s   # 抓取指标的间隔时间\n    path: /metrics  # 指定自定义路径\n    scheme: http    # 使用 HTTP 协议抓取指标\n    tlsConfig:\n      insecureSkipVerify: true # 跳过 TLS 证书验证（用于自签名证书场景）\n  selector:\n    matchLabels:\n      app: example-app\n  jobLabel: jobname  # 用于 Prometheus 中 job 标签的值，取自Pod的该标签，不指定时，默认为PodMonitor 对象的命名空间和名称（例如 <namespace>/<name>）\n  selector:\n    matchLabels:\n      app: example-app  # 选择带有 app=example-app 标签的 Pod\n```\n\n访问：http://nodeip:30900/targets\n\n查看podmonitor已经将指标获取到,并且job名称为pod标签中设置的名称\n\n![](../images/Prometheus-targer-podmonitor-example-app.png)","tags":["Kubernetes","Prometheus"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（三）：使用serviceMonitor监控微服务","url":"/2025/07/09/【云原生】云原生监控方案（三）：使用serviceMonitor监控微服务/","content":"\n### 介绍\n上一篇基于Prometheus-Operator部署了Prometheus、AlertManager：[原文地址](./【云原生】云原生监控方案（二）：基于Prometheus-Operator部署Prometheus、AlertManager.md)\n\n### 部署微服务\n\n采用Prometheus官方提供的基于golang编写的微服务示例\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-app\n        image: quay.io/brancz/prometheus-example-app:v0.5.0\n        ports:\n        - name: web\n          containerPort: 8080\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: example-app\n    jobname: app # servicemonitor 使用\nspec:\n  selector:\n    app: example-app\n  ports:\n  - name: web\n    port: 8080\n    nodePort: 30080 # 外部端口\n  type: NodePort  # 演示用，实际微服务为ClusterIP\n```\n访问：http://nodeip:30080/metrics\n\n可以看到已经将微服务本身的监控信息暴露出去\n\n### 部署ServiceMonitor\n告诉Prometheus基于service获取\n\n官方api参考：https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.ServiceMonitor\n\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: example-app  # ServiceMonitor 的标签，用于标识和选择\n  name: example-app   # ServiceMonitor 的名称\nspec:\n  endpoints:\n  - port: web       # 抓取指标的端口名称（对应 Service 中定义的端口名称）\n    interval: 15s   # 抓取指标的间隔时间\n    path: /metrics  # 指定自定义路径\n    scheme: http    # 使用 HTTP 协议抓取指标\n    tlsConfig:\n      insecureSkipVerify: true # 跳过 TLS 证书验证（用于自签名证书场景）\n  selector:\n    matchLabels:\n      app: example-app\n  jobLabel: jobname  # 用于 Prometheus 中 job 标签的值，取自 Service 的该标签，不指定时，默认使用service名称\n  selector:\n    matchLabels:\n      app: example-app  # 选择带有 app=example-app 标签的 Service\n```\n\n访问：http://nodeip:30900/targets\n\n查看servicemonitor已经将指标获取到,并且job名称为service标签中设置的名称\n\n![](../images/Prometheus-targer-servicemonitor-example-app.png)","tags":["Kubernetes","Prometheus"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（二）：基于Prometheus-Operator部署Prometheus、AlertManager","url":"/2025/07/09/【云原生】云原生监控方案（二）：基于Prometheus-Operator部署Prometheus、AlertManager/","content":"\n## Prometheus社区提供了在k8s部署的三种方式：\n1. 部署Prometheus-Operator，自定义部署（yaml方式，本文）\n2. 通过kube-Prometheus部署通用监控环境\n3. 通过kube-prometheus-stack部署（helm方式，简化第二种方式）\n\n上文使用了第二种方式快速部署了Prometheus集成监控方案\n\n优点是快速和全面，但对于定制化需求不能得到满足，内置的规则和模板也比较杂\n\n本文和后续都使用Prometheus-Operator方式集成，精简和高度定制一套属于自己的监控方案\n\n\n## 开始部署\n\n### 兼容性\nPrometheus-Operator不再像kube-Prometheus需要关注版本信息\n\n由于使用了 apiextensions.k8s.io/v1 CustomResourceDefinitions，prometheus-operator 需要 Kubernetes >= v1.16.0。\n\n\n### 部署Prometheus-Operator\n\n官方文档：https://prometheus-operator.dev/docs/getting-started/installation\n\n获取最新版\n```\nLATEST=$(curl -s https://api.github.com/repos/prometheus-operator/prometheus-operator/releases/latest | jq -cr .tag_name)\n```\n下载\n```\ncurl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/${LATEST}/bundle.yaml \n# 这将下载一个bundle.yaml\n```\n\n```\n# 创建命名空间\nkubectl create ns monitoring\n\n# 修改命名空间\nsed -i 's/namespace: default/namespace: monitoring/g' bundle.yaml\n# 查看 grep -r 'namespace: monitoring' bundle.yaml\n\n# 安装operator\nkubectl create -f bundle.yaml \n```\n\n\n\n## 部署Prometheus\n\n`rbac.yaml`\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\n```\n\n`prometheus.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 3  # Prometheus 实例副本数，设置为 3 表示高可用部署\n  image: quay.io/prometheus/prometheus:v3.4.2  # 使用的 Prometheus 镜像版本\n  version: v3.4.2  # 指定 Prometheus 版本，供 Operator 使用\n  serviceAccountName: prometheus  # 指定使用的 ServiceAccount 名称\n  ruleSelector:\n    matchLabels:\n      role: alert-rules  # 选择带有 role=alert-rules 标签的 PrometheusRule 资源\n  scrapeInterval: 5s  # 抓取指标的时间间隔\n  alerting:\n    alertmanagers:\n    - apiVersion: v2\n      name: alertmanager  # 关联的 Alertmanager 名称\n      namespace: monitoring  # Alertmanager 所在命名空间\n      port: web  # 使用的端口名称，与service的ports下name对应\n  enableFeatures: []  # 启用的 Prometheus 特性列表，默认不启用额外特性\n  externalLabels: {}  # 添加在每条时间序列上的外部标签\n  nodeSelector:\n    kubernetes.io/os: linux  # 指定调度到运行 Linux 系统的节点\n  podMonitorNamespaceSelector: {}  # 选择哪些命名空间下的 PodMonitor 生效（空表示所有）\n  podMonitorSelector: {}  # 选择哪些 PodMonitor 生效（空表示所有）\n  probeNamespaceSelector: {}  # 选择哪些命名空间下的 Probe 生效（空表示所有）\n  probeSelector: {}  # 选择哪些 Probe 生效（空表示所有）\n  ruleNamespaceSelector: {}  # 选择哪些命名空间下的 PrometheusRule 生效（空表示所有）\n  ruleSelector: {}  # 选择哪些 PrometheusRule 生效（空表示所有）\n  scrapeConfigNamespaceSelector: {}  # 选择哪些命名空间下的 ScrapeConfig 生效（空表示所有）\n  scrapeConfigSelector: {}  # 选择哪些 ScrapeConfig 生效（空表示所有）\n  securityContext:\n    fsGroup: 2000  # 设置 Pod 的文件系统组 ID\n    runAsNonRoot: true  # 禁止以 root 用户运行\n    runAsUser: 1000  # 指定运行用户 ID\n  serviceMonitorNamespaceSelector: {}  # 选择哪些命名空间下的 ServiceMonitor 生效（空表示所有）\n  serviceMonitorSelector: {}  # 选择哪些 ServiceMonitor 生效（空表示所有）\n  resources:\n    requests:\n      memory: \"400Mi\"  # 请求的内存资源\n      cpu: \"500m\"      # 请求的 CPU 资源（500m = 0.5 核）\n  storage:\n    volumeClaimTemplate:\n      spec:\n        storageClassName: \"nfs-storage\"  # 使用的 StorageClass 名称\n        accessModes: [ \"ReadWriteOnce\" ]  # 访问模式，ReadWriteOnce 表示单节点读写\n        resources:\n          requests:\n            storage: 5Gi  # 请求的存储大小\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  type: NodePort\n  ports:\n  - name: web\n    nodePort: 30900\n    port: 9090\n    protocol: TCP\n    targetPort: web\n  - name: reloader-web\n    port: 8080\n    targetPort: reloader-web\n  selector:\n    prometheus: prometheus\n\n---\n# Prometheus自身指标\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  endpoints:\n  - interval: 30s\n    port: web\n  - interval: 30s\n    port: reloader-web\n  selector:\n    matchLabels:\n      app: prometheus\n```\n\n\n## 部署AlertManager\n\n`alertmanager.yaml`\n```\napiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: alertmanager\n  namespace: monitoring\nspec:\n  replicas: 1\n  version: 0.28.1\n  image: quay.io/prometheus/alertmanager:v0.28.1\n  nodeSelector:\n    kubernetes.io/os: linux\n  podMetadata:\n    labels:\n      app: alertmanager\n  resources:\n    limits:\n      cpu: 100m\n      memory: 100Mi\n    requests:\n      cpu: 4m\n      memory: 100Mi\n  #alertmanagerConfiguration: # alertmanager全局配置\n  #  name: alertroute\n  alertmanagerConfigSelector: # alertmanager配置\n    matchLabels:\n      alert: alert-config\n  secrets: []\n  securityContext:\n    fsGroup: 2000\n    runAsNonRoot: true\n    runAsUser: 1000\n  storage:\n    volumeClaimTemplate:\n      spec:\n        storageClassName: nfs-storage\n        resources:\n          requests:\n            storage: 5Gi\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: alertmanager\n  name: alertmanager\n  namespace: monitoring\nspec:\n  ports:\n  - name: web\n    port: 9093\n    targetPort: web\n    nodePort: 30903\n  - name: reloader-web\n    port: 8080\n    targetPort: reloader-web\n    nodePort: 30980\n  selector:\n    app: alertmanager\n  sessionAffinity: ClientIP\n  type: NodePort\n\n---\n# alertmanager暴露指标给alertmanager\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: alertmanager\n  namespace: monitoring\nspec:\n  endpoints:\n  - interval: 30s\n    port: web\n  - interval: 30s\n    port: reloader-web\n  selector:\n    matchLabels:\n      app: alertmanager\n\n---\n# alertmanager配置\napiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: config-example\n  namespace: monitoring\n  labels:\n    alert: alert-config\nspec:\n  route:\n    groupBy: ['job']\n    groupWait: 30s\n    groupInterval: 5m\n    repeatInterval: 12h\n    receiver: 'webhook'\n  receivers:\n  - name: 'webhook'\n    webhookConfigs:\n    - url: 'http://example.com/'\n```\n\n查看pod\n```\nkubectl get pod -n monitoring\n\nNAME                                   READY   STATUS    RESTARTS   AGE\nalertmanager-alertmanager-0            2/2     Running   0          3m6s\nprometheus-operator-7587858ff6-pcncq   1/1     Running   0          31m\nprometheus-prometheus-0                2/2     Running   0          27m\nprometheus-prometheus-1                2/2     Running   0          27m\nprometheus-prometheus-2                2/2     Running   0          27m\n```\n\n访问http://nodeip:30900/targets\n\n验证服务可以访问，并且已经通过`serviceMonitor`获取到alertmanager指标（下一篇详解）","tags":["Kubernetes","Prometheus","AlertManager"],"categories":["云原生"]},{"title":"【云原生】云原生监控方案（一）：基于kube-Prometheus部署Prometheus集成监控","url":"/2025/07/09/【云原生】云原生监控方案（一）：基于kube-Prometheus部署Prometheus集成监控/","content":"\n## Prometheus社区提供了在k8s部署的三种方式：\n1. 部署Prometheus-Operator，自定义部署\n2. 通过kube-Prometheus部署通用监控环境（yaml方式，本文）\n3. 通过kube-prometheus-stack部署（helm方式，简化第二种方式）\n\n后两者都是基于第一种Prometheus-Operator的扩展版，内置了监控规则和模板\n\n本文先使用第二种方式快速部署\n\n\n\n## 开始部署\n\n\n### 版本信息\n| kube-prometheus stack                                                                      | Kubernetes 1.27 | Kubernetes 1.28 | Kubernetes 1.29 | Kubernetes 1.30 | Kubernetes 1.31 | Kubernetes 1.32 | Kubernetes 1.33 |\n|--------------------------------------------------------------------------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| [`release-0.13`](https://github.com/prometheus-operator/kube-prometheus/tree/release-0.13) | ✔               | ✔               | x               | x               | x               | x               | x               |\n| [`release-0.14`](https://github.com/prometheus-operator/kube-prometheus/tree/release-0.14) | x               | x               | ✔               | ✔               | ✔               | x               | x               |\n| [`release-0.15`](https://github.com/prometheus-operator/kube-prometheus/tree/release-0.15) | x               | x               | x               | x               | ✔               | ✔               | ✔               |\n| [`main`](https://github.com/prometheus-operator/kube-prometheus/tree/main)                 | x               | x               | x               | x               | ✔               | ✔               | ✔               |\n\n\n### 拉取代码\n\n手动下载：[下载地址](https://github.com/prometheus-operator/kube-prometheus/releases)\n\n```\ngit clone https://github.com/prometheus-operator/kube-prometheus.git -b release-0.14\ncd kube-prometheus/\n```\n\n### 部署kube-Prometheus\n\n```\nkubectl apply --server-side -f manifests/setup\n# 如果是1.22以前的版本，使用kubectl create -f manifests/setup\n```\n\n#### 部署Prometheus、alertmanager、grafana、kube-state-metrics、Prometheus-adapter、blackbox-exporter、node-exporter\n\n这是kube-Prometheus基于Prometheus-operator为通用k8s环境提供的一整套监控方案\n\n这整套方案都配置了PrometheusRule告警规则\n\n并且kube-Prometheus为grafana内置了多个dashboard\n\n```\nkubectl apply -f manifests/\n```\n\n---\n\n### 镜像问题\n\n镜像查询：https://docker.aityp.com/\n\n#### 打tag方式\n\n```\ncrictl pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.13.0\ncrictl pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.12.0\n\n# 打tag\nctr -n k8s.io i tag swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.13.0 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.13.0\nctr -n k8s.io i tag swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.12.0 registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.12.0\n```\n#### yaml修改镜像地址\n\n进`kube-prometheus/manifests`对应的yaml文件中修改即可\n\n---\n\n### 外部访问\n\n这套方案应用了networkPolicy，取消即可\n```\nkubectl delete -f prometheus-networkPolicy.yaml -f grafana-networkPolicy.yaml -f alertmanager-networkPolicy.yaml\n```\n修改service\n```\nkubectl edit svc -n monitoring prometheus-k8s\n```\n\n```\ntype: ClusterIP\n# 改为\ntype: NodePort\n```\n\n---\n\n### 持久化存储\n#### Prometheus\n修改`kube-prometheus/manifests/prometheus-prometheus.yaml`文件\n```\n  storage:\n    volumeClaimTemplate:\n      spec:\n        storageClassName: nfs-storage # 实际sc名称\n        resources:\n          requests:\n            storage: 50Gi # 可根据需要调整\n```\n#### Alertmanager\n与Prometheus一样\n\n修改`kube-prometheus/manifests/alertmanager-alertmanager.yaml`文件\n```\n  storage:\n    volumeClaimTemplate:\n      spec:\n        storageClassName: nfs-storage # 实际sc名称\n        resources:\n          requests:\n            storage: 50Gi # 可根据需要调整\n```\n\n#### grafana\n修改`kube-prometheus/manifests/grafana-deployment.yaml`文件\n```\n      volumes:\n      - emptyDir: {}\n        name: grafana-storage\n# 改为\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana\n```\n\n底部新增\n```\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  storageClassName: nfs-storage # 实际sc名称\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi  # 可根据需要调整\n```\n\n应用更改\n```\nkubectl apply -f prometheus-prometheus.yaml\nkubectl apply -f alertmanager-alertmanager.yaml\nkubectl apply -f grafana-deployment.yaml\n```","tags":["Kubernetes","Prometheus"],"categories":["云原生"]},{"title":"【云原生】日志收集方案（一）：基于ECK部署生产级ElasticSearch + Kibana + FileBeat","url":"/2025/06/30/【云原生】日志收集方案（一）：基于ECK部署生产级ElasticSearch + Kibana + FileBeat/","content":"\n## 介绍\n\n官方文档：[*Deploy ECK in your Kubernetes cluster*](https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-deploy-eck.html)\n\n### Beats成员\n\n| 官方 Beat        | 采集对象         | 核心优势                                                            |\n| -------------- | ------------ | --------------------------------------------------------------- |\n| **Filebeat**   | 日志文件         | 极低的 CPU/内存占用；内建“背压感知”与“断点续传”；支持多行合并、容器日志、模块一键解析（Nginx、MySQL 等）。 |\n| **Metricbeat** | 系统与服务指标      | 100+ 现成模块（CPU、内存、Redis、Kafka…）；支持指标自动发现与维度标签；可按周期分片抓取，避免雪崩。     |\n| **Packetbeat** | 网络流量/协议      | 零侵入地捕获 L7 协议（HTTP、DNS、MySQL、PG…），实时生成事务级日志；适合 APM 与故障定位。        |\n| **Winlogbeat** | Windows 事件日志 | 原生解析 EVTX；支持事件 ID 过滤、通道多路复用、域控级批量部署；资源消耗远低于 WMI。                |\n| **Auditbeat**  | 审计/安全事件      | 监控文件完整性、用户登录、进程启动、SELinux/AppArmor 事件；可与 SIEM 规则直接对接。           |\n| **Heartbeat**  | 运行状态/可用性     | 轻量级“黑盒探针”，支持 HTTP、TCP、ICMP、SSL 证书到期检测；可输出 Uptime SLA 与告警。       |\n\n\n### **部署ECK crd**\n\n```\nkubectl create -f https://download.elastic.co/downloads/eck/2.13.0/crds.yaml\n```\n\n### **部署Operator**\n\n```\nkubectl apply -f https://download.elastic.co/downloads/eck/2.13.0/operator.yaml\n```\n\n### **部署ElasticSearch集群**\n\n```\ncat > elasticsearch.yaml << EOF\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\n  name: elasticsearch\nspec:\n  version: 8.14.1\n  nodeSets:\n  - name: master\n    count: 3\n    config:\n      node.store.allow_mmap: false\n    podTemplate:\n      spec:\n        initContainers:\n        - name: sysctl\n          securityContext:\n            privileged: true\n          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']\n    volumeClaimTemplates:\n    - metadata:\n        name: elasticsearch-data\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 200Gi\n        storageClassName: nfs-storage\nEOF\n```\n\n```\nkubectl apply -f elasticsearch.yaml\n```\n\n#### 获取密码\n\n```\nkubectl get secret elasticsearch-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo\n```\n\n### **部署kibana**\n\n```\ncat > kibana.yaml << EOF\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\n  name: kibana\nspec:\n  version: 8.14.1\n  count: 1\n  elasticsearchRef:\n    name: elasticsearch\n  podTemplate:\n    spec:\n      containers:\n      - name: kibana\n        env:\n        - name: I18N_LOCALE\n          value: \"zh-CN\"\nEOF\n```\n\n```\nkubectl apply -f kibana.yaml\n```\n\n#### 访问\n\n修改svc为NodePort或LoadBalancer\n\n账号：elastic\n\n密码：`kubectl get secret elasticsearch-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo`\n\n### **部署Beats（DaemonSet模式）**\n\n```\ncat > beats.yaml << EOF\napiVersion: beat.k8s.elastic.co/v1beta1\nkind: Beat\nmetadata:\n  name: filebeat\nspec:\n  type: filebeat\n  version: 8.14.1\n  elasticsearchRef:\n    name: elasticsearch\n  config:\n    filebeat.inputs:\n    - type: container\n      paths:\n      - /var/log/containers/*.log\n  daemonSet:\n    podTemplate:\n      spec:\n        dnsPolicy: ClusterFirstWithHostNet\n        hostNetwork: true\n        securityContext:\n          runAsUser: 0\n        containers:\n        - name: filebeat\n          volumeMounts:\n          - name: varlogcontainers\n            mountPath: /var/log/containers\n          - name: varlogpods\n            mountPath: /var/log/pods\n          - name: varlibdockercontainers\n            mountPath: /var/lib/docker/containers\n        volumes:\n        - name: varlogcontainers\n          hostPath:\n            path: /var/log/containers\n        - name: varlogpods\n          hostPath:\n            path: /var/log/pods\n        - name: varlibdockercontainers\n          hostPath:\n            path: /var/lib/docker/containers\nEOF\n```\n\n```\nkubectl apply -f beats.yaml\n```\n\n#### 配置索引\n\n![](/images/image-xgfq.png)\n\n![](/images/image-hsrd.png)\n\n### ![](/images/image-fjjo.png)\n","tags":["Kubernetes","ECK","Elasticsearch"],"categories":["云原生"]},{"title":"【笔记】基于github Page创建hexo个人博客","url":"/2025/05/27/【笔记】基于github page创建hexo个人博客/","content":"\n### 安装环境\n\n- Node.js：https://nodejs.org/zh-cn\n- Git：https://git-scm.com/downloads\n\n\n### 链接Github\n\n```\ngit config --global user.name \"GitHub 用户名\"\ngit config --global user.email \"GitHub 邮箱\"\n```\n\n### 创建SSH密钥\n```\nssh-keygen -t rsa -C \"GitHub 邮箱\"\n```\n\n### 添加密匙\n\n进入 [C:\\Users\\用户名\\.ssh] 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 id_rsa.pub 文件并复制里面的内容。\n\n登陆 GitHub ，进入 Settings 页面，选择左边栏的 SSH and GPG keys，点击 New SSH key。\n\nTitle 随便取个名字，粘贴复制的 id_rsa.pub 内容到 Key 中，点击 Add SSH key 完成添加。\n\n### 验证连接\n\n打开 Git Bash，输入 ssh -T git@github.com 出现 “Are you sure……”，输入 yes 回车确认。\n\n### 创建仓库\n\n填写仓库名，格式必须为`<用户名>.github.io`，然后点击`Create repository`。\n\n\n### 安装 Hexo\n使用 npm 一键安装 Hexo 博客程序\n```\nnpm install -g hexo-cli\n```\nMac 用户需要管理员权限（sudo）\n```\nsudo npm install -g hexo-cli\n```\n\n\n### 初始化hexo\n创建blog文件夹\n进入并输入以下命令完成初始化\n```\nhexo init      # 初始化\nnpm install    # 安装组件\n```\n\n### 启动本地服务器进行预览\n\n```\nhexo g   # 生成页面\nhexo s   # 启动预览,访问 http://localhost:4000\n```\n\n### 使用\n\n#### 创建新页面\n```\nhexo new [layout] <title>\n```\n#### 写作\n`source/_posts`目录下创建markdown文章\n\n了解更多：https://hexo.io/zh-cn/docs/writing\n\n### 修改主题\n\n本博客主题：[hexo-theme-Anatole2](https://github.com/sreok/hexo-theme-Anatole2)，文档在README.md\n官方主题：https://hexo.io/themes/\n\n### 部署 Hexo 到 GitHub Pages\n安装 hexo-deployer-git\n```\nnpm install hexo-deployer-git --save\n```\n然后修改 _config.yml 文件末尾的 Deployment 部分，修改成如下：\n```\ndeploy:\n  type: 'git'\n  repo: 'https://github.com/用户名/用户名.github.io.git'\n  branch: master\n```\n完成后运行` hexo d `将网站上传部署到 GitHub Pages。\n\n### 常见问题\n\n1. npm 下载速度慢\n    临时更换方法：在 npm 安装命令后面加上：\n    ```\n    --registry https://registry.npm.taobao.org \n    ```","tags":["hexo","Github Page"],"categories":["Linux"]},{"title":"【云原生】基于Percona Operator支持全命名空间创建PXC集群（manifest方式）","url":"/2025/02/26/【云原生】基于Percona Operator支持全命名空间创建PXC集群（manifest方式）/","content":"\n官方文档：[通用 Kubernetes 安装 - Percona Operator for MySQL --- Generic Kubernetes installation - Percona Operator for MySQL](https://docs.percona.com/percona-operator-for-mysql/pxc/kubernetes.html)\n\n### **安装Operator**\n\n```\ngit clone -b v1.14.0 https://github.com/percona/percona-xtradb-cluster-operator\ncd percona-xtradb-cluster-operator\n```\n\n#### **方式一：分步安装**\n\n```\nkubectl apply -f deploy/crd.yaml\n```\n\n```\nkubectl apply -f deploy/cw-operator.yaml -n kube-system\n```\n\n```\nsed -i \"s/pxc-operator/kube-system/g\" deploy/cw-rbac.yaml\nsed -i \"s/imagePullPolicy: Always/imagePullPolicy: IfNotPresent/g\" deploy/cw-operator.yaml\n```\n\n```\nkubectl apply -f deploy/cw-rbac.yaml -n kube-system\n```\n\n```\nkubectl apply -f deploy/secrets.yaml -n kube-system\n```\n\n#### **方式二：简易安装**\n\n```\nsed -i \"s/pxc-operator/kube-system/g\" deploy/cw-bundle.yaml\nsed -i \"s/imagePullPolicy: Always/imagePullPolicy: IfNotPresent/g\" deploy/cw-bundle.yaml\n```\n\n```\nkubectl apply -f deploy/crd.yaml\nkubectl apply -f deploy/cw-bundle.yaml -n kube-system\n```\n\n### **安装pxc集群**\n\n```\nvim deploy/cr.yaml\n```\n\n```\napiVersion: pxc.percona.com/v1\nkind: PerconaXtraDBCluster\nmetadata:\n  name: cluster1\n  finalizers:\n    - delete-pxc-pods-in-order\nspec:\n  crVersion: 1.14.0\n  allowUnsafeConfigurations: false\n  updateStrategy: SmartUpdate\n  upgradeOptions:\n    versionServiceEndpoint: https://check.percona.com\n    apply: disabled\n    schedule: \"0 4 * * *\"\n  pxc:\n    size: 3\n    image: percona/percona-xtradb-cluster:8.0.35-27.1\n    imagePullPolicy: IfNotPresent\n    autoRecovery: true\n    resources:\n      requests:\n        memory: 1G\n        cpu: 600m\n    affinity:\n      antiAffinityTopologyKey: \"kubernetes.io/hostname\"\n    podDisruptionBudget:\n      maxUnavailable: 1\n    volumeSpec:\n      persistentVolumeClaim:\n        storageClassName: nfs-storage # 改为实际storageclass\n        resources:\n          requests:\n            storage: 6G\n    gracePeriod: 600\n  haproxy:\n    enabled: true\n    size: 3\n    image: percona/percona-xtradb-cluster-operator:1.14.0-haproxy\n    imagePullPolicy: IfNotPresent\n    resources:\n      requests:\n        memory: 1G\n        cpu: 600m\n    affinity:\n      antiAffinityTopologyKey: \"kubernetes.io/hostname\"\n    podDisruptionBudget:\n      maxUnavailable: 1\n    gracePeriod: 30\n  proxysql:\n    enabled: false\n    size: 3\n    image: percona/percona-xtradb-cluster-operator:1.14.0-proxysql\n    imagePullPolicy: IfNotPresent\n    resources:\n      requests:\n        memory: 1G\n        cpu: 600m\n    affinity:\n      antiAffinityTopologyKey: \"kubernetes.io/hostname\"\n    volumeSpec:\n      persistentVolumeClaim:\n        resources:\n          requests:\n            storage: 2G\n    podDisruptionBudget:\n      maxUnavailable: 1\n    gracePeriod: 30\n  logcollector:\n    enabled: true\n    image: percona/percona-xtradb-cluster-operator:1.14.0-logcollector\n    imagePullPolicy: IfNotPresent\n    resources:\n      requests:\n        memory: 100M\n        cpu: 200m\n  pmm:\n    enabled: false\n    image: percona/pmm-client:2.41.1\n    serverHost: monitoring-service\n    resources:\n      requests:\n        memory: 150M\n        cpu: 300m\n  backup:\n    image: percona/percona-xtradb-cluster-operator:1.14.0-pxc8.0-backup-pxb8.0.35\n    pitr:\n      enabled: false\n      storageName: STORAGE-NAME-HERE\n      timeBetweenUploads: 60\n      timeoutSeconds: 60\n    storages:\n      s3-us-west:\n        type: s3\n        verifyTLS: true\n        s3:\n          bucket: S3-BACKUP-BUCKET-NAME-HERE\n          credentialsSecret: my-cluster-name-backup-s3\n          region: us-west-2\n      azure-blob:\n        type: azure\n        azure:\n          credentialsSecret: azure-secret\n          container: test\n      fs-pvc:\n        type: filesystem\n        volume:\n          persistentVolumeClaim:\n            storageClassName: nfs-storage  # 改为实际storageclass\n            accessModes: [ \"ReadWriteOnce\" ]\n            resources:\n              requests:\n                storage: 6G\n    schedule:\n      - name: \"daily-backup\"\n        schedule: \"0 0 * * *\"\n        keep: 5\n        storageName: fs-pvc\n```\n\n```\nkubectl create ns pxc\nkubectl apply -f cr.yaml -n pxc\n```\n\n### **最小化pxc集群**\n\n```\nvim cr-minimal.yaml\n```\n\n```\napiVersion: pxc.percona.com/v1\nkind: PerconaXtraDBCluster\nmetadata:\n  name: minimal-cluster\nspec:\n  crVersion: 1.14.0\n  secretsName: minimal-cluster-secrets\n  allowUnsafeConfigurations: true\n  upgradeOptions:\n    apply: disabled\n    schedule: \"0 4 * * *\"\n  pxc:\n    size: 1\n    image: percona/percona-xtradb-cluster:8.0.35-27.1\n    imagePullPolicy: IfNotPresent\n    volumeSpec:\n      persistentVolumeClaim:\n        storageClassName: nfs-storage\n        resources:\n          requests:\n            storage: 6G\n  haproxy:\n    enabled: true\n    size: 1\n    image: percona/percona-xtradb-cluster-operator:1.14.0-haproxy\n    imagePullPolicy: IfNotPresent\n  logcollector:\n    enabled: true\n    image: percona/percona-xtradb-cluster-operator:1.14.0-logcollector\n    imagePullPolicy: IfNotPresent\n```\n\n```\nkubectl apply -f cr-minimal.yaml\n```\n\n### **卸载**\n\n```\nkubectl delete pxc <pxc集群名称>\n```\n\n```\nkubectl delete -f deploy/cw-bundle.yaml\nkubectl delete -f deploy/crd.yaml\nkubectl delete validatingwebhookconfigurations percona-xtradbcluster-webhook\n```\n","tags":["MySQL","PXC"],"categories":["云原生"]},{"title":"【云原生】基于 PostgreSQL Operator 在 k8s 集群中部署Postgres 集群并启动 PostGIS（kustomize方式）","url":"/2025/02/17/【云原生】基于 PostgreSQL Operator 在 k8s 集群中部署Postgres 集群并启动 PostGIS（kustomize方式）/","content":"\n### 准备\n\n```bash\ngit clone --depth 1 \"https://github.com/CrunchyData/postgres-operator-examples.git\"\ncd postgres-operator-examples\n```\n\n### **安装PGO**\n\n官方地址：<https://access.crunchydata.com/documentation/postgres-operator/latest/quickstart>\n\n```\nkubectl apply -k kustomize/install/namespace\nkubectl apply --server-side -k kustomize/install/default\n```\n\n### **配置集群**\n\n参考地址：<https://www.crunchydata.com/blog/postgis-with-postgresql-operator>\n\n```\nvim kustomize/postgres/postgres.yaml\n```\n\n```\napiVersion: postgres-operator.crunchydata.com/v1beta1\nkind: PostgresCluster\nmetadata:\n  name: hippo\nspec:\n  #image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres:ubi8-16.3-0\n  image : registry.developers.crunchydata.com/crunchydata/crunchy-postgres-gis:ubi8-16.3-3.4-0\n  #image: RELATED_IMAGE_POSTGRES_16_GIS_3.4\n  postgresVersion: 16\n  postGISVersion: \"3.4\"\n  instances:\n    - name: instance1\n      dataVolumeClaimSpec:\n        accessModes:\n        - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: 100Gi\n  users:\n  - name: \"hippo\"\n    options: 'SUPERUSER'\n  backups:\n    pgbackrest:\n      image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:ubi8-2.51-0\n      repos:\n      - name: repo1\n        volume:\n          volumeClaimSpec:\n            accessModes:\n            - \"ReadWriteOnce\"\n            resources:\n              requests:\n                storage: 100Gi\n```\n\n### **安装集群**\n\n官方地址：[Create a Postgres Cluster (](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorials/basic-setup/create-cluster)[crunchydata.com](http://crunchydata.com)[)](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorials/basic-setup/create-cluster)\n\n```\nkubectl apply -k kustomize/postgres\n```\n\n#### **已知问题**\n\n##### **问题1:**\n\nstorageclass调用nfs失败\n\n原因： 没有应用nfs\\_provisioner\n\n解决： yaml文件放在/data/nfsyml目录下。\n\n***\n\n##### **问题2:**\n\nOutput: mount: /var/lib/kubelet/pods/bd8b92ad-3311-4743-8154-0e75e5298fbe/volumes/[kubernetes.io](http://kubernetes.io)\\~nfs/pvc-db9f2bae-9eae-4f5a-91d0-831cf832389c: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.\\<type> helper program.\n\n原因： node节点没有客户端，ubuntu高版本必须安装utils和common centos系统安装utils即可\n\n解决： 节点安装nfs客户端 `apt install nfs-common -y`\n\n***\n\n##### **问题3：**\n\nOutput: mount.nfs: access denied by server while mounting 192.168.0.170:/postgres-data\n\n原因：nfs server配置错误\n\n解决：确认nfs的ip和端口，重启nfs-server\n\n### **配置**\n\n> pgo使用PostgresCluster资源说明，官方地址：[PostgresCluster (](https://access.crunchydata.com/documentation/postgres-operator/latest/references/crd/5.5.x/postgrescluster)[crunchydata.com](http://crunchydata.com)[)](https://access.crunchydata.com/documentation/postgres-operator/latest/references/crd/5.5.x/postgrescluster)\n\n#### **高可用**\n\n```\nkubectl edit postgresclusters.postgres-operator.crunchydata.com -n postgres-operator hippo # 修改replicas = 2\n```\n\n#### **PostGIS**\n\n> 如果安装时制定了gis的地址就不需要这一步\n\n##### **修改镜像**\n\n```\nkubectl edit postgresclusters -n postgres-operator hippo\n```\n\n##### **查看镜像**\n\n```\ncat kustomize/install/manager/manager.yaml\n```\n\n##### **替换spec.image镜像地址**\n\n> 这里使用postgre v16和gis v3.4\n\n```\nregistry.developers.crunchydata.com/crunchydata/crunchy-postgres-gis:ubi8-16.3-3.4-0\n```\n\n### **连接**\n\n#### **直接连接**\n\n```\npsql $(kubectl -n postgres-operator get secrets hippo-pguser-hippo -o go-template='{{.data.uri | base64decode}}')\n```\n\n#### **端口转发连接**\n\n```\nPG_CLUSTER_PRIMARY_POD=$(kubectl get pod \\\n  -n postgres-operator \\\n  -o name \\\n  -l postgres-operator.crunchydata.com/cluster=hippo,postgres-operator.crunchydata.com/role=master)\n​\nkubectl -n postgres-operator port-forward \"${PG_CLUSTER_PRIMARY_POD}\" --address 0.0.0.0 5432:5432\n```\n\n##### **获取密码**\n\n```\nkubectl get secrets -n postgres-operator \"hippo-pguser-hippo\" -o go-template='{{.data.password | base64decode}}'\n```\n\n##### **获取用户**\n\n```\nkubectl get secrets -n postgres-operator \"hippo-pguser-hippo\" -o go-template='{{.data.user | base64decode}}'\n```\n\n##### **获取数据库**\n\n```\nkubectl get secrets -n postgres-operator \"hippo-pguser-hippo\" -o go-template='{{.data.dbname | base64decode}}'\n```\n\n##### 连接数据库\n\n```\npsql -h localhost -U hippo \n```\n\n> 输入密码\n\n### **删除集群**\n\n#### **卸载集群**\n\n```\nkubectl delete -k kustomize/postgres\n```\n\n#### **卸载pgo**\n\n```\nkubectl delete -k kustomize/install/default\nkubectl delete -k kustomize/install/namespace\n```\n\n#### **已知问题**\n\n##### **namespace terminal**\n\n强制删除ns\n\n```\n kubectl proxy --port=8081\n```\n\n```\nkubectl get ns postgres-operator -o json > 1.json\n​\n​\ncurl -k -H \"Content-Type: application/json\" -X PUT --data-binary @1.json http://127.0.0.1:8081/api/v1/namespaces/postgres-operator/finalize\n```\n","tags":["Kubernetes","PostgreSQL"],"categories":["云原生"]},{"title":"【云原生】Kubernetes通过device-plugin调度GPU资源-NVIDIA","url":"/2025/02/03/【云原生】Kubernetes通过device-plugin调度GPU资源-NVIDIA/","content":"\nNVIDIA官方文档：[NVIDIA/k8s-device-plugin：用于 Kubernetes 的 NVIDIA 设备插件](https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#nvidia-device-plugin-for-kubernetes)\n\n### 前提条件\n\n* 安装驱动，原文地址：[Linux Kernel 3.10安装NVIDIA 550.107.02驱动GeForce RTX 2080 Ti - Elijah Blog (](https://sreok.cn/archives/1723448671609)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/1723448671609)\n\n### 安装NVIDIA Container Toolkit\n\n官方地址：<https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html>\n\n```\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \\\n  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\nyum install -y nvidia-container-toolkit\n```\n\n### 更新运行时\n\n#### docker\n\n```\nnvidia-ctk runtime configure --runtime=docker\nsystemctl restart docker\n```\n\n无根模式下运行的docker\n\n```\nnvidia-ctk runtime configure --runtime=docker --config=$HOME/.config/docker/daemon.json\nsystemctl --user restart docker\nnvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place\n```\n\n#### Containerd\n\n```\nnvidia-ctk runtime configure --runtime=containerd\n\nsystemctl daemon-reload\nsystemctl restart containerd\n```\n\n仅用于nerdctl无需配置，直接运行`nerdctl run --gpus=all`\n\n如果以上修改不生效，修改默认运行时\n\n> \\# 编辑 `/etc/containerd/config.toml`\n>\n> 搜索 `plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc`\n>\n> `runtime_type = \"io.containerd.runc.v2\"` 改成 `io.containerd.runtime.v1.linux`\n>\n> 搜索`io.containerd.runtime.v1.linux`\n>\n> `runtime = \"runc\"`改成`runtime = \"nvidia-container-runtime\"`\n\n![](/images/image-mzdz.png)\n\n![](/images/image-vhrk.png)\n\n```\nsystemctl daemon-reload\nsystemctl restart containerd\n```\n\n#### CRI-O\n\n```\nnvidia-ctk runtime configure --runtime=crio\nsystemctl restart crio\n```\n\n### 部署nvidia-device-plugin\n\n```\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.16.2/deployments/static/nvidia-device-plugin.yml\n```\n\n#### 查看GPU所在节点的pod日志\n\n正常情况（将GPU注册到kubelet）：\n\n```\nI0820 08:26:47.926058       1 main.go:317] Retrieving plugins.\nI0820 08:26:51.674618       1 server.go:216] Starting GRPC server for 'nvidia.com/gpu'\nI0820 08:26:51.682183       1 server.go:147] Starting to serve 'nvidia.com/gpu' on /var/lib/kubelet/device-plugins/nvidia-gpu.sock\nI0820 08:26:51.690851       1 server.go:154] Registered device plugin for 'nvidia.com/gpu' with Kubelet\n```\n\n未找到GPU（尝试将nvidia修改为默认运行时）：\n\n```\nI0820 08:26:53.370854       1 main.go:317] Retrieving plugins.\nE0820 08:26:53.370997       1 factory.go:87] Incompatible strategy detected auto\nE0820 08:26:53.371004       1 factory.go:88] If this is a GPU node, did you configure the NVIDIA Container Toolkit?\nE0820 08:26:53.371010       1 factory.go:89] You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin#prerequisites\nE0820 08:26:53.371016       1 factory.go:90] You can learn how to set the runtime at: https://github.com/NVIDIA/k8s-device-plugin#quick-start\nE0820 08:26:53.371021       1 factory.go:91] If this is not a GPU node, you should set up a toleration or nodeSelector to only deploy this plugin on GPU nodes\nI0820 08:26:53.371029       1 main.go:346] No devices found. Waiting indefinitely.\n```\n\n更新运行时后，重启device-plugin daemonset\n\n```\nkubectl rollout restart ds -n kube-system nvidia-device-plugin-daemonset\n```\n\n### 测试\n\n```\n# gpu节点打标签\nkubectl label nodes k8s-dell-r740-worker01 nvidia.com/gpu=true\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: IfNotPresent\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      resources:\n        limits:\n          nvidia.com/gpu: 1\n      command: ['nvidia-smi']\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  #nodeSelector:\n  #  viadia.com/gpu: \"true\"\n```\n\n#### 查看pod日志\n\n![](/images/image-ssaz.png)\n","tags":["Kubernetes","GPU"],"categories":["云原生"]},{"title":"【云原生】K8sGPT + LocalAI 开源自动化诊断工具（GPT4All-j模型）","url":"/2024/12/27/【云原生】K8sGPT + LocalAI 开源自动化诊断工具（GPT4All-j模型）/","content":"\nk8sgpt官方文档：[*In-Cluster Operator - k8sgpt*](https://docs.k8sgpt.ai/getting-started/in-cluster-operator/)\n\nlocalai官方文档：[*Run with Kubernetes | LocalAI documentation*](https://localai.io/basics/kubernetes/)\n\n参考文献：[*K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! | by Tyler | ITNEXT*](https://itnext.io/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)\n\n### **安装LocalAI**\n\n#### 可选一：（kubectl方式）\n\n```\ncat > local-ai.yaml << EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: local-ai\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: models-pvc\n  namespace: local-ai\nspec:\n  storageClassName: nfs-storage # 实际的storageclass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-ai\n  namespace: local-ai\n  labels:\n    app: local-ai\nspec:\n  selector:\n    matchLabels:\n      app: local-ai\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: local-ai\n      name: local-ai\n    spec:\n      containers:\n        - env:\n          - name: DEBUG\n            value: \"true\"\n          name: local-ai\n          #image: quay.io/go-skynet/local-ai:latest\n          image: quay.io/go-skynet/local-ai:master-ffmpeg-core\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: models-volume\n              mountPath: /build/models\n      volumes:\n        - name: models-volume\n          persistentVolumeClaim:\n            claimName: models-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: local-ai\n  namespace: local-ai\nspec:\n  selector:\n    app: local-ai\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\nEOF\n```\n\n```\nkubectl apply -f local-ai.yaml\n```\n\n#### 可选二：（helm方式）\n\n```\nhelm repo add go-skynet https://go-skynet.github.io/helm-charts/\nhelm show values go-skynet/local-ai > values.yaml\n```\n\n```\nvim values.yaml\n```\n\n```\n\nreplicaCount: 1\n​\ndeployment:\n  image:\n    repository: quay.io/go-skynet/local-ai\n    tag: latest    # latest镜像23.5G\n    # tag: master-ffmpeg-core\n  env:\n    threads: 8  # 最好8cpu\n    context_size: 512\n  modelsPath: \"/models\"\n  download_model:\n    image: busybox\n  prompt_templates:\n    image: busybox\n  pullPolicy: IfNotPresent\n  imagePullSecrets: []\nresources: {}\npromptTemplates: {}\nmodels:\n  forceDownload: false\n  list:\n    - url: \"https://gpt4all.io/models/ggml-gpt4all-j.bin\" # 提前下载，放到models的pvc里\n      # basicAuth: base64EncodedCredentials\ninitContainers: []\nsidecarContainers: []\npersistence:\n  models: \n    enabled: true\n    annotations: {}\n    storageClass: nfs-storage # 实际的storageclassName\n    accessModes: ReadWriteMany\n    size: 10Gi\n    globalMount: /models\n  output:\n    enabled: true\n    annotations: {}\n    storageClass: nfs-storage # 实际的storageclassName\n    accessModes: ReadWriteMany\n    size: 5Gi\n    globalMount: /tmp/generated\nservice:\n  type: ClusterIP\n  port: 80\n  annotations: {}\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\nnodeSelector: {}\ntolerations: []\naffinity: {}\n```\n\n```\nhelm install local-ai go-skynet/local-ai -f values.yaml\n```\n\n### **下载模型**\n\nGPT4all-j下载地址（3.52G）：[*https://gpt4all.io/models/ggml-gpt4all-j.bin*](https://gpt4all.io/models/ggml-gpt4all-j.bin)\n\n下载后安装到models的pvc下\n\n#### 测试LocalAI GPT4All模型\n\n```\ncurl http://10.20.13.140:27410/v1/models\n```\n\n![](/images/image-vwti.png)\n\n```\ncurl http://10.20.13.140:27410/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"ggml-gpt4all-j.bin\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}],\n  \"temperature\": 0.7\n}'\n```\n\n![](/images/image-xjyf.png)\n\n### **安装k8sgpt客户端，诊断集群**\n\n下载地址：[*Installation - k8sgpt*](https://docs.k8sgpt.ai/getting-started/installation/)\n\n```\nrpm -ivh -i k8sgpt_amd64.rpm\n```\n\n```\nk8sgpt auth add --backend localai --model ggml-gpt4all-j.bin --baseurl http://10.20.13.140:27410/v1\n```\n\n```\nk8sgpt analyze --explain -b localai # 如果这一步卡住，很可能你的资源问题不能被确认\n```\n\n![](/images/image-riug.png)\n\n```\nk8sgpt analyze --explain -b localai --filter Pod # 只过滤Pod\n```\n\n![](/images/image-mmzs.png)\n\n#### 资源使用问题\n\n![](/images/image-wvvc.png)\n\n查看local-ai的pod在node01节点，如果正在运算可以看到top使用率\n\n![](/images/image-zuwt.png)\n\n### **集成k8sgpt-operator配置自动诊断**\n\n### **安装k8sgpt-operator**\n\n```\nhelm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n```\n\n### **创建K8sGPT**\n\n```\nvim k8sgpt.yaml\n```\n\n```\n\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-local\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    backend: localai\n    model: ggml-gpt4all-j.bin\n    baseUrl: http://local-ai.local-ai.svc.cluster.local:8080/v1\n  noCache: false\n  version: v0.3.8\n```\n\n```\nkubectl apply -f k8sgpt.yaml\n```\n\n#### 测试自动诊断\n\n创建一个错误的pod，镜像tag故意写一个不存在的\n\n```\n\ncat > broken-pod.yaml << EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pod\nspec:\n  containers:\n    - name: broken-pod\n      image: nginx:1.a.b.c\n      livenessProbe:\n        httpGet:\n          path: /\n          port: 90\n        initialDelaySeconds: 3\n        periodSeconds: 3\nEOF\n```\n\n![](/images/image-ycna.png)\n\n查看results\n\n![](/images/image-qjeq.png)\n\n查看详情\n\n```\nkubectl get results -n k8sgpt-operator-system xxxxxxxxx -o json\n```\n\n![](/images/image-rdoz.png)\n\n创建一个没有绑定的svc\n\n```\nkubectl create svc clusterip no-endpoint --tcp=80:80\n```\n\n查看result\n\n![](/images/image-nnat.png)\n\n![](/images/image-gbbr.png)\n\n### **注意：**\n\nai的回答不是统一的，有时候会有弱智回答，这个看主要大模型和机器的资源，如果回答比较模糊，可以删掉results，等待重建后再查看details\n","tags":["localAI"]},{"title":"【云原生】istio服务网格进阶-故障注入与请求超时（详解）","url":"/2024/12/13/【云原生】istio服务网格进阶-故障注入与请求超时（详解）/","content":"\n\n官方文档：[*Istio / 故障注入*](https://istio.io/latest/zh/docs/tasks/traffic-management/fault-injection/)、[*Istio / 设置请求超时*](https://istio.io/latest/zh/docs/tasks/traffic-management/request-timeouts/)\n\n前提条件：\n\n* 部署服务（我这里是官方bookinfo程序）\n\n* 服务注入sidecar\n\n部署在这里提到过，原文地址：[*istio服务网格入门-灰度发布 - (*](https://sreok.cn/archives/1b49e9ee-2a71-4239-913a-fc8913b985b4)[*sreok.cn*](http://sreok.cn)[*)*](https://sreok.cn/archives/1b49e9ee-2a71-4239-913a-fc8913b985b4)\n\n## **故障注入**\n\n### **将请求转发给reviews v2版本**\n\n只有这个版本才会调用ratings服务\n\n```\nkubectl apply -f - << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v2\nEOF\n```\n\n### **设置ratings服务注入延迟**\n\n如果是jason用户的请求，就注入3秒的延迟，比例是100%请求\n\n```\nkubectl apply -f - << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - match:\n    - headers:\n        end-user:\n          exact: jason\n    fault:\n      delay:\n        fixedDelay: 3s\n        percentage:\n          value: 100\n    route:\n    - destination:\n        host: ratings\n        subset: v1\n  - route:\n    - destination:\n        host: ratings\n        subset: v1\nEOF\n```\n\n![](/images/image-eqtw.png)\n\n此时请求bookinfo程序，流量被转移到reviews v2版本，没有任何问题\n\n![](/images/image-oesh.png)\n\n当登录用户为jason时，延迟被注入，按照预期，我们引入的3秒延迟不会影响到 `reviews` 服务，但是因为 `reviews` 和 `ratings` 服务间的超时被硬编码为 10 秒。 但是，在 `productpage` 和 `reviews` 服务之间也有一个 3 秒的硬编码的超时，再加 1 次重试，一共 6 秒。 结果，`productpage` 对 `reviews` 的调用在 6 秒后提前超时并抛出错误了。\n\n简单来说，productpage在等待reviews，reviews在等待ratings，productpage最多等待3秒就会重试，每次重试都会让reviews 重新请求ratings，所以reviews 与ratings直接的请求`必须在3秒内响应`，否则就会抛出错误\n\n可以再次尝试将注入延迟改为2.9秒（只要不超过3秒就会成功，但存在一定误差，这个误差可能时大量请求导致的，我这里是测试，没有其他人的请求，所有可以设置2.9秒）\n\n```\nkubectl apply -f - << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - match:\n    - headers:\n        end-user:\n          exact: jason\n    fault:\n      delay:\n        fixedDelay: 2.9s\n        percentage:\n          value: 100\n    route:\n    - destination:\n        host: ratings\n        subset: v1\n  - route:\n    - destination:\n        host: ratings\n        subset: v1\nEOF\n```\n\n![](/images/image-qdhw.png)\n\n这是实际上是官方提供给我们测试的BUG，reviews v3修复了这BUG，将ratings的超时时间设置为2.5秒\n\n### **更新reviews**\n\n```\nkubectl apply -f - << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v3\nEOF\n```\n\n此时将流量转发给reviews v3版本，ratings响应时间只要超过2.5秒，reviews就不会继续等待，立即返回给productpage，避免ratings故障影响reviews服务\n\n延迟如果在2.5秒内，响应没问题，超过2.5秒，抛出异常\n\n这个时候ratings服务还是设置3s延迟，模拟ratings故障导致无法响应reviews服务，reviews服务也会在2.5秒后，将响应返回给productpage服务，不会影响reviews服务。\n\n![](/images/image-dqov.png)\n\n## **请求超时**\n\n保留ratings 服务的延迟注入，我设置了3秒，这时设置reviews超时时间\n\n### **设置reviews 超时时间**\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v3\n    timeout: 1s\nEOF\n```\n\n过程：\n\n\\=> productpage v1（硬编码3s超时，1次重试）\n\n\\=> reviews v3（硬编码2.5s超时，设置超时1s，此时会提前超时）\n\n\\=> ratings v1（设置延时3s响应，延时只要大于等于超时就会响应失败）\n\n解析：\n\n1. productpage请求reviews服务，reviews请求ratings服务，ratings服务延时3s响应\n\n2. 1秒后reviews服务超时，因为reviews还在等待reviews响应，而ratings要3s后才会响应\n\n3. productpage因为硬编码会再次重试，还是以上步骤，1秒后reviews服务再次返回超时\n\n4. productpage发送两次请求，共等待了2秒后返回给客户端，并且此时reviews是不可用的\n\n注意：\n\nproductpage硬编码的3s超时没有触发，是因为reviews返回了超时\n\nreviews硬编码2.5s超时没有触发，是因为设置了超时时间（作用就是这个）\n\n结果：\n\n2秒后抛出错误：`Sorry, product reviews are currently unavailable for this book.`\n\n![](/images/image-rofb.png)\n\n原因：\n\nreviews服务1秒内没有收到ratings服务的响应，reviews服务超时\n\n这个能解决什么问题？\n\n在故障注入时，我们发现一个bug，就是调用链的下游超时时间大于上游，此时站在架构层解决的方式就可以使用timeout来拦截流量使下游小于上游的超时时间。避免影响中间层服务。\n\n注意：\n\n1. 超时时间只能设置小于硬编码的超时时间\n\n2. 如遇到上面的bug问题，应及时提交并协助修复，高耦合istio并不是最佳方案\n","tags":["istio"]},{"title":"【云原生】istio服务网格入门-灰度发布","url":"/2024/12/11/【云原生】istio服务网格入门-灰度发布/","content":"\n\n官方文档：[*Istio / 流量转移*](https://istio.io/latest/zh/docs/tasks/traffic-management/traffic-shifting/)\n\n前提条件：\n\n* 微服务使用deployment+service部署到集群\n\n### **1、配置命名空间自动注入sidecar**\n\n```\nkubectl label namespace default istio-injection=enabled\n```\n\n我这里使用istio官方提供的bookinfo程序演示\n\n### **2、部署bookinfo示例程序**\n\n```\ncat > bookinfo.yaml << EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: details\n  labels:\n    app: details\n    service: details\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: details\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-details\n  labels:\n    account: details\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: details-v1\n  labels:\n    app: details\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: details\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: details\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-details\n      containers:\n      - name: details\n        image: docker.io/istio/examples-bookinfo-details-v1:1.18.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ratings\n  labels:\n    app: ratings\n    service: ratings\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: ratings\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-ratings\n  labels:\n    account: ratings\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ratings-v1\n  labels:\n    app: ratings\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ratings\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: ratings\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-ratings\n      containers:\n      - name: ratings\n        image: docker.io/istio/examples-bookinfo-ratings-v1:1.18.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: reviews\n  labels:\n    app: reviews\n    service: reviews\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: reviews\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-reviews\n  labels:\n    account: reviews\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v1\n  labels:\n    app: reviews\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v1:1.18.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: \"/tmp/logs\"\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.18.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: \"/tmp/logs\"\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v3\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v3\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v3:1.18.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: \"/tmp/logs\"\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: productpage\n  labels:\n    app: productpage\n    service: productpage\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: productpage\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-productpage\n  labels:\n    account: productpage\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: productpage-v1\n  labels:\n    app: productpage\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: productpage\n      version: v1\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9080\"\n        prometheus.io/path: \"/metrics\"\n      labels:\n        app: productpage\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-productpage\n      containers:\n      - name: productpage\n        image: docker.io/istio/examples-bookinfo-productpage-v1:1.18.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\nEOF\n```\n\n```\nkubectl apply -f bookinfo.yaml\n```\n\n![](/images/image-jxrm.png)\n\n所有pod运行时2/2即为部署成功，sidecar也以容器形式注入到pod内。\n\n### **3、部署网关**\n\n```\ncat > bookinfo-gateway.yaml << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 8080\n      name: http\n      protocol: HTTP\n    hosts:\n    # 允许所有域名通过gateway\n    - \"*\"\nEOF\n```\n\n```\nkubectl apply -f bookinfo-gateway.yaml\n```\n\n### **4、部署前端virtual service**\n\n这里的前端指的时发送请求的服务，可以是中间件也可以是后端的接口，我这里使用`productpage`服务\n\n```\ncat > virtual-service-productpage.yaml << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  # 访问时服务使用的域名，*表示允许所有访问\n  - \"*\"\n  # 外部访问时需要指定gateways，选择入口网关\n  gateways:\n  - bookinfo-gateway\n  http:\n  # 匹配规则\n  - match:\n    - uri:\n        # uri 完全 匹配时走下面路由\n        exact: /productpage\n    - uri:\n        # uri 前缀 匹配时走下面路由\n        prefix: /static\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    # 目标路由\n    - destination:\n        # service名称\n        host: productpage\n        # service端口\n        port:\n          number: 9080\nEOF\n```\n\n```\nkubectl apply -f virtual-service-productpage.yaml\n```\n\n### **5、访问**\n\n![](/images/image-wfga.png)\n\n通过istio-ingress服务的外部IP访问`http://10.20.12.190/productpage`，如果没有部署MetalLB可以使用nodeIP+30271（istio-ingress服务80端口的映射端口）访问`http://10.20.12.10:30271/productpage`\n\n![](/images/image-fokw.png)\n\n访问后刷新页面，发现reviews服务时每次刷新都会请求不同的版本，这是因为后端服务还没有被纳管，此时的请求是通过`productpage`请求reviews服务的service返回的\n\n### **6、创建目标规则**\n\n```\ncat > destination-rule-all.yaml << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: productpage\nspec:\n  host: productpage\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: ratings\nspec:\n  host: ratings\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v2-mysql\n    labels:\n      version: v2-mysql\n  - name: v2-mysql-vm\n    labels:\n      version: v2-mysql-vm\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: details\nspec:\n  host: details\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\nEOF\n```\n\n```\nkubectl apply -f destination-rule-all.yaml\n```\n\n此时将所有服务都设置了规则，按照pod label为version 的识别出来，这一步并不会对请求发生变化，这是为了方便后面调整变化时使用的。\n\n### **7、创建后端virtual service**\n\n```\ncat > virtual-service-all-v1.yaml <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: productpage\nspec:\n  hosts:\n  - productpage\n  http:\n  - route:\n    - destination:\n        host: productpage\n        subset: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - route:\n    - destination:\n        host: ratings\n        subset: v1\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: details\nspec:\n  hosts:\n  - details\n  http:\n  - route:\n    - destination:\n        host: details\n        subset: v1\nEOF\n```\n\n```\nkubectl apply -f virtual-service-all-v1.yaml\n```\n\n此时后端服务都被istio纳管，并将路由目标指向v1版本，现在不管怎么请求，reviews服务都是v1版本\n\n![](/images/image-nqyu.png)\n\n### **8、灰度发布**\n\n之前的操作都是为了能模拟一个正常使用的业务，并将服务通过istio的virtual service管理，接下来才是灰度发布的操作\n\n要升级Review服务从v1到v2版本，首先部署这个deployment，这个demo中已经部署了，并且之前访问时也看到了\n\n```\ncat > virtual-service-reviews.yaml << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 80\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 20\nEOF\n```\n\n```\nkubectl apply -f virtual-service-reviews.yaml\n```\n\n再次刷新页面模拟请求，发现80%的流量在v1，20%流量分配到v2\n\n在kiali中也可以看到流量分配情况\n\n![](/images/image-scse.png)\n\n运行一段时间再次调整v2流量的比例直到100%流量到v2，这时可以下线v1版本。\n","tags":["istio"]},{"title":"【云原生】istio服务网格入门-流量熔断","url":"/2024/12/10/【云原生】istio服务网格入门-流量熔断/","content":"\n\n前提条件：\n\n* 部署服务（deployment、service）并注入sidecar\n\n我这里还是使用官方提供的bookinfo程序，选择productpage作为熔断目标\n\n```\nkubectl apply -f - << EOF\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: productpage\nspec:\n  host: productpage\n  subsets:\n  - name: v1\n    labels:\n      version: v1\nEOF\n```\n\n> 在之前的文档中，设置了目标路由的subset，使virtual service能够识别版本，本文中流量熔断是基于目标路由配置的设置。\n\n### **测试工具**\n\n这是一个名为 [*Fortio*](https://github.com/istio/fortio) 的负载测试客户端， 它可以控制连接数、并发数及发送 HTTP 请求的延迟。 通过 Fortio 能够有效的触发前面在 `DestinationRule` 中设置的熔断策略\n\n`注意：Fortio也需要注入sidecar`\n\n```\nkubectl apply -f - << EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: fortio\n  labels:\n    app: fortio\n    service: fortio\nspec:\n  ports:\n  - port: 8080\n    name: http\n  selector:\n    app: fortio\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fortio-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fortio\n  template:\n    metadata:\n      annotations:\n        # This annotation causes Envoy to serve cluster.outbound statistics via 15000/stats\n        # in addition to the stats normally served by Istio. The Circuit Breaking example task\n        # gives an example of inspecting Envoy stats via proxy config.\n        proxy.istio.io/config: |-\n          proxyStatsMatcher:\n            inclusionPrefixes:\n            - \"cluster.outbound\"\n            - \"cluster_manager\"\n            - \"listener_manager\"\n            - \"server\"\n            - \"cluster.xds-grpc\"\n      labels:\n        app: fortio\n    spec:\n      containers:\n      - name: fortio\n        image: fortio/fortio:latest_release\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http-fortio\n        - containerPort: 8079\n          name: grpc-ping\nEOF\n```\n\n### **测试并发数**\n\n1. 获取Fortio pod名称\n\n   ```\n   export FORTIO_POD=$(kubectl get pods -l app=fortio -o 'jsonpath={.items[0].metadata.name}')\n   ```\n\n2. 测试并发\n\n   ```\n   kubectl exec \"$FORTIO_POD\" -c fortio -- /usr/bin/fortio load -c 10 -qps 0 -n 1 -loglevel Warning http://productpage:9080/productpage\n   ```\n\n   > -c ： 每次连接的并发连接数\n   >\n   > -n ： 发送连接次数\n\n   ![](/images/image-mdhb.png)\n\n### **配置Destination Rule**\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: productpage\nspec:\n  host: productpage\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  # 以下是新增的流量策略\n  trafficPolicy:\n    # 设置连接池\n    connectionPool:\n      tcp:\n        # tcp最大链接数为1\n        maxConnections: 1\n      http:\n        # http最大等待请求数为1\n        http1MaxPendingRequests: 1\n        # 每个连接的最大请求数为1\n        maxRequestsPerConnection: 1\n    # 异常检测，用于自动检测并排除异常的服务实例\n    outlierDetection:\n      # 连续的 5xx 错误计数，当达到这个阈值时，实例会被标记为异常\n      consecutive5xxErrors: 1\n      # 检测的时间窗口，这里是每秒检测一次\n      interval: 1s\n      # 基础排除时间，当实例被标记为异常后，它会在这段时间内被排除，这里是 3 分钟\n      baseEjectionTime: 3m\n      # 允许被排除的最大实例百分比，这里是 100%，意味着所有实例都可能被排除\n      maxEjectionPercent: 100\nEOF\n```\n\n### **再次测试**\n\n1. 获取Fortio pod名称\n\n   ```\n   export FORTIO_POD=$(kubectl get pods -l app=fortio -o 'jsonpath={.items[0].metadata.name}')\n   ```\n\n2. 测试并发\n\n   ```\n   kubectl exec \"$FORTIO_POD\" -c fortio -- /usr/bin/fortio load -c 10 -qps 0 -n 1 -loglevel Warning http://productpage:9080/productpage\n   ```\n\n![](/images/image-ugvn.png)\n\n![](/images/image-eonx.png)\n\n> istio-proxy 允许存在一些误差，所以偶尔会出现拦截9个请求偶尔拦截8个请求，这与各种因素有关，只需要关心大概范围即可。\n\n#### 查看详细熔断日志\n\n多次并发测试后可以通过日志了解更多熔断详情\n\n```\nkubectl exec \"$FORTIO_POD\" -c istio-proxy -- pilot-agent request GET stats | grep productpage | grep pending\n```\n\n![](/images/image-mzti.png)\n\n> 其中110个被请求标记为熔断，20个请求通过\n","tags":["istio"]},{"title":"【云原生】istio服务网格入门-流量镜像","url":"/2024/12/08/【云原生】istio服务网格入门-流量镜像/","content":"\n\n官方文档：[*Istio / 镜像*](https://istio.io/latest/zh/docs/tasks/traffic-management/mirroring/)\n\n### **启动httpbin服务（演示服务）**\n\n```\n# v1版本\ncat <<EOF | istioctl kube-inject -f - | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: httpbin-v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: httpbin\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: httpbin\n        version: v1\n    spec:\n      containers:\n      - image: docker.io/kennethreitz/httpbin\n        imagePullPolicy: IfNotPresent\n        name: httpbin\n        command: [\"gunicorn\", \"--access-logfile\", \"-\", \"-b\", \"0.0.0.0:80\", \"httpbin:app\"]\n        ports:\n        - containerPort: 80\nEOF\n```\n\n```\n# v2版本\ncat <<EOF | istioctl kube-inject -f - | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: httpbin-v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: httpbin\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: httpbin\n        version: v2\n    spec:\n      containers:\n      - image: docker.io/kennethreitz/httpbin\n        imagePullPolicy: IfNotPresent\n        name: httpbin\n        command: [\"gunicorn\", \"--access-logfile\", \"-\", \"-b\", \"0.0.0.0:80\", \"httpbin:app\"]\n        ports:\n        - containerPort: 80\nEOF\n```\n\n### **创建服务**\n\n```\nkubectl create -f - <<EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: httpbin\n  labels:\n    app: httpbin\nspec:\n  ports:\n  - name: http\n    port: 8000\n    targetPort: 80\n  selector:\n    app: httpbin\nEOF\n```\n\n### **创建网关**\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gw\nspec:\n  selector:\n    app: httpbin\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\nEOF\n```\n\n### **创建目标规则**\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: httpbin\nspec:\n  host: httpbin\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\nEOF\n```\n\n### **创建虚拟服务**\n\n将流量全部转发到v1版本\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  gateways:\n  - bookinfo-gateway\n  hosts:\n  - \"*\"\n  http:\n  - match:\n    - uri:\n        exact: /headers\n    route:\n    - destination:\n        host: httpbin\n        subset: v1\n      weight: 100\nEOF\n```\n\n#### 浏览器访问\n\n![](/images/image-tjnh.png)\n\n### **流量镜像**\n\n将转发到v1的流量克隆一份到v2版本\n\n```\nkubectl apply -f - <<EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  gateways:\n  - bookinfo-gateway\n  hosts:\n  - \"*\"\n  http:\n  - match:\n    - uri:\n        exact: /headers\n    route:\n    - destination:\n        host: httpbin\n        subset: v1\n      weight: 100\n    mirror:\n      # 镜像目标\n      host: httpbin\n      subset: v2\n    # 镜像比例100%\n    mirrorPercentage:\n      value: 100.0\nEOF\n```\n\n#### 再次访问\n\n![](/images/image-hvnz.png)\n\n> 重点注意这些被镜像的流量是『即发即弃』的，就是说镜像请求的响应会被丢弃，服务响应还是v1版本的\n","tags":["istio"]},{"title":"【云原生】Kubernetes部署Zookeeper v3.9 + Kafka v3.7.0 集群 （manifest方式）","url":"/2024/09/02/【云原生】Kubernetes部署Zookeeper v3.9 + Kafka v3.7.0 集群 （manifest方式）/","content":"\n### 部署zookeeper（三节点）\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-headless\n  namespace: kafka\n  labels:\n    app: zookeeper\nspec:\n  ports:\n    - port: 2181\n      name: client\n    - port: 2888\n      name: fllower\n    - port: 3888\n      name: election\n  clusterIP: None\n  selector:\n    app: zookeeper\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-adminserver\n  namespace: kafka\n  labels:\n    app: zookeeper\nspec:\n  type: NodePort\n  ports:\n    - port: 8080\n      name: adminserver\n      nodePort: 31801\n  selector:\n    app: zookeeper\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: zookeeper-config\n  namespace: kafka\n  labels:\n    app: zookeeper\ndata:\n  zoo.cfg: |+\n    tickTime=2000\n    initLimit=10\n    syncLimit=5\n    dataDir=/data\n    dataLogDir=/datalog\n    clientPort=2181\n    server.1=zookeeper-0.zookeeper-headless.kafka.svc.cluster.local:2888:3888\n    server.2=zookeeper-1.zookeeper-headless.kafka.svc.cluster.local:2888:3888\n    server.3=zookeeper-2.zookeeper-headless.kafka.svc.cluster.local:2888:3888\n    4lw.commands.whitelist=*\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  namespace: kafka\nspec:\n  serviceName: \"zookeeper-headless\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      initContainers:\n      - name: set-id\n        # 源镜像地址：busybox:latest\n        image: harbor.basepoint.net/library/busybox:latest\n        command: ['sh', '-c', \"hostname | cut -d '-' -f 2 | awk '{print $0 + 1}' > /data/myid\"]\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      containers:\n      - name: zookeeper\n        # 源镜像地址：zookeeper:3.9\n        image: harbor.basepoint.net/library/zookeeper:3.9\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: \"500Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1000Mi\"\n            cpu: \"1000m\"\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: fllower\n        - containerPort: 3888\n          name: election\n        volumeMounts:\n          - name: zook-config\n            mountPath: /conf/zoo.cfg\n            subPath: zoo.cfg\n          - name: data\n            mountPath: /data\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: zook-config\n        configMap:\n          name: zookeeper-config\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteMany\"]\n        storageClassName: longhorn\n        resources:\n          requests:\n            storage: 10Gi\n```\n\n### 部署Kafka（三节点）\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\n  namespace: kafka\n  labels:\n    app: kafka\nspec:\n  ports:\n    - port: 9092\n      name: server\n  clusterIP: None\n  selector:\n    app: kafka\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka\n  namespace: kafka\nspec:\n  type: NodePort\n  selector:\n    app: kafka\n  ports:\n    - protocol: TCP\n      port: 9092\n      targetPort: 9092\n      nodePort: 30092\n      name: bootstrap\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: kafka\nspec:\n  serviceName: \"kafka-headless\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                      - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: kafka\n        # 源镜像地址：apache/kafka:3.7.0\n        image: harbor.basepoint.net/library/kafka:3.7.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: \"500Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1000Mi\"\n            cpu: \"2000m\"\n        ports:\n        - containerPort: 9092\n          name: server\n        #env:\n        #  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://172.25.2.4:30092\n        #  --override advertised.listeners=PLAINTEXT://172.25.2.4:30092 \\\n        command:\n        - sh\n        - -c\n        - \"exec /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \\\n          --override listeners=PLAINTEXT://:9092 \\\n          --override zookeeper.connect=zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181 \\\n          --override log.dirs=/var/lib/kafka/data/logs \\\n          --override auto.create.topics.enable=true \\\n          --override auto.leader.rebalance.enable=true \\\n          --override background.threads=10 \\\n          --override compression.type=producer \\\n          --override delete.topic.enable=true \\\n          --override leader.imbalance.check.interval.seconds=300 \\\n          --override leader.imbalance.per.broker.percentage=10 \\\n          --override log.flush.interval.messages=9223372036854775807 \\\n          --override log.flush.offset.checkpoint.interval.ms=60000 \\\n          --override log.flush.scheduler.interval.ms=9223372036854775807 \\\n          --override log.retention.bytes=-1 \\\n          --override log.retention.hours=168 \\\n          --override log.roll.hours=168 \\\n          --override log.roll.jitter.hours=0 \\\n          --override log.segment.bytes=1073741824 \\\n          --override log.segment.delete.delay.ms=60000 \\\n          --override message.max.bytes=1000012 \\\n          --override min.insync.replicas=1 \\\n          --override num.io.threads=8 \\\n          --override num.network.threads=3 \\\n          --override num.recovery.threads.per.data.dir=1 \\\n          --override num.replica.fetchers=1 \\\n          --override offset.metadata.max.bytes=4096 \\\n          --override offsets.commit.required.acks=-1 \\\n          --override offsets.commit.timeout.ms=5000 \\\n          --override offsets.load.buffer.size=5242880 \\\n          --override offsets.retention.check.interval.ms=600000 \\\n          --override offsets.retention.minutes=1440 \\\n          --override offsets.topic.compression.codec=0 \\\n          --override offsets.topic.num.partitions=50 \\\n          --override offsets.topic.replication.factor=3 \\\n          --override offsets.topic.segment.bytes=104857600 \\\n          --override queued.max.requests=500 \\\n          --override quota.consumer.default=9223372036854775807 \\\n          --override quota.producer.default=9223372036854775807 \\\n          --override replica.fetch.min.bytes=1 \\\n          --override replica.fetch.wait.max.ms=500 \\\n          --override replica.high.watermark.checkpoint.interval.ms=5000 \\\n          --override replica.lag.time.max.ms=10000 \\\n          --override replica.socket.receive.buffer.bytes=65536 \\\n          --override replica.socket.timeout.ms=30000 \\\n          --override request.timeout.ms=30000 \\\n          --override socket.receive.buffer.bytes=102400 \\\n          --override socket.request.max.bytes=104857600 \\\n          --override socket.send.buffer.bytes=102400 \\\n          --override unclean.leader.election.enable=true \\\n          --override zookeeper.session.timeout.ms=6000 \\\n          --override zookeeper.set.acl=false \\\n          --override broker.id.generation.enable=true \\\n          --override connections.max.idle.ms=600000 \\\n          --override controlled.shutdown.enable=true \\\n          --override controlled.shutdown.max.retries=3 \\\n          --override controlled.shutdown.retry.backoff.ms=5000 \\\n          --override controller.socket.timeout.ms=30000 \\\n          --override default.replication.factor=1 \\\n          --override fetch.purgatory.purge.interval.requests=1000 \\\n          --override group.max.session.timeout.ms=300000 \\\n          --override group.min.session.timeout.ms=6000 \\\n          --override log.cleaner.backoff.ms=15000 \\\n          --override log.cleaner.dedupe.buffer.size=134217728 \\\n          --override log.cleaner.delete.retention.ms=86400000 \\\n          --override log.cleaner.enable=true \\\n          --override log.cleaner.io.buffer.load.factor=0.9 \\\n          --override log.cleaner.io.buffer.size=524288 \\\n          --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \\\n          --override log.cleaner.min.cleanable.ratio=0.5 \\\n          --override log.cleaner.min.compaction.lag.ms=0 \\\n          --override log.cleaner.threads=1 \\\n          --override log.cleanup.policy=delete \\\n          --override log.index.interval.bytes=4096 \\\n          --override log.index.size.max.bytes=10485760 \\\n          --override log.message.timestamp.difference.max.ms=9223372036854775807 \\\n          --override log.message.timestamp.type=CreateTime \\\n          --override log.preallocate=false \\\n          --override log.retention.check.interval.ms=300000 \\\n          --override max.connections.per.ip=2147483647 \\\n          --override num.partitions=1 \\\n          --override producer.purgatory.purge.interval.requests=1000 \\\n          --override replica.fetch.backoff.ms=1000 \\\n          --override replica.fetch.max.bytes=1048576 \\\n          --override replica.fetch.response.max.bytes=10485760 \\\n          --override reserved.broker.max.id=1000\"\n        volumeMounts:\n          - name: data\n            mountPath: /var/lib/kafka/data\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: ALLOW_PLAINTEXT_LISTENER\n          value: \"yes\"\n        - name: KAFKA_HEAP_OPTS\n          value : \"-Xms1g -Xmx1g\"\n        - name: KAFKA_OPTS\n          value: \"-Dlogging.level=INFO\"\n        - name: JMX_PORT\n          value: \"5555\"\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        storageClassName: longhorn\n        resources:\n          requests:\n            storage: 10Gi\n```\n\n#### 检查所有pod启动无报错\n\n```\n[root@k8s-h3c-master01 kafka]# kubectl get pod -n kafka\nNAME          READY   STATUS    RESTARTS   AGE\nkafka-0       1/1     Running   0          1m\nkafka-1       1/1     Running   0          1m\nkafka-2       1/1     Running   0          1m\nzookeeper-0   1/1     Running   0          7m\nzookeeper-1   1/1     Running   0          7m\nzookeeper-2   1/1     Running   0          7m\n```\n\n### 部署Kafka-eagle可视化\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-eagle\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kafka-eagle\n  template:\n    metadata:\n      labels:\n        app: kafka-eagle\n    spec:\n      restartPolicy: Always\n      containers:\n          # 源镜像地址：nickzurich/efak:3.0.1\n        - image: harbor.basepoint.net/library/efak:3.0.1\n          imagePullPolicy: IfNotPresent\n          name: kafka-eagle\n          env:\n            - name: EFAK_CLUSTER_ZK_LIST\n              value: zookeeper-0.zookeeper-headless.kafka.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.kafka.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.kafka.svc.cluster.local:2181\n          ports:\n            - containerPort: 8048\n              name: web\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-eagle\n  namespace: kafka\nspec:\n  type: NodePort\n  ports:\n    - port: 8048\n      targetPort: 8048\n      nodePort: 30048\n      name: web\n  selector:\n    app: kafka-eagle\n```\n\n### 验证\n\n登录eagle，http\\://ip:30048\n\n账户admin，密码123456\n\n![](/images/image-gbbm.png)\n\n![](/images/image-wjpu.png)\n","tags":["Kubernetes","kafka","Zookeeper"],"categories":["云原生"]},{"title":"【云原生】Kafka数据同步（三）：利用Debezium实时捕获MySQL数据变更输出到MySQL","url":"/2024/08/24/【云原生】Kafka数据同步（三）：利用Debezium实时捕获MySQL数据变更输出到MySQL/","content":"\n### 前提条件\n\n* 安装kafka\n\n  * 原文地址：[Kubernetes部署Zookeeper（v3.9）+ Kafka（v3.7.0）集群 （manifest方式） - Elijah Blog (](https://sreok.cn/archives/82c5ae3b-ffd0-4089-90b5-d048a0243e74)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/82c5ae3b-ffd0-4089-90b5-d048a0243e74)\n\n- 安装kafka connect（debezium）\n\n  * 原文地址：[利用Debezium实时捕获MySQL数据变更到Kafka（manifest方式） - Elijah Blog (](https://sreok.cn/archives/c8d918ed-5252-490d-b08c-86f7e257c37b)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/c8d918ed-5252-490d-b08c-86f7e257c37b)\n\n### 架构\n\n![](/images/image-jyzn.png)\n\n### 获取所有connector-plugins\n\n```\ncurl http://localhost:3083/connector-plugins | jq\n```\n\n> `type: source`：源数据插件\n>\n> `type: sink`：目标数据插件\n>\n> 默认内置插件：\n>\n> ```\n> [kafka@kafka-connect-769747b4bb-vqkld connect]$ ls -l\n> total 52\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-db2\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-ibmi\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-informix\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-jdbc\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-mongodb\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-mysql\n> drwxr-xr-x 2 kafka kafka 8192 Jul 26 04:26 debezium-connector-oracle\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-postgres\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-spanner\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-sqlserver\n> drwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-vitess\n> ```\n>\n> 源数据插件使用：`io.debezium.connector.mysql.MySqlConnector`\n>\n> debezium镜像默认只有一个jdbc的目标数据插件：`io.debezium.connector.jdbc.JdbcSinkConnector`\n>\n> ```\n> curl http://localhost:3083/connector-plugins | jq ｜ grep sink -C 1\n> ```\n\n### 创建source connector（源连接器）\n\n```json\n{\n\t\"name\": \"source-mysql-connector\",\n\t\"config\": {\n\t\t\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n\t\t\"tasks.max\": \"1\",\n        // 源数据库配置 \n        \"database.hostname\": \"mysql.mysql.svc.cluster.local\",  \n        \"database.port\": \"3306\",\n        \"database.user\": \"root\",\n        \"database.password\": \"password\",\n        //  此数据库客户机的数字ID,必须是唯一的,连接器使用此ID将MySQL数据库集群加入为另一台服务器。\n        \"database.server.id\": \"184054\",\n        // 源连接器的kafka主题，这个主题可以获取表的增删改操作\n        \"topic.prefix\": \"mysql\",\n        // 只获取testdb库\n        \"database.include.list\": \"testdb\",\n        // 仅同步testdb数据库下的test_tables表\n        \"table.include.list\": \"testdb.test_tables\",\n        // kafka地址\n        \"schema.history.internal.kafka.bootstrap.servers\": \"kafka.kafka.svc.cluster.local:9092\",\n        // 历史记录字段，仅debezium内部使用。\n        \"schema.history.internal.kafka.topic\": \"schema-changes.testdb\",\n        // 主键字段，testdb开头的数据库，id字段\n        \"message.key.columns\": \"testdb(.*):id\"\n        // 指定连接器向 Kafka 主题发送检测信号消息的频率,默认0\n\t    \"heartbeat.interval.ms\": \"100\",\n        // 指定连接器是否将数据库架构发生的更改发布到具有数据库服务器 ID 名称的 Kafka 主题\n\t    \"include.schema.changes\": \"true\",\n        // 指定连接器是否在元数据对象上分析和发布表和列注释。\n\t    \"include.schema.comments\": \"true\",\n        // 快照模式，默认就是initial\n\t    \"snapshot.mode\": \"initial\",\n        // 通过route获取数据库名作为kafka主题名\n\t    // \"transforms\": \"route\",\n\t    // \"transforms.route.type\": \"org.apache.kafka.connect.transforms.RegexRouter\",\n\t    // \"transforms.route.regex\": \"([^.]+)\\\\.([^.]+)\\\\.([^.]+)\",\n\t    // \"transforms.route.replacement\": \"$3\",\n        // 指定删除事件后是否跟有逻辑删除事件，默认就是true\n\t    // \"tombstones.on.delete\": \"true\"\n    }\n}\n```\n\n参数地址：<https://debezium.io/documentation/reference/stable/connectors/mysql.html>\n\n```\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:3083/connectors -d @source-mysql-connector.json\n```\n\n### 创建sink connector（目标连接器）\n\n```json\n{\n \"name\": \"sink-mysql-connector\",\n \"config\":{\n   \"connector.class\": \"io.debezium.connector.jdbc.JdbcSinkConnector\",\n   // 目标数据库：jdbc:mysql://mysql地址/数据库\n   \"connection.url\": \"jdbc:mysql://mysqltest.mysql.svc.cluster.local:3306/test\",\n   \"connection.username\": \"root\",\n   \"connection.password\": \"password\",\n   \"tasks.max\": \"1\",\n   // 源kafka主题\n   \"topics\": \"mysql.testdb.test_tables\",\n   \"dialect.name\": \"MySqlDatabaseDialect\",\n   // 保证删除能正确消费的参数\n   \"debezium.sink.databend.upsert-keep-deletes\": \"false\",\n   \"transforms\": \"unwrap\",\n   \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n   \"transforms.unwrap.drop.tombstones\": \"true\",\n   \"transforms.unwrap.delete.handling.mode\": \"rewrite\",\n   \"schema.evolution\": \"basic\",\n   // 如果主键不存在，则连接器执行INSERT操作，如果键存在，则连接器执行UPDATE操作\n   \"insert.mode\": \"upsert\",\n   // \n   \"delete.enabled\": \"true\",\n   // 主键,如果\"delete.enabled\": \"true\"，这个配置不能为空\n   \"primary.key.fields\": \"id\"\n   \"primary.key.mode\": \"record_key\",\n   \"database.time_zone\": \"UTC\",\n   }\n}\n```\n\n参数地址：<https://debezium.io/documentation/reference/stable/connectors/jdbc.html>\n\n```\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:3083/connectors -d @sink-mysql-connector.json\n```\n\n### 测试\n\n源数据库，新增数据，并删除一部分\n\n![](/images/image-wngk.png)\n\n目标数据库查看\n\n> 数据库表名为kafka主题名，`.`改为`_`\n\n![](/images/image-uuhh.png)\n","tags":["Kubernetes","kafka"],"categories":["云原生"]},{"title":"【云原生】Kafka数据同步（二）：Debezium Kafka Connect创建connector三种方式（manifest方式）","url":"/2024/08/21/【云原生】Kafka数据同步（二）：Debezium Kafka Connect创建connector三种方式（manifest方式）/","content":"\n### 方式一：通过Debezium UI 创建connector\n\n![](/images/image-gkes.png)\n\n![](/images/image-vnfk.png)\n\n![](/images/image-nxgd.png)\n\n![](/images/image-hhxz.png)\n\n### 方式二：通过connect REST API 创建connector（curl方式）\n\n查看所有connector\n\n```\ncurl -H \"Accept:application/json\" localhost:8083/connectors/\n```\n\n创建connector\n\n```\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d '{ \"name\": \"test-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"root\", \"database.password\": \"password\", \"database.server.id\": \"184054\", \"topic.prefix\": \"mysql\", \"database.include.list\": \"testdb\", \"schema.history.internal.kafka.bootstrap.servers\": \"kafka.kafka.svc.cluster.local:9092\", \"schema.history.internal.kafka.topic\": \"schemahistory.inventory\" } }'\n```\n\n查看创建的test-connector\n\n```\ncurl -H \"Accept:application/json\" localhost:8083/connectors/test-connector\n```\n\n> test-connector：新建的connector名称\n\n删除test-connector\n\n```\ncurl -i -X DELETE -H \"Accept:application/json\" localhost:8083/connectors/test-connector\n```\n\n### 方式三：通过connect REST API 创建connector（post工具方式）\n\n查看所有 connector\n\n![](/images/image-dcwf.png)\n\n创建connector\n\n```\n{\n\t\"name\": \"test-connector\",\n\t\"config\": {\n\t\t\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n\t\t\"tasks.max\": \"1\",  \n        \"database.hostname\": \"mysql\",  \n        \"database.port\": \"3306\",\n        \"database.user\": \"root\",\n        \"database.password\": \"password\",\n        \"database.server.id\": \"184054\",\n        \"topic.prefix\": \"mysql\",\n        \"database.include.list\": \"testdb\",\n        \"table.include.list\": \"testdb.test_tables\",\n        \"schema.history.internal.kafka.bootstrap.servers\": \"kafka.kafka.svc.cluster.local:9092\",  \n        \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\"\n    }\n}\n```\n\n![](/images/image-wzrf.png)\n\n查看创建的test-connector\n\n![](/images/image-czlc.png)删除connector\n\n![](/images/image-zxff.png)\n\n### 注意事项\n\n以上三种方式创建connector时，connect容器日志中会输出以下内容，验证新增的connector是否有效\n\n![](/images/image-toyr.png)\n\n#### 获取所有connect plugins\n\n```\ncurl http://localhost:8083/connector-plugins\n```\n\n#### **查看表更新**\n\n```\n# 启动一个pod watcher监控数据表增删改，只作用database.include.list定义的数据库 \nkubectl run -n kafka -it --rm --image=quay.io/debezium/tooling:1.2  --restart=Never watcher -- kcat -b kafka-kafka-bootstrap:9092 -C -o beginning -t mysql\n​\n# kubectl run -n kafka -it --rm --image=harbor.basepoint.net/library/tooling:1.2  --restart=Never watcher -- kcat -b kafka.kafka.svc.cluster.local:9092 -C -o beginning -t mysql\n```\n\n#### **查看数据更新**\n\n```\n# 启动一个pod watcher2监控user表数据增删改\nkubectl run -n kafka -it --rm --image=quay.io/debezium/tooling:1.2  --restart=Never watcher2 -- kcat -b kafka-kafka-bootstrap:9092 -C -o beginning -t mysql.testdb.user\n​\n# kubectl run -n kafka -it --rm --image=harbor.basepoint.net/library/tooling:1.2  --restart=Never watcher2 -- kcat -b kafka.kafka.svc.cluster.local:9092 -C -o beginning -t mysql.testdb.user\n```\n\n`如果提示watcher或watcher2 Already exists，需要执行下面命令删除pod`\n\n```\nkubectl delete pod -n kafka watcher --force\nkubectl delete pod -n kafka watcher2 --force\n```\n","tags":["Kubernetes","kafka"],"categories":["云原生"]},{"title":"【云原生】Kafka数据同步（一）：Kafka Connect部署Debezium v2.7数据实时捕获工具（manifest方式）","url":"/2024/08/20/【云原生】Kafka数据同步（一）：Kafka Connect部署Debezium v2.7数据实时捕获工具（manifest方式）/","content":"\n### 前提条件\n\n* 部署kafka，原文地址：[Kubernetes部署Zookeeper（v3.9）+ Kafka（v3.7.0）集群 （manifest方式） - Elijah Blog (](https://sreok.cn/archives/82c5ae3b-ffd0-4089-90b5-d048a0243e74)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/82c5ae3b-ffd0-4089-90b5-d048a0243e74)\n\n### 部署Debezium Plugins\n\n创建pvc，用于放connect plugins\n\n```\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: kafka-plugins\n  namespace: kafka\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  # 使用nfs方便放置\n  storageClassName: nfs-storage\n```\n\n```\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-debezium-connect\n  namespace: kafka\n  labels:\n    app: kafka-debezium-connect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kafka-debezium-connect\n  template:\n    metadata:\n      labels:\n        app: kafka-debezium-connect\n    spec:\n      containers:\n      - name: connect\n        image: harbor.basepoint.net/library/connect:2.7\n        env:\n          - name: GROUP_ID\n            value: \"1\"\n          - name: CONFIG_STORAGE_TOPIC\n            value: connect-cluster-offsets\n          - name: OFFSET_STORAGE_TOPIC\n            value: connect-cluster-configs\n          - name: STATUS_STORAGE_TOPIC\n            value: connect-cluster-status\n          - name: KAFKA_CONNECT_PLUGINS_DIR\n            value: /kafka/connect/,/usr/local/share/kafka/plugins\n          - name: CONNECT_TOPIC_CREATION_ENABLE\n            value: \"true\"\n        volumeMounts:\n        - name: plugins\n          mountPath: /usr/local/share/kafka/plugins\n      volumes:\n      - name: plugins\n        persistentVolumeClaim:\n          claimName: kafka-plugins\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-debezium-connect\n  namespace: kafka\nspec:\n  type: NodePort\n  selector:\n    app: kafka-debezium-connect\n  ports:\n    - protocol: TCP\n      port: 8083\n      targetPort: 8083\n      nodePort: 31083\n      name: restapi\n```\n\n容器已内置了以下插件\n\n```\n[kafka@kafka-connect-769747b4bb-vqkld connect]$ ls -l\ntotal 52\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-db2\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-ibmi\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-informix\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-jdbc\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-mongodb\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-mysql\ndrwxr-xr-x 2 kafka kafka 8192 Jul 26 04:26 debezium-connector-oracle\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-postgres\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-spanner\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-sqlserver\ndrwxr-xr-x 2 kafka kafka 4096 Jul 26 04:26 debezium-connector-vitess\n```\n\nconfluence的jdbc下载地址：[JDBC Connector (Source and Sink) | Confluent Hub: Apache Kafka Connectors for Streaming Data](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n\n放置pvc目录中，重启connect\n\n![](/images/image-dfzv.png)\n\n#### 已知问题\n\nto many open file\n\n```\nvi /etc/sysctl.conf\n\nfs.inotify.max_user_instances = 819200\nfs.inotify.max_queued_events = 1638400\nfs.inotify.max_user_watches = 52428800\n```\n\n```\nsysctl -p\n```\n\n### 部署debezium UI\n\n```\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: debezium-ui\n  namespace: kafka\n  labels:\n    app: debezium-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: debezium-ui\n  template:\n    metadata:\n      labels:\n        app: debezium-ui\n    spec:\n      containers:\n      - name: ui\n        # 最新源地址：quay.io/debezium/debezium-ui:2.5，版本不兼容\n        # image: harbor.basepoint.net/library/debezium-ui:2.5\n        # 源地址：debezium/debezium-ui:2.1.2.Final\n        image: harbor.basepoint.net/library/debezium-ui:2.1.2.Final\n        imagePullPolicy: IfNotPresent\n        env:\n          - name: KAFKA_CONNECT_URIS\n            # connect地址\n            value: http://kafka-connect.kafka.svc.cluster.local:8083\n          - name: DEPLOYMENT_MODE\n            value: default\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: debezium-ui\n  namespace: kafka\nspec:\n  type: NodePort\n  selector:\n    app: debezium-ui\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n      nodePort: 30080\n      name: restapi\n```\n\nUI请求有问题请求不到MangoDB.json，先保留后续研究。\n\n> 结论：debezium-ui镜像版本与debezium-connect镜像版本不兼容，目前ui没有2.7版本，2.5不兼容，最终使用：`debezium/debezium-ui:2.1.2.Final`镜像版本。\n>\n> 虽然debezium-ui:2.1.2.Final版本还是有报错，但至少能创建Connector了\n","tags":["Kubernetes","kafka"],"categories":["云原生"]},{"title":"【云原生】利用Strimzi Operator部署kafka集群并将MySQL数据通过Debezium同步到Kafka","url":"/2024/08/18/【云原生】利用Strimzi Operator部署kafka集群并将MySQL数据通过Debezium同步到Kafka/","content":"\n\n官方地址：[在 Kubernetes 上部署 Debezium ：： Debezium Documentation --- Deploying Debezium on Kubernetes :: Debezium Documentation](https://debezium.io/documentation/reference/2.7/operations/kubernetes.html)\n\n### 前提条件\n\n* 私有镜像仓库（制作connect镜像）\n\n  * Harbor原文地址：[【DevOps】Harbor私有镜像仓库并利用cert-manager自签名证书开启HTTPS（helm方式安装） - Elijah Blog (](https://sreok.cn/archives/d5be0c00-46b6-44d9-9a32-6a7c933ab822)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/d5be0c00-46b6-44d9-9a32-6a7c933ab822)\n\n  * docker registry：\n\n#### 安装OLM\n\n```\ncurl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/install.sh | bash -s v0.20.0\n```\n\n#### 安装Operator\n\n```\nkubectl create -f https://operatorhub.io/install/strimzi-kafka-operator.yaml\n```\n\n### 部署kafka集群\n\n```\napiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: kafka\nspec:\n  kafka:\n    replicas: 1\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n          type: tls\n      - name: external\n        port: 9094\n        type: nodeport\n        tls: false\n    storage:\n      type: jbod\n      volumes:\n      - id: 0\n        type: persistent-claim\n        size: 20Gi\n        deleteClaim: false\n    config:\n      offsets.topic.replication.factor: 1\n      transaction.state.log.replication.factor: 1\n      transaction.state.log.min.isr: 1\n      default.replication.factor: 1\n      min.insync.replicas: 1\n  zookeeper:\n    replicas: 1\n    storage:\n      type: persistent-claim\n      size: 20Gi\n      deleteClaim: false\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n```\n\n### 部署Kafka Connect 集群（构建MySQL插件镜像）\n\n```\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\n  name: kafka-connect\n  annotations:\n    strimzi.io/use-connector-resources: \"true\"\nspec:\n  # 版本3.7.0，默认3.1.0与MySQL 8.0.x不兼容\n  version: 3.7.0\n  replicas: 1\n  bootstrapServers: kafka-kafka-bootstrap:9092\n  config:\n    config.providers: secrets\n    config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider\n    group.id: connect-cluster\n    offset.storage.topic: connect-cluster-offsets\n    config.storage.topic: connect-cluster-configs\n    status.storage.topic: connect-cluster-status\n    # -1 means it will use the default replication factor configured in the broker\n    config.storage.replication.factor: -1\n    offset.storage.replication.factor: -1\n    status.storage.replication.factor: -1\n  build:\n    output:\n      type: docker\n      # 存在的私有镜像仓库地址\n      image: 10.20.13.140:5000/debezium-connect-mysql:latest\n    plugins:\n      - name: debezium-mysql-connector\n        artifacts:\n          - type: tgz\n            url: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/2.7.0.Final/debezium-connector-mysql-2.7.0.Final-plugin.tar.gz\n```\n\n如果已存在可用镜像，使用以下方式\n\n### 部署Kafka Connect 集群（使用已有镜像）\n\n```\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\n  name: kafka-connect\n  annotations:\n    strimzi.io/use-connector-resources: \"true\"\nspec:\n  version: 3.7.0\n  image: 10.20.13.140:5000/debezium-connect-mysql:latest\n  replicas: 1\n  bootstrapServers: kafka-kafka-bootstrap:9092\n  config:\n    config.providers: secrets\n    config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider\n    group.id: connect-cluster\n    offset.storage.topic: connect-cluster-offsets\n    config.storage.topic: connect-cluster-configs\n    status.storage.topic: connect-cluster-status\n    # -1 means it will use the default replication factor configured in the broker\n    config.storage.replication.factor: -1\n    offset.storage.replication.factor: -1\n    status.storage.replication.factor: -1\n```\n\n### 创建MySQL Connector\n\n```\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnector\nmetadata:\n  name: debezium-connector-mysql\n  labels:\n    strimzi.io/cluster: kafka-connect\nspec:\n  class: io.debezium.connector.mysql.MySqlConnector\n  tasksMax: 1\n  config:\n    tasks.max: 1\n    database.hostname: mysql\n    database.port: 3306\n    database.user: root\n    database.password: password\n    # database.user: ${secrets:debezium-example/debezium-secret:username}\n    # database.password: ${secrets:debezium-example/debezium-secret:password}\n    database.server.id: 184054\n    topic.prefix: mysql\n    database.include.list: inventory\n    schema.history.internal.kafka.bootstrap.servers: kafka-kafka-bootstrap:9092\n    schema.history.internal.kafka.topic: schema-changes.inventory\n```\n\n### **注意事项**\n\n#### **多库多表同步**\n\n编辑`mysql-connect.yaml`或者`kubectl edit kafkaconnector -n kafka debezium-connector-mysql`\n\n> 推荐修改yaml文件，因为edit不会立即更新\n\n示例内容：\n\n```\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnector\nmetadata:\n  name: debezium-connector-mysql\n  labels:\n    strimzi.io/cluster: debezium-connect-cluster\nspec:\n  class: io.debezium.connector.mysql.MySqlConnector\n  tasksMax: 1\n  config:\n    tasks.max: 1\n    database.hostname: # mysql svc地址，如果不在一个命名空间(比如keycloak命名空间)使用：mysql.keycloak.svc\n    database.port: 3306\n    database.user: #mysql用户名\n    database.password: #mysql密码\n    database.server.id: 184054\n    topic.prefix: mysql\n    database.include.list: testdb,testdb1 # 数据库\n    table.include.list: testdb.test_tables,testdb1.test_tables\n    schema.history.internal.kafka.bootstrap.servers: debezium-cluster-kafka-bootstrap:9092\n    schema.history.internal.kafka.topic: schema-changes.inventory\n```\n\n```\n# 修改以下内容，库与库之间、表于表之间用英文逗号隔开\ndatabase.include.list: testdb,testdb1\ntable.include.list: testdb.test_tables,testdb1.test_tables\n```\n\n修改后重新应用\n\n```\nkubectl delete -f mysql-connect.yaml -n kafka\nkubectl apply -f mysql-connect.yaml -n kafka\n```\n\n#### **验证kafka连接状态（排查时使用）**\n\n```\nkubectl get kafkaconnect -n kafka\n# ready状态为true即可\n# 查看详情\nkubectl describe kafkaconnect -n kafka\n```\n\n#### **验证mysql连接状态**\n\n```\nkubectl get kafkaconnector -n kafka\n# ready状态为true即可\n# 查看详情\nkubectl describe kafkaconnector -n kafka\n```\n\n#### **查看表更新**\n\n```\n# 启动一个pod watcher监控数据表增删改，只作用database.include.list定义的数据库 \nkubectl run -n kafka -it --rm --image=quay.io/debezium/tooling:1.2  --restart=Never watcher -- kcat -b kafka-kafka-bootstrap:9092 -C -o beginning -t mysql\n​\n# kubectl run -n kafka -it --rm --image=harbor.basepoint.net/library/tooling:1.2  --restart=Never watcher -- kcat -b kafka.kafka.svc.cluster.local:9092 -C -o beginning -t mysql\n```\n\n#### **查看数据更新**\n\n```\n# 启动一个pod watcher2监控user表数据增删改\nkubectl run -n kafka -it --rm --image=quay.io/debezium/tooling:1.2  --restart=Never watcher2 -- kcat -b kafka-kafka-bootstrap:9092 -C -o beginning -t mysql.testdb.user\n​\n# kubectl run -n kafka -it --rm --image=harbor.basepoint.net/library/tooling:1.2  --restart=Never watcher2 -- kcat -b kafka.kafka.svc.cluster.local:9092 -C -o beginning -t mysql.testdb.user\n```\n\n`如果提示watcher或watcher2 Already exists，需要执行下面命令删除pod`\n\n```\nkubectl delete pod -n kafka watcher --force\nkubectl delete pod -n kafka watcher2 --force\n```\n","tags":["kafka"],"categories":["云原生"]},{"title":"【云原生】使用rook在k8s集群部署ceph（manifest方式）","url":"/2024/07/14/【云原生】使用rook在k8s集群部署ceph（manifest方式）/","content":"\n## **前提条件**\n\n* k8s 集群\n\n* 节点挂载空盘用于ceph osd\n\n下载源文件，本文档是1.14.1[*Source code (tar.gz) (*](https://github.com/rook/rook/archive/refs/tags/v1.14.1.tar.gz)[*github.com*](http://github.com)[*)*](https://github.com/rook/rook/archive/refs/tags/v1.14.1.tar.gz)\n\n`解压进入deploy/examples目录，以下操作如果没有说明，都是在examples目录下操作`\n\n## **部署**\n\n### **部署operator**\n\n##### **打开自动发现**\n\n修改operator.yaml 中的ROOK\\_ENABLE\\_DISCOVERY\\_DAEMON为true\n\n![](/images/image-fsrb.png)\n\n##### **修改镜像地址**\n\n国内访问不到google地址，使用`修改为阿里云地址`，我这里使用私有仓库，只要保证可以拉取到即可。\n\n```\nROOK_CSI_CEPH_IMAGE: \"registry.aliyuncs.com/google_containers/cephcsi:v3.10.2\"\nROOK_CSI_REGISTRAR_IMAGE: \"registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.9.1\"\nROOK_CSI_RESIZER_IMAGE: \"registry.aliyuncs.com/google_containers/csi-resizer:v1.9.2\"\nROOK_CSI_PROVISIONER_IMAGE: \"registry.aliyuncs.com/google_containers/csi-provisioner:v3.6.3\"\nROOK_CSI_SNAPSHOTTER_IMAGE: \"registry.aliyuncs.com/google_containers/csi-snapshotter:v6.3.2\"\nROOK_CSI_ATTACHER_IMAGE: \"registry.aliyuncs.com/google_containers/csi-attacher:v4.4.2\"\n```\n\n![](/images/image-hdxy.png)\n\n##### **开始部署**\n\n```\n kubectl apply -f crds.yaml -f common.yaml -f operator.yaml\n```\n\n等待所有pod运行\n\n![](/images/image-ypep.png)\n\n如果这是你第一次安装，不需要初始化，否则需要将磁盘清空。`谨慎操作!!!`\n\n```\nDISK=$1\n​\n# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)\nsgdisk --zap-all $DISK\n​\n# Wipe a large portion of the beginning of the disk to remove more LVM metadata that may be present\ndd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\n​\n# SSDs may be better cleaned with blkdiscard instead of dd\nblkdiscard $DISK\n​\n# Inform the OS of partition table changes\npartprobe $DISK\n​\n# This command hangs on some systems: with caution, 'dmsetup remove_all --force' can be used\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % &> /dev/null\n​\n# ceph-volume setup can leave ceph-<UUID> directories in /dev and /dev/mapper (unnecessary clutter)\nrm -rf /dev/ceph-*\nrm -rf /dev/mapper/ceph--*\nrm -rf /var/lib/rook/\n```\n\n集群中每个节点都运行以上脚本初始化磁盘\n\n![](/images/image-fwwu.png)\n\n```\n$ ./cleanup.sh /dev/sdx # 将sdb盘清空，并删除ceph遗留文件\n```\n\n![](/images/image-jjku.png)\n\n### **部署集群**\n\n修改cluster.yaml文件中的storage，如果想使用集群中所有的节点上的所有存储，使用默认配置即可\n\n![](/images/image-bywk.png)\n\n同时支持定义指定节点指定磁盘。如下，指定k8s-master01、2、3中的sd开头的磁盘\n\n```\nstorage: # cluster level storage configuration and selection\n    useAllNodes: false\n    useAllDevices: false\n    deviceFilter:\n    config:\n    nodes:\n      - name: \"k8s-master01\"\n        deviceFilter: \"^sd.\"\n      - name: \"k8s-master02\"\n        deviceFilter: \"^sd.\"\n      - name: \"k8s-master03\"\n        deviceFilter: \"^sd.\"\n```\n\n如果指定单个磁盘\n\n```\nstorage: # cluster level storage configuration and selection\n    useAllNodes: false\n    useAllDevices: false\n    config:\n    nodes:\n      - name: \"k8s-master01\"\n        devices: \n          - name: \"sdb\"\n      - name: \"k8s-master02\"\n        devices: \n          - name: \"sdb\"\n      - name: \"k8s-master03\"\n        devices: \n          - name: \"sdb\"\n```\n\n开始部署\n\n```\nkubectl apply -f cluster.yaml\n```\n\n![](/images/image-ibhg.png)\n\n检查pod状态\n\n![](/images/image-dgtq.png)\n\n### **部署工具**\n\n```\nkubectl apply -f toolbox.yaml\n```\n\n```\nkubectl exec -it -n rook-ceph rook-ceph-tools-66b77b8df5-b7r22 -- bash\n```\n\n```\nceph -s # 检查状态\n```\n\n![](/images/image-epbe.png)\n\n### **部署StorageClass并测试**\n\n#### 使用cephfs\n\nCephFS 共享文件系统（Shared Filesystem）适用于多个Pod之间共享读写（RWX）的存储\n\n```\nkubectl apply -f filesystem.yaml\n```\n\n等待文件系统状态ready\n\n![](/images/image-lzxv.png)\n\n部署storageclass\n\n```\ncd csi/cephfs\nkubectl apply -f storageclass.yaml\n```\n\n检查状态\n\n![](/images/image-ghha.png)\n\n##### **测试cephfs申请动态pvc**\n\n```\nkubectl apply -f pvc.yaml\n```\n\n绑定成功\n\n![](/images/image-jout.png)\n\n#### 使用rbd存储\n\n块存储（Block）适用于为单个 Pod 提供读写一致性（RWO）的存储\n\n```\ncd csi/rbd\nkubectl apply -f storageclass.yaml\n```\n\n![](/images/image-npfr.png)\n\n##### **测试rbd申请动态pvc**\n\n```\nkubectl apply -f pvc.yaml\n```\n\n![](/images/image-xtii.png)\n\n#### 使用对象存储\n\n对象存储（Object）提供了一个可通过内部或外部的Kubernetes集群的S3端点访问的存储\n\n```\nkubectl apply -f object.yaml\n```\n\n等待状态ready\n\n![](/images/image-jnea.png)\n\n部署storageclass\n\n```\nkubectl create -f storageclass-bucket-delete.yaml\n```\n\n![](/images/image-qogf.png)\n\n##### **测试使用对象存储**\n\n创建OBC对象存储桶\n\n```\n kubectl apply -f object-bucket-claim-delete.yaml\n```\n\n![](/images/image-uitq.png)\n\n##### **安装测试工具s5cmd**\n\n```\nwget https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\ntar -xvf s5cmd_2.2.2_Linux-64bit.tar.gz\nmv s5cmd /usr/local/bin/\n```\n\n###### 设置凭证\n\n可以使用一下命令生成，或者手动获取OBC同名secret，base64解码获取并写到配置文件\n\n```\nexport AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode)\nexport AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode)\n\nmkdir ~/.aws\ncat > ~/.aws/credentials << EOF\n[default]\naws_access_key_id = ${AWS_ACCESS_KEY_ID}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\nEOF\n```\n\n##### **测试获取桶列表**\n\n列出当前凭据可以访问的所有桶\n\n```\nkubectl get svc -n rook-ceph\n```\n\n![](/images/image-hqgp.png)\n\n```\ns5cmd --endpoint-url  http://10.107.248.38 ls\n```\n\n![](/images/image-wvrs.png)\n\n桶名称与configmap对应即可。\n\n## **卸载**\n\n清空ceph的pvc资源（根据环境删除）\n\n```\nkubectl delete pvc rbd-pvc cephfs-pvc\n```\n\n删除storageclass\n\n```\nkubectl delete sc rook-cephfs rook-ceph-block\nkubectl delete -f filesystem.yaml # 如果没有部署cephfs就不需要删除\n```\n\n卸载集群\n\n```\nkubectl delete -f cluster.yaml\n或者\nkubectl delete cephcluster -n rook-ceph rook-ceph\n```\n\n卸载工具\n\n```\nkubectl delete -f toolbox.yaml\n```\n\n卸载operator\n\n```\nkubectl apply -f crds.yaml -f common.yaml -f operator.yaml\n```\n\n清理磁盘\n\n```\n$ ./cleanup.sh /dev/sdx \n```\n","tags":["Kubernetes","ceph"]},{"title":"【云原生】Rocky Linux 9.4 部署高可用Kubernetes v1.30.2","url":"/2024/06/16/【云原生】Rocky Linux 9.4 部署高可用Kubernetes v1.30.2/","content":"\n### **配置网络**\n\n> Rocky 9.4使用NetworkManager管理网络\n\n```\nvim /etc/NetworkManager/system-connections/ens5f0.nmconnection\n```\n\n```\n[connection]\nid=ens5f0\nuuid=48245c90-e7e7-3e84-8f6d-a1f8cf5cd9e1\ntype=ethernet\nautoconnect-priority=-999\ninterface-name=ens5f0\n​\n[ethernet]\ncloned-mac-address=random\n​\n[ipv4]\naddress1=172.25.2.4/24,172.25.2.1\ndns=218.2.2.2;\nmethod=manual\n​\n[ipv6]\naddr-gen-mode=eui64\nmethod=auto\n​\n[proxy]\n```\n\n```\n# 设置时区（所有节点）\ntimedatectl set-timezone Asia/Shanghai\n# 24小时制\nlocalectl set-locale LC_TIME=en_GB.UTF-8\n​\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \n# 从公网同步\npool ntp.aliyun.com iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度\ndriftfile /var/lib/chrony/drift\n# 当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整\nmakestep 1.0 3\n# 启用硬件时钟同步功能，可以提高时钟的准确性\nrtcsync\n# 允许10.20.13.0/24网段范围内的主机与chrony进行时间同步\nallow 0.0.0.0/24\n# 将本地时钟设为stratum 10，stratum值表示时钟的准确度，值越小表示准确度越高\nlocal stratum 10\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证\nkeyfile /etc/chrony.keys\n# 指定时区为UTC\nleapsectz right/UTC\n# 指定日志文件存放目录\nlogdir /var/log/chrony\nEOF\nsystemctl restart chronyd ; systemctl enable chronyd\n​\n# 客户端\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \n# 从服务端同步\npool 172.25.2.4 iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度\ndriftfile /var/lib/chrony/drift\n# 当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整\nmakestep 1.0 3\n# 启用硬件时钟同步功能，可以提高时钟的准确性\nrtcsync\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证\nkeyfile /etc/chrony.keys\n# 指定时区为UTC\nleapsectz right/UTC\n# 指定日志文件存放目录\nlogdir /var/log/chrony\nEOF\nsystemctl restart chronyd ; systemctl enable chronyd\n​\n#使用客户端进行验证\nchronyc sources -v\n```\n\n#### **关闭防火墙**\n\n```\n# 关闭防火墙\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X && iptables -P FORWARD ACCEPT\nsystemctl disable firewalld --now\n```\n\n#### **禁用SELinux**\n\n```\n# 禁用selinux\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=disabled/g' /etc/selinux/config\n```\n\n#### **关闭swap**\n\n```\n# 临时关闭swap\nswapoff -a\n# 永久关闭swap\nsed -i 's/.*swap.*/#&/g' /etc/fstab\n```\n\n#### **安装系统工具**\n\n```\nyum update -y && yum -y install wget psmisc jq vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl\n```\n\n#### **加载IPVS模块**\n\n```\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\ncat > /etc/modules-load.d/k8s.conf <<EOF \n# IPVS 是 Linux 内核中的一个模块，用于实现负载均衡和高可用性。它通过在前端代理服务器上分发传入请求到后端实际服务器上，提供了高性能和可扩展的网络服务\nip_vs\n# IPVS 轮询调度算法\nip_vs_rr\n# IPVS 加权轮询调度算法\nip_vs_wrr\n# IPVS 哈希调度算法\nip_vs_sh\n# overlay是containerd默认使用的存储驱动，它提供了一种轻量级的、可堆叠的、逐层增量的文件系统,它通过在现有文件系统上叠加文件系统层来创建容器的文件系统视图。每个容器可以有自己的一组文件系统层，这些层可以共享基础镜像中的文件，并在容器内部进行修改。使用overlay可以有效地使用磁盘空间，并使容器更加轻量级\noverlay\n# nf_conntrack用于跟踪和管理网络连接，包括 TCP、UDP 和 ICMP 等协议。它是实现防火墙状态跟踪的基础\nnf_conntrack\n# ip_tables提供了对 Linux 系统 IP 数据包过滤和网络地址转换（NAT）功能的支持\nip_tables\n# 扩展了 iptables 的功能，支持更高效的 IP 地址集合操作\nip_set\n# 扩展了 iptables 的功能，支持更高效的数据包匹配和操作\nxt_set\n# 用户空间工具，用于配置和管理 xt_set 内核模块\nipt_set\n# 用于实现反向路径过滤，用于防止 IP 欺骗和 DDoS 攻击\nipt_rpfilter\n# 用于拒绝 IP 数据包，并向发送方发送响应，指示数据包被拒绝\nipt_REJECT\n# 用于实现 IP 封装在 IP（IP-over-IP）的隧道功能。它可以在不同网络之间创建虚拟隧道来传输 IP 数据包\nipip\nEOF\n​\nsystemctl restart systemd-modules-load.service\n```\n\n#### **配置ulimit**\n\n```\nulimit -SHn 65535\ncat > /etc/security/limits.conf <<EOF\n# soft表示软限制，nofile表示一个进程可打开的最大文件数，默认值为1024。这里的软限制设置为655360，即一个进程可打开的最大文件数为655360\n* soft nofile 655360\n# hard表示硬限制，即系统设置的最大值。nofile表示一个进程可打开的最大文件数，默认值为4096。这里的硬限制设置为131072，即系统设置的最大文件数为131072\n* hard nofile 131072\n# nproc表示一个用户可创建的最大进程数，默认值为30720。即一个用户可创建的最大进程数为655350\n* soft nproc 655350\n# nproc表示一个用户可创建的最大进程数，默认值为4096。即系统设置的最大进程数为655350\n* hard nproc 655350\n# memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的软限制设置为unlimited，即一个进程可锁定的最大内存为无限制\n* seft memlock unlimited\n# memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。即系统设置的最大内存锁定为无限制\n* hard memlock unlimitedd\nEOF\n```\n\n#### **修改内核参数**\n\n```\ncat > /etc/sysctl.d/k8s.conf << EOF\n# 启用了IPv4的IP转发功能，允许服务器作为网络路由器转发数据包\nnet.ipv4.ip_forward = 1\n# 当使用网络桥接技术时，将数据包传递到iptables进行处理\nnet.bridge.bridge-nf-call-iptables = 1\n# 当该参数设置为1时，IPv6数据包将被传递到ip6tables进行处理；当该参数设置为0时，IPv6数据包将绕过ip6tables直接传递。默认情况下，这个参数的值是1\nnet.bridge.bridge-nf-call-ip6tables = 1\n# 允许在挂载文件系统时，允许被其他进程使用\nfs.may_detach_mounts = 1\n# 允许原始的内存过量分配策略，当系统的内存已经被完全使用时，系统仍然会分配额外的内存\nvm.overcommit_memory=1\n# 当系统内存不足（OOM）时，禁用系统崩溃和重启\nvm.panic_on_oom=0\n# 设置系统允许一个用户的inotify实例可以监控的文件数目的上限\nfs.inotify.max_user_watches=89100\n# 设置系统同时打开的文件数的上限\nfs.file-max=52706963\n# 设置系统同时打开的文件描述符数的上限\nfs.nr_open=52706963\n# 设置系统可以创建的网络连接跟踪表项的最大数量\nnet.netfilter.nf_conntrack_max=2310720\n# 设置TCP套接字的空闲超时时间（秒），超过该时间没有活动数据时，内核会发送心跳包\nnet.ipv4.tcp_keepalive_time = 600\n# 设置未收到响应的TCP心跳探测次数\nnet.ipv4.tcp_keepalive_probes = 3\n# 设置TCP心跳探测的时间间隔（秒）\nnet.ipv4.tcp_keepalive_intvl =15\n# 设置系统可以使用的TIME_WAIT套接字的最大数量\nnet.ipv4.tcp_max_tw_buckets = 36000\n# 启用TIME_WAIT套接字的重新利用，允许新的套接字使用旧的TIME_WAIT套接字\nnet.ipv4.tcp_tw_reuse = 1\n# 设置系统可以同时存在的TCP套接字垃圾回收包裹数的最大数量\nnet.ipv4.tcp_max_orphans = 327680\n# 设置系统对于孤立的TCP套接字的重试次数\nnet.ipv4.tcp_orphan_retries = 3\n# 启用TCP SYN cookies保护，用于防止SYN洪泛攻击\nnet.ipv4.tcp_syncookies = 1\n# 设置新的TCP连接的半连接数（半连接队列）的最大长度\nnet.ipv4.tcp_max_syn_backlog = 16384\n# 设置系统可以创建的网络连接跟踪表项的最大数量\nnet.ipv4.ip_conntrack_max = 65536\n# 关闭TCP时间戳功能，用于提供更好的安全性\nnet.ipv4.tcp_timestamps = 0\n# 设置系统核心层的连接队列的最大值\nnet.core.somaxconn = 16384\n​\n# 启用IPv6协议\nnet.ipv6.conf.all.disable_ipv6 = 0\n# 启用IPv6协议\nnet.ipv6.conf.default.disable_ipv6 = 0\n# 启用IPv6协议\nnet.ipv6.conf.lo.disable_ipv6 = 0\n# 允许IPv6数据包转发\nnet.ipv6.conf.all.forwarding = 1\nEOF\n​\nsysctl --system\n```\n\n### **安装containerd**\n\n```\n# 安装系统工具\nyum install -y yum-utils device-mapper-persistent-data lvm2\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 修改阿里源地址\nsed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n```\n\n```\nyum -y install containerd\nsystemctl enable containerd --now\ncontainerd -v\n```\n\n#### **加载模块**\n\n```\ncat > /etc/modules-load.d/containerd.conf << EOF\noverlay\nbr_netfilter\nEOF\nsystemctl restart systemd-modules-load\n```\n\n#### **修改内核参数**\n\n```\ncat > /etc/sysctl.d/99-kubernetes-cri.conf << EOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\n```\n\n#### **修改配置文件**\n\n```\nmkdir -p /etc/containerd\n# 生成默认配置文件\ncontainerd config default > /etc/containerd/config.toml\n# SystemdCgroup参数的作用是为了确保containerd能够正确地管理容器的资源使用，以实现资源的限制、隔离和公平分配\nsed -i \"s#SystemdCgroup\\ \\=\\ false#SystemdCgroup\\ \\=\\ true#g\" /etc/containerd/config.toml\n# 修改镜像拉取地址为国内地址，这里是pause镜像地址\nsed -i \"s#registry.k8s.io#registry.aliyuncs.com/google_containers#g\" /etc/containerd/config.toml\n# 指定配置文件目录\nsed -i \"s#config_path\\ \\=\\ \\\"\\\"#config_path\\ \\=\\ \\\"/etc/containerd/registry\\\"#g\" /etc/containerd/config.toml\nsed -i 's/pause:3.6/pause:3.9/g'  /etc/containerd/config.toml\n# 设置镜像加速\nmkdir /etc/containerd/registry/docker.io -pv\ncat > /etc/containerd/registry/docker.io/hosts.toml << EOF\nserver = \"https://docker.io\"\n[host.\"https://docker.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\n[host.\"https://xk9ak4u9.mirror.aliyuncs.com\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://dockerproxy.com\"]\n  capabilities = [\"pull\", \"resolve\"]\n[host.\"https://docker.mirrors.sjtug.sjtu.edu.cn\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://docker.mirrors.ustc.edu.cn\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://docker.nju.edu.cn\"] \n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://registry-1.docker.io\"]\n  capabilities = [\"pull\",\"resolve\",\"push\"]\nEOF\n​\nmkdir /etc/containerd/registry/gcr.io -pv\ncat > /etc/containerd/registry/gcr.io/hosts.toml << EOF\nserver = \"https://gcr.io\"\n[host.\"https://gcr.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n​\nmkdir /etc/containerd/registry/registry.k8s.io -pv\ncat > /etc/containerd/registry/registry.k8s.io/hosts.toml << EOF\nserver = \"https://registry.k8s.io\"\n[host.\"https://k8s.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n​\nmkdir /etc/containerd/registry/k8s.gcr.io -pv\ncat > /etc/containerd/registry/k8s.gcr.io/hosts.toml << EOF\nserver = \"https://k8s.gcr.io\"\n[host.\"https://k8s.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n​\nmkdir /etc/containerd/registry/quay.io -pv\ncat > /etc/containerd/registry/quay.io/hosts.toml << EOF\nserver = \"https://quay.io\"\n[host.\"https://quay.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n```\n\n#### **启动并设置开机自启**\n\n```\nsystemctl daemon-reload\nsystemctl enable containerd --now\nsystemctl restart containerd\nsystemctl status containerd\n```\n\n#### **安装crictl**\n\n下载地址：[Releases · kubernetes-sigs/cri-tools (](https://github.com/kubernetes-sigs/cri-tools/releases)[github.com](http://github.com)[)](https://github.com/kubernetes-sigs/cri-tools/releases)\n\n```\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.30.0/crictl-v1.30.0-linux-amd64.tar.gz\ntar -zxf crictl-v1.30.0-linux-amd64.tar.gz \nmv crictl /usr/local/bin/\n```\n\n```\ncat > /etc/crictl.yaml <<EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\ndebug: false\npull-image-on-create: false\nEOF\n```\n\n#### **安装Kube-vip**\n\n```\n# 设置vip，前提先ping一下，确保IP没有被占用\nexport VIP=172.25.2.50\n# 指定网卡\nexport INTERFACE=ens5f0\n# 获取最新版本\n# KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r \".[0].name\")\n# 或者指定版本（这个版本目前比较稳定，不会出现报错）\nexport KVVERSION=v0.6.4\n# 安装kube-vip镜像\n# ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION; \nalias kube-vip=\"ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip\"\n```\n\n```\n# 生成清单文件\nmkdir -p /etc/kubernetes/manifests\n​\nkube-vip manifest pod \\\n    --interface $INTERFACE \\\n    --address $VIP \\\n    --controlplane \\\n    --services \\\n    --arp \\\n    --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n```\n# 1.29版本以后需要对kube-vip修改kubernetes客户端路径\nsed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \\\n          /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n### **安装k8s工具**\n\n版本查看：[kubernetes-new-core-stable安装包下载\\_开源镜像站-阿里云 (](https://mirrors.aliyun.com/kubernetes-new/core/stable/)[aliyun.com](http://aliyun.com)[)](https://mirrors.aliyun.com/kubernetes-new/core/stable/)\n\n```\n# 指定安装的k8s工具版本\nexport k8sVersion=v1.30\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/repodata/repomd.xml.key\nEOF\nyum install -y kubelet kubeadm kubectl\nsystemctl enable kubelet --now\n```\n\n### **初始化k8s**\n\n```\ncat > kubeadm.yml << EOF\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 172.25.2.4 # 修改自己的ip\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  name: k8s-h3c-master01 # 本机的主机名\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/k8s-master\n---\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - 172.25.2.50 # vip\n  - 172.25.2.4 # master01\n  - 172.25.2.5 # master02\n  - 172.25.2.6 # master03\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\n# 控制平面高可用入口，所有的高可用操作，最终都是为了这个位置的ip\ncontrolPlaneEndpoint: 172.25.2.50:6443\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.30.2\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n---\n# 配置ipvs\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n---\n# 指定cgroup为systemd\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\nEOF\n```\n\n```\n# 查看所需镜像列表\nkubeadm config images list --config kubeadm.yml\n```\n\n```\n# 预拉取镜像\nkubeadm config images pull --config kubeadm.yml\n```\n\n```\n# 初始化\nkubeadm init --config=kubeadm.yml --upload-certs\n```\n\n```\n# 配置 kubectl\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n#### **安装命令自动补齐**\n\n```\nyum install bash-completion -y\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\n```\n\n#### **新增工作节点**\n\n```\n# 生成加入节点命令\nkubeadm token create --print-join-command\n```\n\n#### **新增控制节点**\n\n```\n# 生成加入节点命令\nkubeadm token create --print-join-command\n# 生成控制节点certificate-key \nkubeadm init phase upload-certs --upload-certs\n```\n\n通过`--control-plane --certificate-key`拼接命令\n\n> kubeadm join 10.20.13.100:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxx **--control-plane --certificate-key** xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n### **安装CNI网络插件**\n\nCalico：[CNI插件-使用calico支持IPv4/IPv6双协议栈 - (](https://sreok.cn/archives/b72d4657-a17a-4209-a4fd-b631e7d06422)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/b72d4657-a17a-4209-a4fd-b631e7d06422)\n","tags":["etcd"],"categories":["云原生"]},{"title":"【云原生】Kubernetes v1.30 高可用集群支持IPv4IPv6双栈（kubeadm方式）","url":"/2024/06/10/【云原生】Kubernetes v1.30 高可用集群支持IPv4IPv6双栈（kubeadm方式）/","content":"\n### 1、初始化系统\n\n原文地址：[【云原生】k8s节点系统初始化+内核优化 - (](http://localhost:4000/2022/05/14/%E3%80%90%E4%BA%91%E5%8E%9F%E7%94%9F%E3%80%91k8s%E8%8A%82%E7%82%B9%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96+%E5%86%85%E6%A0%B8%E4%BC%98%E5%8C%96/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/942c699f-2ad0-428e-809c-9957a2e331ce)\n\n### **2、安装容器运行时**\n\ncontainerd原文地址：[【笔记】使用Containerd作为Kubernetes Runtime - (](http://localhost:4000/2023/05/12/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8Containerd%E4%BD%9C%E4%B8%BAKubernetes%20Runtime/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/4896568f-2a8e-4a1d-bad6-de894920131d)\n\ndocker原文地址：[【笔记】使用Docker作为Kubernetes Runtime - (](http://localhost:4000/2023/05/12/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8Docker%E4%BD%9C%E4%B8%BAKubernetes%20Runtime/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/8da2f508-f580-4a8d-9b51-55856a2f7b16)\n\n### **3、安装k8s工具**\n\n```\n# 指定安装的k8s工具版本，版本查看：https://mirrors.aliyun.com/kubernetes-new/core/stable/\nexport k8sVersion=v1.30\n```\n\n```\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/repodata/repomd.xml.key\nEOF\n```\n\n```\nyum install -y kubelet kubeadm kubectl\nsystemctl enable kubelet --now\n```\n\n### **4、高可用负载入口**\n\n`单主集群跳过此步骤`\n\n**方式一： kube-vip**\n\n原文地址：[【云原生】高可用负载方案：使用kube-vip作为控制平面负载入口 - (](http://localhost:4000/2023/11/23/%E3%80%90%E4%BA%91%E5%8E%9F%E7%94%9F%E3%80%91%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E6%96%B9%E6%A1%88%EF%BC%9A%E4%BD%BF%E7%94%A8kube-vip%E4%BD%9C%E4%B8%BA%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%B4%9F%E8%BD%BD%E5%85%A5%E5%8F%A3/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/9d06c7be-1885-4df7-bfbf-b80ed16e4d47)\n\n**方式二：Keepalived + HAProxy**\n\n原文地址：[【云原生】高可用负载方案：使用Keepalived + HAProxy作为控制平面负载入口 - (](http://localhost:4000/2023/11/12/%E3%80%90%E4%BA%91%E5%8E%9F%E7%94%9F%E3%80%91%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E6%96%B9%E6%A1%88%EF%BC%9A%E4%BD%BF%E7%94%A8Keepalived%20+%20HAProxy%E4%BD%9C%E4%B8%BA%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%B4%9F%E8%BD%BD%E5%85%A5%E5%8F%A3/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/642868c1-f000-4476-85ad-cbeaf9e53fd4)\n\n### **5、初始化集群**\n\n#### 可选一：单节点-**不支持ipv6双协议栈**\n\n`不支持新增控制平面，因为控制平面的IP已经固定`\n\n`仅支持外部解析ipv4地址访问业务`\n\n```\ncat > kubeadm.yml << EOF\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.20.13.10 # 修改自己的ip\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  name: k8s-master01 # 本机的主机名\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/k8s-master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\n# 控制平面高可用入口，所有的高可用操作，最终都是为了这个位置的ip\n# controlPlaneEndpoint: 10.20.13.100:6443\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.30.0 # 版本必须与kubeadm版本一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n---\n# 配置ipvs\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n---\n# 指定cgroup为systemd\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\nEOF\n```\n\n#### 可选二：单节点-支持ipv4/ipv6双协议栈\n\n`不支持新增控制平面，因为控制平面的IP已经固定`\n\n`支持外部解析ipv4/ipv6地址访问业务`\n\n```\ncat > kubeadm.yml << EOF\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.20.13.10 # 修改自己的ip\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  kubeletExtraArgs:\n    # 这里使用maser01的IP \n    node-ip: 10.20.13.10,2031:0:130c::10\n  name: k8s-master01 # 本机的主机名\n  taints:\n  - effect: PreferNoSchedule\n    key: node-role.kubernetes.io/k8s-master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\n# 控制平面高可用入口，所有的高可用操作，最终都是为了这个位置的ip\n# controlPlaneEndpoint: 10.20.13.100:6443\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.30.0 # 版本必须与kubeadm版本一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16,2000::/64\n  serviceSubnet: 10.96.0.0/12,3000::/112\nscheduler: {}\n---\n# 配置ipvs\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n---\n# 指定cgroup为systemd\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\nEOF\n```\n\n#### **可选三：高可用-不支持ipv6双协议栈**\n\n`支持新增控制平面，api-server通过负载ip访问，控制平面高可用`\n\n`仅支持外部解析ipv4地址访问业务`\n\n```\nkubeadm config print init-defaults > kubeadm.yml\n```\n\n```\ncat > kubeadm.yml << EOF\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.20.13.10 # 修改自己的ip\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  name: k8s-master01 # 本机的主机名\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/k8s-master\n---\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - 10.20.13.100 # vip\n  - 10.20.13.10 # master01\n  - 10.20.13.11 # master02\n  - 10.20.13.12 # master03\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\n# 控制平面高可用入口，所有的高可用操作，最终都是为了这个位置的ip\ncontrolPlaneEndpoint: 10.20.13.100:6443\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.30.0 # 版本必须与kubeadm版本一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n---\n# 配置ipvs\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n---\n# 指定cgroup为systemd\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\nEOF\n```\n\n```\nkubeadm init --config=kubeadm.yml --upload-certs\n```\n\n#### **可选四：高可用-**支持ipv4/ipv6双协议栈\n\n`支持新增控制平面，api-server通过负载ip访问，控制平面高可用`\n\n`支持外部解析ipv4/ipv6地址访问业务`\n\n```\ncat > kubeadm.yml << EOF\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.20.13.10 # 修改自己的ip\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  kubeletExtraArgs:\n    # 这里使用maser01的IP \n    node-ip: 10.20.13.10,2031:0:130c::10\n  name: k8s-master01 # 本机的主机名\n  taints:\n  - effect: PreferNoSchedule\n    key: node-role.kubernetes.io/k8s-master\n---\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - 10.20.13.100 # vip\n  - 10.20.13.10 # master01\n  - 10.20.13.11 # master02\n  - 10.20.13.12 # master03\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\n# 控制平面高可用入口，所有的高可用操作，最终都是为了这个位置的ip\ncontrolPlaneEndpoint: 10.20.13.100:6443\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.30.0 # 版本必须与kubeadm版本一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16,2000::/64\n  serviceSubnet: 10.96.0.0/12,3000::/112\nscheduler: {}\n---\n# 配置ipvs\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n---\n# 指定cgroup为systemd\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\nEOF\n```\n\n```\nkubeadm init --config=kubeadm.yml --upload-certs\n```\n\n> 需要注意：上面无论是单节点还是高可用，初始化后都是单节点，高可用可以在后面操作新增控制节点。\n\n#### 配置命令行\n\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n#### 新增工作节点\n\n```\n# 生成加入节点命令\nkubeadm token create --print-join-command\n```\n\n#### 新增控制节点\n\n```\n# 生成加入节点命令\nkubeadm token create --print-join-command\n```\n\n```\n# 生成控制节点certificate-key \nkubeadm init phase upload-certs --upload-certs\n```\n\n通过`--control-plane --certificate-key`拼接命令\n\n> kubeadm join 10.20.13.100:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxx **--control-plane --certificate-key** xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n#### 安装命令自动补齐\n\n```\nyum install bash-completion -y\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\n```\n\n### 6、安装CNI插件\n\n原文地址：[【云原生】Kubernetes 安装CNI插件calico并设置双协议栈- (](http://localhost:4000/2022/05/16/%E3%80%90%E4%BA%91%E5%8E%9F%E7%94%9F%E3%80%91Kubernetes%20%E5%AE%89%E8%A3%85CNI%E6%8F%92%E4%BB%B6calico%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%8F%8C%E5%8D%8F%E8%AE%AE%E6%A0%88/)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/b72d4657-a17a-4209-a4fd-b631e7d06422)\n\n### **卸载集群**\n\n```\nkubeadm reset -f\nrm -rf /etc/kubernetes/\nrm -rf /var/lib/kubelet/\nrm -rf /var/lib/etcd/\nrm -rf $HOME/.kube/\nrm -rf /opt/cni/\nrm -rf /etc/cni/net.d/\nsystemctl restart network\n```\n","tags":["Kubernetes"],"categories":["云原生"]},{"title":"【云原生】使用trust-manager将TLS证书同步到其他命名空间","url":"/2024/05/13/【云原生】使用trust-manager将TLS证书同步到其他命名空间/","content":"\n### **安装**\n\n```\nhelm repo add jetstack https://charts.jetstack.io --force-update\nhelm upgrade --install trust-manager jetstack/trust-manager \\\n--namespace cert-manager \\\n--set secretTargets.enabled=true \\\n--set secretTargets.authorizedSecretsAll=true \\\n--set app.trust.namespace=cert-manager\n```\n\n### **创建Bundle**\n\n```\ncat > harbor-tls.yaml << EOF\napiVersion: trust.cert-manager.io/v1alpha1\nkind: Bundle\nmetadata:\n  # bundle名称被用为target secret名称\n  name: harbor-tls\nspec:\n  sources:\n  # 基于secret同步，secret必须存在于cert-manager命名空间（bundle是全局资源，不能通过设置namaespace来使用其他命名空间的secret）\n  - secret:\n      name: \"devops-selfsigned-secret\"\n      key: \"ca.crt\"\n  target:\n    # 目标是secret，安装时必须开启secretTargets.enabled和secretTargets.authorizedSecretsAll\n    secret:\n      key: \"ca.crt\"\n    # label存在的命名空间才创建secret，删除以下内容，即在所有命名空间创建secret\n    namespaceSelector:\n      matchLabels:\n        devops: \"yes\"\nEOF\n```\n\n```\nkubectl apply -f harbor-tls.yaml\n```\n\n### **验证**\n\n```\n[root@k8s-h3c-master01 harbor]# kubectl get bundles.trust.cert-manager.io\nNAME         CONFIGMAP TARGET   SECRET TARGET   SYNCED   REASON   AGE\nharbor-tls                      ca.crt          True     Synced   39s\n```\n\n```\n[root@k8s-h3c-master01 harbor]# kubectl get namespaces --show-labels\nNAME              STATUS   AGE     LABELS\ncert-manager      Active   4d2h    kubernetes.io/metadata.name=cert-manager\ndefault           Active   7d12h   kubernetes.io/metadata.name=default\ndevops            Active   4d7h    devops=yes,kubernetes.io/metadata.name=devops\n```\n\n```\n[root@k8s-h3c-master01 harbor]# kubectl get secrets -n devops\nNAME                                      TYPE                 DATA   AGE\nharbor-tls                                Opaque               1      42s\n```\n\n#### **查看ca.crt**\n\n```\nkubectl get secrets -n cert-manager devops-selfsigned-secret  -o yaml\n```\n\n```\nkubectl get secrets -n devops harbor-tls -o yaml\n```\n\n验证以上secret中的ca.crt内容是否一致\n\n### 示例：（将cert-manager生成的harborTLS证书同步给全局）\n\n安装trust-manager时指定trust命名空间到TLS证书所在命名空间\n\n```\nhelm upgrade --install trust-manager jetstack/trust-manager \\\n--namespace cert-manager \\\n--set secretTargets.enabled=true \\\n--set secretTargets.authorizedSecretsAll=true \\\n--set app.trust.namespace=devops\n```\n\n#### 创建bundle\n\n```\napiVersion: trust.cert-manager.io/v1alpha1\nkind: Bundle\nmetadata:\n  name: harbor-tls\nspec:\n  sources:\n  - secret:\n      name: \"harbor-selfsigned-secret\"\n      key: \"ca.crt\"\n  target:\n    configMap:\n      key: \"ca.crt\"\n```\n\n#### 验证\n\n```\n[root@k8s-master01 ~]# kubectl get bundles.trust.cert-manager.io\nNAME         CONFIGMAP TARGET   SECRET TARGET   SYNCED   REASON   AGE\nharbor-tls   ca.crt                             True     Synced   3m18s\n\n# 所有命名空间下都有这个configMap\n[root@k8s-master01 ~ ]# kubectl get cm\nNAME               DATA   AGE\nharbor-tls         1      2m16s\n```\n\n### **卸载**\n\n```\nhelm uninstall trust-manager -n cert-manager\nkubectl delete crd bundles.trust.cert-manager.io\n```\n","tags":["trust-manager"],"categories":["云原生"]},{"title":"【云原生】使用cert-manager 生成自签名证书","url":"/2024/05/07/【云原生】使用cert-manager 生成自签名证书/","content":"\n`自签名证书一般用于服务间或内网访问使用，在公网访问会有不安全提示。`\n\n## **前提条件**\n\n* k8s集群<1.19\n\n* cert-manager\n\n* ingress-nginx\n\n## 方式一：使用自签名作为根证书（简单明了）\n\n### **创建ClusterIssuer**\n\n```\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n   name: selfsigned\nspec:\n   selfSigned: {}\n```\n\n![](/images/image-fmvw.png)\n\n### **创建证书**\n\n```\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: selfsigned-test\n  namespace: devops\n  #namespace: cert-manager\nspec:\n  isCA: false\n  dnsNames: # dns列表\n  - sreok.cn\n  commonName: \"*.sreok.cn\"\n  secretName: selfsigned-cert-tls\n  duration: 876000h\n  renewBefore: 8760h\n  subject:\n    countries:\n    - China\n    localities:\n    - NanJing\n    organizations:\n    - sreok\n    organizationalUnits:\n    - devops\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    name: selfsigned # 与clusterissusr对应\n    kind: ClusterIssuer\n    group: cert-manager.io\n```\n\n检查证书状态，True即可使用\n\n![](/images/image-gsha.png)\n\n### **ingress配置**\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    cert-manager.io/cluster-issuer: selfsigned\nspec:\n  ingressClassName: nginx\n  tls:\n    - secretName: selfsigned-test # 证书名,与certificate一致\n      hosts:\n        - example.com # 证书域名\n  rules:\n    - host: example.com # 访问域名\n      http:\n        paths:\n          - path: /\n            #pathType: ImplementationSpecific\n            pathType: Prefix\n            backend:\n              service:\n                name: demo # 服务名\n                port:\n                  number: 80 # 服务的端口号 service port，非pod port\n```\n\n![](/images/image-honq.png)\n\n## 方式二：使用CA生成链式证书（复杂、安全）\n\n### 创建根证书\n\n```\n# 创建根证书\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: selfsigned\nspec:\n  selfSigned: {}\n---\n# 创建CA\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: devops-ca\n  # 如果要根据此CA创建ClusterIssuer（集群范围）必须在cert-manager命名空间\n  namespace: cert-manager\nspec:\n  isCA: true\n  commonName: \"*.sreok.cn\"\n  secretName: devops-selfsigned-secret\n  # 100年\n  duration: 876000h\n  # 1年时更新\n  renewBefore: 8760h\n  # 证书信息\n  subject:\n    countries:\n    - China\n    localities:\n    - NanJing\n    organizations:\n    - sreok\n    organizationalUnits:\n    - devops\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    name: selfsigned\n    kind: ClusterIssuer\n    group: cert-manager.io\n```\n\n### 颁发CA证书\n\n```\n# 基于CA创建ClusterIssuer\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: devops-issuer\nspec:\n  ca:\n    secretName: devops-selfsigned-secret\n```\n\n### 基于CA颁发TLS证书\n\n```\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: selfsigned-tls\n  # ingress在哪个命名空间就写哪个\n  namespace: devops\nspec:\n  # TLS证书需要禁止CA\n  isCA: false\n  usages:\n    - server auth\n    - client auth\n  # dns必须设置\n  dnsNames:\n  - abc.sreok.cn\n  # 名称需要和域名对应或包含域名\n  commonName: \"*.sreok.cn\"\n  # 最终TLS证书\n  secretName: selfsigned-cert-tls\n  # 100年有效期\n  duration: 876000h\n  # 有效期剩1年时更新\n  renewBefore: 8760h\n  # 证书信息\n  subject:\n    countries:\n    - China\n    localities:\n    - NanJing\n    organizations:\n    - sreok\n    organizationalUnits:\n    - harbor\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  # issuer绑定到CA证书\n  issuerRef:\n    name: devops-issuer\n    kind: ClusterIssuer\n    group: cert-manager.io\n```\n\n### Ingress配置\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: abc-ingress\nspec:\n  ingressClassName: nginx\n  tls:\n    - secretName: selfsigned-cert-tls # 证书名,与certificate一致\n      hosts:\n        - abc.sreok.cn # 证书域名\n  rules:\n    - host: abc.sreok.cn # 访问域名\n      http:\n        paths:\n          - path: /\n            #pathType: ImplementationSpecific\n            pathType: Prefix\n            backend:\n              service:\n                name: demo # 服务名\n                port:\n                  number: 80 # 服务的端口号 service port，非pod port\n```\n\n## **测试（Harbor平台开启TLS认证）**\n\n修改hosts文件，浏览器访问域名\n\n![](/images/image-tixg.png)\n\n![](/images/image-oqrj.png)\n\n### 系统颁发证书\n\n#### Windows\n\n![](/images/image-iydt.png)\n\n`复制ca.crt内容`\n\n```\necho \"connect in ca.crt\" | base64 -d\n```\n\n![](/images/image-fhla.png)\n\n保存到电脑\n\n![](/images/image-pblp.png)\n\n![](/images/image-faxz.png)\n\n![](/images/image-iyyw.png)\n\n#### MacOS\n\n双击导入`ca.crt`\\\n![](/images/image-mwml.png)\n\n![](/images/image-tcdd.png)\n\n![](/images/image-ybtx.png)\n\n#### Linux\n\n##### 方法一：只更新TLS证书（简单）\n\n`将ca证书追加到以下文件`\n\n```\n/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\n```\n\n![](/images/image-vtnm.png)\n\n##### 方法二：全部信任此证书\n\n安装ca-certificates，一般系统是自带的\n\n```\nyum install ca-certificates\n```\n\n将证书放置以下目录\n\n```\n/etc/pki/ca-trust/source/anchors/\n```\n\n执行信任此证书\n\n```\nupdate-ca-trust\n```\n\n> 此操作会更新以下文件，而不是只是tls-ca-bundle.pem\n>\n> ```\n> /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\n> /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\n> /etc/pki/ca-trust/extracted/pem/email-ca-bundle.pem\n> /etc/pki/ca-trust/extracted/pem/objsign-ca-bundle.pem  \n> ```\n>\n> 删除时在`/etc/pki/ca-trust/source/anchors/`文件中删掉对应的ca证书，重新执行`update-ca-trust`即可。\n\n参考链接：[*SelfSigned - cert-manager Documentation*](https://cert-manager.io/docs/configuration/selfsigned/)\n","tags":["cert-manager"],"categories":["云原生"]},{"title":"【云原生】阿里云域名使用cert-manager通过acme申请https证书(http01方式)","url":"/2024/05/03/【云原生】阿里云域名使用cert-manager通过acme申请https证书(http01方式)/","content":"\n\n## **前提条件**\n\n* k8s集群<1.19\n\n* cert-manager\n\n* ingress-nginx\n\n### **安装cert-manager**\n\n```\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml\n```\n\n> 官方安装文档\n>\n> [*https://cert-manager.io/docs/installation/*](https://cert-manager.io/docs/installation/)\n\n### **安装ingress-nginx**\n\n注意安装镜像地址，改为国内地址，~~建议使用lank8s.cn~~，使用文档在官网，修改一下地址即可。\n\n```\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.1/deploy/static/provider/cloud/deploy.yaml\nkubectl apply -f deploy.yaml\n```\n\n#### 查看ingressclass\n\n```\nkubectl get ingressclass \nNAME    CONTROLLER             PARAMETERS   AGE\nnginx   k8s.io/ingress-nginx   <none>       5d\n```\n\n\\##\n\n### **启动一个http服务**\n\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo\nspec:\n  selector:\n    app: demo\n  ports:\n    - name: http-port\n      port: 80\n      protocol: TCP\n      targetPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo\n  labels:\n    app: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.21.6\n          env:\n            - name: TZ\n              value: Asia/Shanghai\n          ports:\n            - containerPort: 80\n```\n\n![](/images/image-hren.png)\n\n![](/images/image.png)\n\n## **颁发机构（CA）**\n\n```\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory #生产证书地址\n    email: example@vsoul.cn # 接收证书申请信息邮箱\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx #填写安装的ingressclass名称\n```\n\nACME：自动证书管理环境 协议来确定您是否拥有某个域名，进而确定能否被颁发 Let’s Encrypt 证书。对于这个 challenge，需要传递以下参数：\n\n* [metadata.name](http://metadata.name)：ClusterIssuer 名称，在 Kubernetes 安装中需要是唯一的。这个名称将在后面的证书颁发示例中用到。\n\n* [spec.acme.email](http://spec.acme.email)：这是您为生成证书而使用 Let’s Encrypt 注册的电子邮件地址。这应该是您的电子邮件。\n\n* spec.acme.privateKeySecretRef：这是您将用来存储私钥的 Kubernetes secret 的名称。\n\n* spec.acme.solvers：这应保持不变——它指出了您所使用的 challenge 类型（或者 ACME 所说的解析程序）（HTTP-01 或 DNS-01），以及它应该应用于哪种 Ingress 类型（在这种情况下，将是 nginx）。\n\n![](/images/image-agwe.png)\n\n### **签发证书**\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod  # ClusterIssuer名称 \nspec:\n  ingressClassName: nginx\n  tls:\n    - secretName: test-tls # 证书名\n      hosts:\n        - vsoul.cn # 域名\n  rules:\n    - host: vsoul.cn # 域名\n      http:\n        paths:\n          - path: /\n            #pathType: ImplementationSpecific\n            pathType: Prefix\n            backend:\n              service:\n                name: demo # 服务名\n                port:\n                  number: 80 # 服务的端口号 service port，非pod port\n```\n\n![](/images/image-ewnv.png)\n\n## **验证HTTPS**\n\n![](/images/image-hfjr.png)\n\n## **故障排查**\n\nCertificate => CertificateRequest => Order => Challenge\n\n以此使用kubectl get、describe查看状态和日志\n\n参考链接：[*HTTP Validation - cert-manager Documentation*](https://cert-manager.io/docs/tutorials/acme/http-validation/)、[*验证方式 - Let's Encrypt - 免费的SSL/TLS证书 (*](https://letsencrypt.org/zh-cn/docs/challenge-types/)[*letsencrypt.org*](http://letsencrypt.org)[*)*](https://letsencrypt.org/zh-cn/docs/challenge-types/)\n","tags":["cert-manager"]},{"title":"【云原生】阿里云域名使用cert-manager通过acme申请https证书(dns01方式)","url":"/2024/05/02/【云原生】阿里云域名使用cert-manager通过acme申请https证书(dns01方式)/","content":"\n# 阿里云域名使用cert-manager通过acme申请https证书(dns01方式)\n\n## 前提条件\n\n- k8s集群<1.19\n- cert-manager\n- ingress-nginx\n- 阿里云域名\n\n## 申请AccessKey\n\n登录阿里云域名所在账号，开通AccessKey并记录下来\n\n![image-20240416171250857](/images/image-juep.png)\n\n\n\n### 将access-key和secret-key转换成base64\n\n![image-20240416171702060](/images/image-mmsf.png)\n\n### 将转换后的值保存为k8s secret\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: alidns-secret\n  namespace: cert-manager\ndata:\n  access-key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n  secret-key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n\n\n## 安装alidns-webhook\n\n```\nkubectl apply -f https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml\n```\n\n![image-20240416172108074](/images/image-etnp.png)\n\n![image-20240416172143884](/images/image-jmdd.png)\n\n## 启动一个http服务\n\n测试用，如果有服务可跳过\n\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo\nspec:\n  selector:\n    app: demo\n  ports:\n    - name: http-port\n      port: 80\n      protocol: TCP\n      targetPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo\n  labels:\n    app: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.21.6\n          env:\n            - name: TZ\n              value: Asia/Shanghai\n          ports:\n            - containerPort: 80\n```\n\n![image-20240415164809387](/images/image-ufah.png)\n\n![image-20240415165231793](/images/image-ljcx.png)\n\n## 颁发机构（CA）\n\n```yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: user@vsoul.cn\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    solvers:\n    # An empty 'selector' means that this solver matches all domains\n    - dns01:\n        webhook:\n          # 注意这里要改动，在https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml中也要改动对应的groupName\n          groupName: acme.yourcompany.com\n          solverName: alidns\n          config:\n            region: \"\"\n            accessKeySecretRef:\n              name: alidns-secret\n              key: access-key\n            secretKeySecretRef:\n              name: alidns-secret\n              key: secret-key\n```\n\n![image-20240416172852330](/images/image-tbjn.png)\n\n## 签发证书\n\n```yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\" # 强制转换https\n    cert-manager.io/cluster-issuer: letsencrypt-prod # ClusterIssuer名称\nspec:\n  ingressClassName: nginx\n  tls:\n    - secretName: test-tls # 证书名\n      hosts:\n        - vsoul.cn # 域名\n  rules:\n    - host: vsoul.cn # 域名\n      http:\n        paths:\n          - path: /\n            #pathType: ImplementationSpecific\n            pathType: Prefix\n            backend:\n              service:\n                name: demo # 服务名\n                port:\n                  number: 80 # 服务的端口号 service port，非pod port\n```\n\n![image-20240416173038711](/images/image-bylo.png)\n\n等待certificate状态为True\n\n![image-20240416173455625](/images/image-xjwk.png)\n\n## 测试\n\n浏览器通过https访问域名\n\n![image-20240416173343380](/images/image-styt.png)\n\n## 故障排查\n\nCertificate   =>  CertificateRequest  => Order => Challenge \n\n以此使用kubectl get、describe查看状态和日志\n","tags":["cert-manager"],"categories":["云原生"]},{"title":"【云原生】CNI插件完全卸载指南（Flannel、Calico、Cilium）","url":"/2024/03/18/【云原生】CNI插件完全卸载指南（Flannel、Calico、Cilium）/","content":"\n`注意：某些命令是按照预期编写进文档，仅个人参考，未实际尝试，不保证完全可用。`\n\n### **Flannel**\n\n```\n# master\nkubectl delete -f kube-flannel.yml\n# kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n \n# all nodes\nifconfig cni0 down\nip link delete cni0\nifconfig flannel.1 down\nip link delete flannel.1\nip link del kube-ipvs0\nrm -rf /var/lib/cni/\nrm -f /etc/cni/net.d/*\n​\nsystemctl restart kubelet\nsystemctl restart network\n# 如有必要，重启节点\n```\n\n### **Calico**\n\n```\n# master\nkubectl delete -f calico.yaml\n​\n# all nodes\nmodprobe -r ipip\nip link del kube-ipvs0\nip a | grep cali | awk -F ':' '{print $2}' | awk -F '@' '{print $1}' | xargs -i ip link del {}\nrm -f /etc/cni/net.d/*\n​\nsystemctl restart kubelet\nsystemctl restart network\n# 如有必要，重启节点\n```\n\n### **Cilium**\n\n```\n# master\ncilium uninstall\n​\n# all nodes\nip a | grep cili | awk -F '{print $2}' | awk -F '@' '{print $1}' | xargs -i ip link del {}\nip link del kube-ipvs0\nrm -f /etc/cni/net.d/*\n​\nsystemctl restart kubelet\nsystemctl restart network\n# 如有必要，重启节点\n```\n\n删除后查看node状态\n\n```\nkubectl get node\n```\n\n> 理想情况下是NotReady状态\n","tags":["CNI","Kubernetes"],"categories":["云原生"]},{"title":"【云原生】kubernetes集群升级ipv4ipv6双栈","url":"/2024/03/17/【云原生】kubernetes集群升级ipv4ipv6双栈/","content":"\n## kubernetes集群升级ipv4/ipv6双栈\n\n### 前提条件\n\n- 节点配置ipv4/ipv6地址\n- k8s集群<=1.23\n- 安装calico插件>=3.23\n\n#### 地址规划\n\n| 网络         | ipv4          | ipv6       |\n| ------------ | ------------- | ---------- |\n| pod-cidr     | 10.244.0.0/16 | 2000::/112 |\n| service-cidr | 10.96.0.0/12  | 3000::/112 |\n\n`注意：ipv6掩码位需要>=108，否则kube-apiserver会报错`\n\n### 网卡同时配置ipv4/ipv6\n\n```\nvim /etc/sysconfig/network-scripts/ifcfg-XXX\n```\n\n```\nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=none\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=yes\nIPV6_ADDR_GEN_MODE=stable-privacy\nIPV6ADDR=2031:0:130c::90         # 改为实际的ipv6地址\nIPV6_DEFAULTGW=2031:0:130c::1    # 改为实际的ipv6网关地址\nIPV6_DEFROUTE=yes\nDEVICE=ens192\nONBOOT=yes\nIPADDR=10.20.13.90\nPREFIX=24\nGATEWAY=10.20.13.1\nDNS1=218.2.2.2\nDNS2=2001:4860:4860::8888        # ipv6 dns 不用动\nIPV6_PRIVACY=no\n```\n\n```\nsystemctl restart network\n```\n\n#### 测试ipv4\n\n```\nping 10.20.13.90\n```\n\n#### 测试ipv6\n\n```\nping -6 2031:0:130c::90\n```\n\n\n\n### 节点开启ipv6（所有节点）\n\n```\nvim /etc/sysctl.d/k8s.conf\n```\n\n```\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv6.conf.all.disable_ipv6 = 0\nnet.ipv6.conf.default.disable_ipv6 = 0\nnet.ipv6.conf.lo.disable_ipv6 = 0\nnet.ipv6.conf.all.forwarding = 1\n```\n\n```\nsysctl --system\n```\n\n### k8s开启双栈（主节点）\n\n#### 修改kube-apiserver\n\n```\n vim /etc/kubernetes/manifests/kube-apiserver.yaml\n```\n\n```text\n--service-cluster-ip-range=10.96.0.0/12,3000::/112\n```\n\n![image-20240424154319890](/images/image-vbud.png)\n\n#### 修改kube-controller-manager\n\n```\nvim /etc/kubernetes/manifests/kube-controller-manager.yaml\n```\n\n![image-20240424154354671](/images/image-vthx.png)\n\n#### 修改kube-proxy\n\n```\nkubectl edit cm -n kube-system kube-proxy\n```\n\n![image-20240424155114938](/images/image-veqq.png)\n\n重启kube-proxy容器\n\n```text\nkubectl get pod -n kube-system | grep kube-proxy | awk '{print $1}' | xargs kubectl delete pod -n kube-system\n```\n\n#### calico 配置双栈\n\n```\nkubectl edit cm -n kube-system calico-config\n```\n\n```\n\"ipam\": {\n    \"type\": \"calico-ipam\",\n    \"assign_ipv4\": \"true\",\n    \"assign_ipv6\": \"true\"\n},\n```\n\n![image-20240424155614010](/images/image-zwor.png)\n\n```\nkubectl edit ds -n kube-system calico-node\n```\n\n```\n# 设置自动获取ipv6地址\n- name: IP6\n  value: autodetect\n# felix启用ipv6支持\n- name: FELIX_IPV6SUPPORT\n  value: \"true\"\n# 设置ipv6地址池，与pod-cidr一致，官方说使用kubeadm安装的集群不需要配置，但是没试过。\n- name: CALICO_IPV6POOL_CIDR\n  value: 2000::/112\n# pod通信使用vxlan封装，如果节点跨子网，使容器之间可以访问\n- name: CALICO_IPV6POOL_VXLAN\n  value: CrossSubnet\n# 开启ipv6 NAT功能，使容器可以访问外部的ipv6地址\n- name: CALICO_IPV6POOL_NAT_OUTGOING\n  value: \"true\"\n```\n\n![image-20240424160040807](/images/image-zktz.png)\n\n等待calico重启\n\n#### 修改kubeadm-config\n\n上方修改的apiserver、controller-manager是静态文件，如果集群升级kubeadm-config会重置静态文件，导致ipv6的配置消失，所以将这个配置文件同时修改\n\n```\nkubectl edit cm -n kube-system kubeadm-config\n```\n\n![image-20240425091454939](/images/image-bitu.png)\n\n### 验证\n\n#### 查看ipv6地址池\n\n```\nkubectl get ippool\n```\n\n![image-20240424160631893](/images/image-rkbg.png)\n\n#### pod获取ipv6地址\n\n重启pod，查看获取的ipv6地址\n\n```\nkubectl delete pod -n xxxx xxxxxx-xxx-xxx --force\n\nkubectl get pod - n xxxx xxxxxx-xxx-xxx -o jsonpath={.status.podIPs}\n```\n\n![image-20240424161315923](/images/image-tbnz.png)\n\n#### services配置双栈\n\n```\nkubectl edit svc -n xxxx xxxxx-svc\n```\n\n![image-20240424161359493](/images/image-yoqx.png)\n\n- `SingleStack`：单栈 Service。控制面使用第一个配置的服务集群 IP 范围为 Service 分配集群 IP\n\n- `PreferDualStack`：为 Service 分配 IPv4 和 IPv6 集群 IP 地址。\n\n- `RequireDualStack`：从 IPv4 和 IPv6 的地址范围分配 Service 的`ClusterIPs`。\n\n  从 `ipFamilies` 数组中第一个ip地址的 `ClusterIPs`作为`ClusterIP`\n\n`注意：ipFamilies 字段修改是有条件的：你可以添加或删除第二个 IP 地址族， 但你不能更改现有 Service 的主要 IP 地址。`\n\n```\nkubectl describe svc -n xxxx xxxxxx-svc\n```\n\n![image-20240424162335499](/images/image-unvc.png)\n\n### service示例\n\n#### 新增service单栈ipv6\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  ipFamilyPolicy: SingleStack\n  ports:\n  - nodePort: 30080\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n#### service双栈\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  - IPv4\n  ipFamilyPolicy: PreferDualStack\n  ports:\n  - nodePort: 30081\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n参考链接：[IPv4/IPv6 双协议栈 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/services-networking/dual-stack/)、[自定义 Calico 配置 |Calico 文档 (tigera.io)](https://docs.tigera.io/calico/latest/networking/ipam/ipv6#enable-dual-stack)","tags":["kubernetes"],"categories":["云原生"]},{"title":"【云原生】二进制安装Kubernetes v1.29.2","url":"/2024/03/16/【云原生】二进制安装Kubernetes v1.29.2/","content":"\n\n### **系统环境配置**\n\n原文地址：[k8s节点系统初始化+内核优化 - (](https://sreok.cn/archives/942c699f-2ad0-428e-809c-9957a2e331ce)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/942c699f-2ad0-428e-809c-9957a2e331ce)\n\n### k8s基本组件安装\n\n#### **可选一：使用Containerd作为Runtime （推荐）**\n\n原文地址：[使用Containerd作为Kubernetes Runtime - (](https://sreok.cn/archives/4896568f-2a8e-4a1d-bad6-de894920131d)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/4896568f-2a8e-4a1d-bad6-de894920131d)\n\n#### **可选二：使用docker作为Runtime**\n\n原文地址：[使用Docker作为Kubernetes Runtime - (](https://sreok.cn/archives/8da2f508-f580-4a8d-9b51-55856a2f7b16)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/8da2f508-f580-4a8d-9b51-55856a2f7b16)\n\n### 部署etcd集群\n\n原文地址：[etcd 入门 - 二进制部署etcd集群 - (](https://sreok.cn/archives/833fedb6-428a-4110-b3c9-56f8480b9de3)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/833fedb6-428a-4110-b3c9-56f8480b9de3)\n\n#### **软链接etcd证书目录**\n\n```\nmkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now etcd.service\n# 启用并立即启动etcd.service单元。etcd.service是etcd守护进程的systemd服务单元。\n\nsystemctl restart etcd.service\n# 重启etcd.service单元，即重新启动etcd守护进程。\n\nsystemctl status etcd.service\n# etcd.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### **解压k8s安装包**\n\n```\n# 下载安装包\n# wget https://dl.k8s.io/v1.29.2/kubernetes-server-linux-amd64.tar.gz\n​\n# 解压k8s安装文件\ntar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}\n​\n\n# 查看/usr/local/bin下内容\nls /usr/local/bin/\ncontainerd               crictl       etcdctl                  kube-proxy\ncontainerd-shim          critest      kube-apiserver           kube-scheduler\ncontainerd-shim-runc-v1  ctd-decoder  kube-controller-manager\ncontainerd-shim-runc-v2  ctr          kubectl\ncontainerd-stress        etcd         kubelet\n```\n\n### **2.3.2查看版本**\n\n```\n[root@k8s-master01 ~]#  kubelet --version\nKubernetes v1.29.2\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.12\nAPI version: 3.5\n[root@k8s-master01 ~]# \n```\n\n### **2.3.3将组件发送至其他k8s节点**\n\n```\nMaster='k8s-master02 k8s-master03'\nWork='k8s-node01 k8s-node02'\n​\n# 拷贝master组件\nfor NODE in $Master; do echo $NODE; scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\n​\n​\n# 拷贝work组件\nfor NODE in $Work; do echo $NODE; scp /usr/local/bin/kube{let,-proxy} $NODE:/usr/local/bin/ ; done\n\n​\n# 所有节点执行\nmkdir -p /opt/cni/bin\n```\n\n## **2.3创建证书相关文件**\n\n```\n# 请查看Github仓库 或者进行获取已经打好的包\nhttps://github.com/cby-chen/Kubernetes/\nhttps://github.com/cby-chen/Kubernetes/tags\nhttps://github.com/cby-chen/Kubernetes/releases/download/v1.29.2/kubernetes-v1.29.2.tar\n```\n\n# 3.相关证书生成\n\n## **3.2.生成k8s相关证书**\n\n特别说明除外，以下操作在所有master节点操作\n\n### **3.2.1 所有k8s节点创建证书存放目录**\n\n```\nmkdir -p /etc/kubernetes/pki\n```\n\n### **3.2.2 master01节点生成k8s证书**\n\n```\n# 写入生成证书所需的配置文件\ncat > ca-csr.json   << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n\n\n\ncat > apiserver-csr.json << EOF \n{\n  \"CN\": \"kube-apiserver\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n\n\n# 生成一个根证书 ，多写了一些IP作为预留IP，为将来添加node做准备\n# 10.96.0.1是service网段的第一个地址，需要计算，192.168.1.36为高可用vip地址\n# 若没有IPv6 可删除可保留 \n\ncfssl gencert   \\\n-ca=/etc/kubernetes/pki/ca.pem   \\\n-ca-key=/etc/kubernetes/pki/ca-key.pem   \\\n-config=ca-config.json   \\\n-hostname=10.96.0.1,192.168.1.36,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,x.oiox.cn,k.oiox.cn,l.oiox.cn,o.oiox.cn,192.168.1.31,192.168.1.32,192.168.1.33,192.168.1.34,192.168.1.35,192.168.1.36,192.168.0.37,192.168.0.38,192.168.0.39,192.168.1.70,fc00:43f4:1eea:1::10,fc00:43f4:1eea:1::20,fc00:43f4:1eea:1::30,fc00:43f4:1eea:1::40,fc00:43f4:1eea:1::50,fc00:43f4:1eea:1::60,fc00:43f4:1eea:1::70,fc00:43f4:1eea:1::80,fc00:43f4:1eea:1::90,fc00:43f4:1eea:1::100,::1   \\\n-profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n```\n\n### **3.2.3 生成apiserver聚合证书**\n\n```\ncat > front-proxy-ca-csr.json  << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  },\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n\n\n\ncfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n\n\ncat > front-proxy-client-csr.json  << EOF \n{\n  \"CN\": \"front-proxy-client\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  }\n}\nEOF\n\n\ncfssl gencert  \\\n-ca=/etc/kubernetes/pki/front-proxy-ca.pem   \\\n-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   \\\n-config=ca-config.json   \\\n-profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n\n# 这个命令使用cfssl工具生成一个用于Kubernetes的front-proxy-client证书。\n# \n# 主要参数解释如下：\n# - `-ca=/etc/kubernetes/pki/front-proxy-ca.pem`: 指定用于签署证书的根证书文件路径。\n# - `-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem`: 指定用于签署证书的根证书的私钥文件路径。\n# - `-config=ca-config.json`: 指定用于配置证书签署的配置文件路径。该配置文件描述了证书生成的一些规则，如加密算法和有效期等。\n# - `-profile=kubernetes`: 指定生成证书时使用的配置文件中定义的profile，其中包含了一些默认的参数。\n# - `front-proxy-client-csr.json`: 指定用于生成证书的CSR文件路径，该文件包含了证书请求的相关信息。\n# - `| cfssljson -bare /etc/kubernetes/pki/front-proxy-client`: 通过管道将生成的证书输出到cfssljson工具进行解析，并通过`-bare`参数将证书和私钥分别保存到指定路径。\n# \n# 这个命令的作用是根据提供的CSR文件和配置信息，使用指定的根证书和私钥生成一个前端代理客户端的证书，并将证书和私钥分别保存到`/etc/kubernetes/pki/front-proxy-client.pem`和`/etc/kubernetes/pki/front-proxy-client-key.pem`文件中。\n```\n\n### **3.2.4 生成controller-manage的证书**\n\n在《5.高可用配置》选择使用那种高可用方案 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443` 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```\ncat > manager-csr.json << EOF \n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n\n\n\n# 设置一个集群项\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n\n\n  # 设置一个用户项\n  kubectl config set-credentials system:kube-controller-manager \\\n       --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n       --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n       --embed-certs=true \\\n       --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n\n# 设置默认环境\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 这个命令是用来指定kubectl使用指定的上下文环境来执行操作。上下文环境是kubectl用来确定要连接到哪个Kubernetes集群以及使用哪个身份验证信息的配置。\n# \n# 在这个命令中，`kubectl config use-context`是用来设置当前上下文环境的命令。 `system:kube-controller-manager@kubernetes`是指定的上下文名称，它告诉kubectl要使用的Kubernetes集群和身份验证信息。 \n# `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`是用来指定使用的kubeconfig文件的路径。kubeconfig文件是存储集群连接和身份验证信息的配置文件。\n# 通过执行这个命令，kubectl将使用指定的上下文来执行后续的操作，包括部署和管理Kubernetes资源。\n```\n\n### **3.2.5 生成kube-scheduler的证书**\n\n```\ncat > scheduler-csr.json << EOF \n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n```\n\n### **3.2.6 生成admin的证书配置**\n\n```\ncat > admin-csr.json << EOF \n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\n\nkubectl config set-credentials kubernetes-admin  \\\n  --client-certificate=/etc/kubernetes/pki/admin.pem     \\\n  --client-key=/etc/kubernetes/pki/admin-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\n\n\nkubectl config set-context kubernetes-admin@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kubernetes-admin     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\n\n\nkubectl config use-context kubernetes-admin@kubernetes  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n```\n\n### **3.2.7 创建kube-proxy证书**\n\n在《5.高可用配置》选择使用那种高可用方案 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443` 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```\ncat > kube-proxy-csr.json  << EOF \n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-proxy\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n\n\n   \n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-credentials kube-proxy  \\\n  --client-certificate=/etc/kubernetes/pki/kube-proxy.pem     \\\n  --client-key=/etc/kubernetes/pki/kube-proxy-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-context kube-proxy@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kube-proxy     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config use-context kube-proxy@kubernetes  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n```\n\n### **3.2.8 创建ServiceAccount Key ——secret**\n\n```\nopenssl genrsa -out /etc/kubernetes/pki/sa.key 2048\nopenssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n\n# 这两个命令是使用OpenSSL工具生成RSA密钥对。\n# \n# 命令1：openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n# 该命令用于生成私钥文件。具体解释如下：\n# - openssl：openssl命令行工具。\n# - genrsa：生成RSA密钥对。\n# - -out /etc/kubernetes/pki/sa.key：指定输出私钥文件的路径和文件名。\n# - 2048：指定密钥长度为2048位。\n# \n# 命令2：openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n# 该命令用于从私钥中导出公钥。具体解释如下：\n# - openssl：openssl命令行工具。\n# - rsa：与私钥相关的RSA操作。\n# - -in /etc/kubernetes/pki/sa.key：指定输入私钥文件的路径和文件名。\n# - -pubout：指定输出公钥。\n# - -out /etc/kubernetes/pki/sa.pub：指定输出公钥文件的路径和文件名。\n# \n# 总结：通过以上两个命令，我们可以使用OpenSSL工具生成一个RSA密钥对，并将私钥保存在/etc/kubernetes/pki/sa.key文件中，将公钥保存在/etc/kubernetes/pki/sa.pub文件中。\n```\n\n### **3.2.9 将证书发送到其他master节点**\n\n```\n#其他节点创建目录\n# mkdir  /etc/kubernetes/pki/ -p\n\nfor NODE in k8s-master02 k8s-master03; do  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do  scp /etc/kubernetes/pki/${FILE} $NODE:/etc/kubernetes/pki/${FILE}; done;  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do  scp /etc/kubernetes/${FILE} $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n### **3.2.10 查看证书**\n\n```\nls /etc/kubernetes/pki/\nadmin.csr          controller-manager.csr      kube-proxy.csr\nadmin-key.pem      controller-manager-key.pem  kube-proxy-key.pem\nadmin.pem          controller-manager.pem      kube-proxy.pem\napiserver.csr      front-proxy-ca.csr          sa.key\napiserver-key.pem  front-proxy-ca-key.pem      sa.pub\napiserver.pem      front-proxy-ca.pem          scheduler.csr\nca.csr             front-proxy-client.csr      scheduler-key.pem\nca-key.pem         front-proxy-client-key.pem  scheduler.pem\nca.pem             front-proxy-client.pem\n\n# 一共26个就对了\nls /etc/kubernetes/pki/ |wc -l\n26\n```\n\n# 5.高可用配置（在Master服务器上操作）\n\n**注意\\* 5.1.1 和5.1.2 二选一即可**\n\n选择使用那种高可用方案，同时可以俩种都选用，实现内外兼顾的效果，比如： 5.1 的 NGINX方案实现集群内的高可用 5.2 的 haproxy、keepalived 方案实现集群外访问\n\n在《3.2.生成k8s相关证书》\n\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443` 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n\n### kube-vip高可用方案（推荐）\n\n原文地址：[k8s高可用方案-使用kube-vip作为控制平面负载入口 - (](https://sreok.cn/archives/9d06c7be-1885-4df7-bfbf-b80ed16e4d47)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/9d06c7be-1885-4df7-bfbf-b80ed16e4d47)\n\n### **NGINX高可用方案**\n\n#### **5.1.1 进行编译**\n\n```\n# 安装编译环境\nyum install gcc -y\n​\n# 下载解压nginx二进制文件\n# wget http://nginx.org/download/nginx-1.25.3.tar.gz\ntar xvf nginx-*.tar.gz\ncd nginx-*\n​\n# 进行编译\n./configure --with-stream --without-http --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module\nmake && make install \n​\n# 拷贝编译好的nginx\nnode='k8s-master02 k8s-master03 k8s-node01 k8s-node02'\nfor NODE in $node; do scp -r /usr/local/nginx/ $NODE:/usr/local/nginx/; done\n​\n# 这是一系列命令行指令，用于编译和安装软件。\n# \n# 1. `./configure` 是用于配置软件的命令。在这个例子中，配置的软件是一个Web服务器，指定了一些选项来启用流模块，并禁用了HTTP、uwsgi、scgi和fastcgi模块。\n# 2. `--with-stream` 指定启用流模块。流模块通常用于代理TCP和UDP流量。\n# 3. `--without-http` 指定禁用HTTP模块。这意味着编译的软件将没有HTTP服务器功能。\n# 4. `--without-http_uwsgi_module` 指定禁用uwsgi模块。uwsgi是一种Web服务器和应用服务器之间的通信协议。\n# 5. `--without-http_scgi_module` 指定禁用scgi模块。scgi是一种用于将Web服务器请求传递到应用服务器的协议。\n# 6. `--without-http_fastcgi_module` 指定禁用fastcgi模块。fastcgi是一种用于在Web服务器和应用服务器之间交换数据的协议。\n# 7. `make` 是用于编译软件的命令。该命令将根据之前的配置生成可执行文件。\n# 8. `make install` 用于安装软件。该命令将生成的可执行文件和其他必要文件复制到系统的适当位置，以便可以使用该软件。\n# \n# 总之，这个命令序列用于编译一个配置了特定选项的Web服务器，并将其安装到系统中。\n```\n\n### **5.1.2 写入启动配置**\n\n在所有主机上执行\n\n```\n# 写入nginx配置文件\ncat > /usr/local/nginx/conf/kube-nginx.conf <<EOF\nworker_processes 1;\nevents {\n    worker_connections  1024;\n}\nstream {\n    upstream backend {\n        least_conn;\n        hash $remote_addr consistent;\n        server 192.168.1.31:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.32:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.33:6443        max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 127.0.0.1:8443;\n        proxy_connect_timeout 1s;\n        proxy_pass backend;\n    }\n}\nEOF\n# 这段配置是一个nginx的stream模块的配置，用于代理TCP和UDP流量。\n# \n# 首先，`worker_processes 1;`表示启动一个worker进程用于处理流量。\n# 接下来，`events { worker_connections 1024; }`表示每个worker进程可以同时处理最多1024个连接。\n# 在stream块里面，定义了一个名为`backend`的upstream，用于负载均衡和故障转移。\n# `least_conn`表示使用最少连接算法进行负载均衡。\n# `hash $remote_addr consistent`表示用客户端的IP地址进行哈希分配请求，保持相同IP的请求始终访问同一台服务器。\n# `server`指令用于定义后端的服务器，每个服务器都有一个IP地址和端口号，以及一些可选的参数。\n# `max_fails=3`表示当一个服务器连续失败3次时将其标记为不可用。\n# `fail_timeout=30s`表示如果一个服务器被标记为不可用，nginx将在30秒后重新尝试。\n# 在server块内部，定义了一个监听地址为127.0.0.1:8443的服务器。\n# `proxy_connect_timeout 1s`表示与后端服务器建立连接的超时时间为1秒。\n# `proxy_pass backend`表示将流量代理到名为backend的上游服务器组。\n# \n# 总结起来，这段配置将流量代理到一个包含3个后端服务器的上游服务器组中，使用最少连接算法进行负载均衡，并根据客户端的IP地址进行哈希分配请求。如果一个服务器连续失败3次，则将其标记为不可用，并在30秒后重新尝试。\n​\n​\n# 写入启动配置文件\ncat > /etc/systemd/system/kube-nginx.service <<EOF\n[Unit]\nDescription=kube-apiserver nginx proxy\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n​\n[Service]\nType=forking\nExecStartPre=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -t\nExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx\nExecReload=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -s reload\nPrivateTmp=true\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nLimitNOFILE=65536\n \n[Install]\nWantedBy=multi-user.target\nEOF\n# 这是一个用于kube-apiserver的NGINX代理的systemd单位文件。\n# \n# [Unit]部分包含了单位的描述和依赖关系。它指定了在network.target和network-online.target之后启动，并且需要network-online.target。\n# \n# [Service]部分定义了如何运行该服务。Type指定了服务进程的类型（forking表示主进程会派生一个子进程）。ExecStartPre指定了在服务启动之前需要运行的命令，用于检查NGINX配置文件的语法是否正确。ExecStart指定了启动服务所需的命令。ExecReload指定了在重新加载配置文件时运行的命令。PrivateTmp设置为true表示将为服务创建一个私有的临时文件系统。Restart和RestartSec用于设置服务的自动重启机制。StartLimitInterval设置为0表示无需等待，可以立即重启服务。LimitNOFILE指定了服务的文件描述符的限制。\n# \n# [Install]部分指定了在哪些target下该单位应该被启用。\n# \n# 综上所述，此单位文件用于启动和管理kube-apiserver的NGINX代理服务。它通过NGINX来反向代理和负载均衡kube-apiserver的请求。该服务会在系统启动时自动启动，并具有自动重启的机制。\n​\n​\n# 设置开机自启\n​\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now kube-nginx.service\n# 启用并立即启动kube-nginx.service单元。kube-nginx.service是kube-nginx守护进程的systemd服务单元。\nsystemctl restart kube-nginx.service\n# 重启kube-nginx.service单元，即重新启动kube-nginx守护进程。\nsystemctl status kube-nginx.service\n# kube-nginx.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## **5.2 keepalived和haproxy 高可用方案**\n\n### **5.2.1安装keepalived和haproxy服务**\n\n```\nsystemctl disable --now firewalld\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nyum -y install keepalived haproxy\n```\n\n### **5.2.2修改haproxy配置文件（配置文件一样）**\n\n```\n# cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n​\ncat >/etc/haproxy/haproxy.cfg<<\"EOF\"\nglobal\n maxconn 2000\n ulimit-n 16384\n log 127.0.0.1 local0 err\n stats timeout 30s\n​\ndefaults\n log global\n mode http\n option httplog\n timeout connect 5000\n timeout client 50000\n timeout server 50000\n timeout http-request 15s\n timeout http-keep-alive 15s\n​\n​\nfrontend monitor-in\n bind *:33305\n mode http\n option httplog\n monitor-uri /monitor\n​\nfrontend k8s-master\n bind 0.0.0.0:9443\n bind 127.0.0.1:9443\n mode tcp\n option tcplog\n tcp-request inspect-delay 5s\n default_backend k8s-master\n​\n​\nbackend k8s-master\n mode tcp\n option tcplog\n option tcp-check\n balance roundrobin\n default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n server  k8s-master01  192.168.1.31:6443 check\n server  k8s-master02  192.168.1.32:6443 check\n server  k8s-master03  192.168.1.33:6443 check\nEOF\n```\n\n参数\n\n```\n这段配置代码是指定了一个HAProxy负载均衡器的配置。下面对各部分进行详细解释：\n1. global:\n   - maxconn 2000: 设置每个进程的最大连接数为2000。\n   - ulimit-n 16384: 设置每个进程的最大文件描述符数为16384。\n   - log 127.0.0.1 local0 err: 指定日志的输出地址为本地主机的127.0.0.1，并且只记录错误级别的日志。\n   - stats timeout 30s: 设置查看负载均衡器统计信息的超时时间为30秒。\n​\n2. defaults:\n   - log global: 使默认日志与global部分相同。\n   - mode http: 设定负载均衡器的工作模式为HTTP模式。\n   - option httplog: 使负载均衡器记录HTTP协议的日志。\n   - timeout connect 5000: 设置与后端服务器建立连接的超时时间为5秒。\n   - timeout client 50000: 设置与客户端的连接超时时间为50秒。\n   - timeout server 50000: 设置与后端服务器连接的超时时间为50秒。\n   - timeout http-request 15s: 设置处理HTTP请求的超时时间为15秒。\n   - timeout http-keep-alive 15s: 设置保持HTTP连接的超时时间为15秒。\n​\n3. frontend monitor-in:\n   - bind *:33305: 监听所有IP地址的33305端口。\n   - mode http: 设定frontend的工作模式为HTTP模式。\n   - option httplog: 记录HTTP协议的日志。\n   - monitor-uri /monitor: 设置监控URI为/monitor。\n​\n4. frontend k8s-master:\n   - bind 0.0.0.0:9443: 监听所有IP地址的9443端口。\n   - bind 127.0.0.1:9443: 监听本地主机的9443端口。\n   - mode tcp: 设定frontend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - tcp-request inspect-delay 5s: 设置在接收到请求后延迟5秒进行检查。\n   - default_backend k8s-master: 设置默认的后端服务器组为k8s-master。\n​\n5. backend k8s-master:\n   - mode tcp: 设定backend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - option tcp-check: 启用TCP检查功能。\n   - balance roundrobin: 使用轮询算法进行负载均衡。\n   - default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100: 设置默认的服务器参数。\n   - server k8s-master01 192.168.1.31:6443 check: 增加一个名为k8s-master01的服务器，IP地址为192.168.1.31，端口号为6443，并对其进行健康检查。\n   - server k8s-master02 192.168.1.32:6443 check: 增加一个名为k8s-master02的服务器，IP地址为192.168.1.32，端口号为6443，并对其进行健康检查。\n   - server k8s-master03 192.168.1.33:6443 check: 增加一个名为k8s-master03的服务器，IP地址为192.168.1.33，端口号为6443，并对其进行健康检查。\n​\n以上就是这段配置代码的详细解释。它主要定义了全局配置、默认配置、前端监听和后端服务器组的相关参数和设置。通过这些配置，可以实现负载均衡和监控功能。\n```\n\n### **5.2.3Master01配置keepalived master节点**\n\n```\n#cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n​\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n​\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n}\nvrrp_instance VI_1 {\n    state MASTER\n    # 注意网卡名\n    interface eth0 \n    mcast_src_ip 192.168.1.31\n    virtual_router_id 51\n    priority 100\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n​\nEOF\n```\n\n### **5.2.4Master02配置keepalived backup节点**\n\n```\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n​\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n​\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n​\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.32\n    virtual_router_id 51\n    priority 80\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n​\nEOF\n```\n\n### **5.2.5Master03配置keepalived backup节点**\n\n```\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.33\n    virtual_router_id 51\n    priority 50\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n\n参数\n\n```\n这是一个用于配置keepalived的配置文件。下面是对每个部分的详细解释：\n\n- `global_defs`部分定义了全局参数。\n- `router_id`参数指定了当前路由器的标识，这里设置为\"LVS_DEVEL\"。\n\n- `vrrp_script`部分定义了一个VRRP脚本。`chk_apiserver`是脚本的名称，\n    - `script`参数指定了脚本的路径。该脚本每5秒执行一次，返回值为0表示服务正常，返回值为1表示服务异常。\n    - `weight`参数指定了根据脚本返回的值来调整优先级，这里设置为-5。\n    - `fall`参数指定了失败阈值，当连续2次脚本返回值为1时认为服务异常。\n    - `rise`参数指定了恢复阈值，当连续1次脚本返回值为0时认为服务恢复正常。\n\n- `vrrp_instance`部分定义了一个VRRP实例。`VI_1`是实例的名称。\n    - `state`参数指定了当前实例的状态，这里设置为MASTER表示当前实例是主节点。\n    - `interface`参数指定了要监听的网卡，这里设置为eth0。\n    - `mcast_src_ip`参数指定了VRRP报文的源IP地址，这里设置为192.168.1.31。\n    - `virtual_router_id`参数指定了虚拟路由器的ID，这里设置为51。\n    - `priority`参数指定了实例的优先级，优先级越高（数值越大）越有可能被选为主节点。\n    - `nopreempt`参数指定了当主节点失效后不要抢占身份，即不要自动切换为主节点。\n    - `advert_int`参数指定了发送广播的间隔时间，这里设置为2秒。\n    - `authentication`部分指定了认证参数\n    \t- `auth_type`参数指定了认证类型，这里设置为PASS表示使用密码认证，\n    \t- `auth_pass`参数指定了认证密码，这里设置为K8SHA_KA_AUTH。\n    - `virtual_ipaddress`部分指定了虚拟IP地址，这里设置为192.168.1.36。\n    - `track_script`部分指定了要跟踪的脚本，这里跟踪了chk_apiserver脚本。\n```\n\n### **5.2.6健康检查脚本配置（lb主机）**\n\n```\ncat >  /etc/keepalived/check_apiserver.sh << EOF\n#!/bin/bash\n\nerr=0\nfor k in \\$(seq 1 3)\ndo\n    check_code=\\$(pgrep haproxy)\n    if [[ \\$check_code == \"\" ]]; then\n        err=\\$(expr \\$err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ \\$err != \"0\" ]]; then\n    echo \"systemctl stop keepalived\"\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\nEOF\n\n# 给脚本授权\n\nchmod +x /etc/keepalived/check_apiserver.sh\n\n# 这段脚本是一个简单的bash脚本，主要用来检查是否有名为haproxy的进程正在运行。\n# \n# 脚本的主要逻辑如下：\n# 1. 首先设置一个变量err为0，用来记录错误次数。\n# 2. 使用一个循环，在循环内部执行以下操作：\n#    a. 使用pgrep命令检查是否有名为haproxy的进程在运行。如果不存在该进程，将err加1，并暂停1秒钟，然后继续下一次循环。\n#    b. 如果存在haproxy进程，将err重置为0，并跳出循环。\n# 3. 检查err的值，如果不为0，表示检查失败，输出一条错误信息并执行“systemctl stop keepalived”命令停止keepalived进程，并退出脚本返回1。\n# 4. 如果err的值为0，表示检查成功，退出脚本返回0。\n# \n# 该脚本的主要作用是检查是否存在运行中的haproxy进程，如果无法检测到haproxy进程，将停止keepalived进程并返回错误状态。如果haproxy进程存在，则返回成功状态。这个脚本可能是作为一个健康检查脚本的一部分，在确保haproxy服务可用的情况下，才继续运行其他操作。\n```\n\n### **5.2.7启动服务**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now haproxy.service\n# 启用并立即启动haproxy.service单元。haproxy.service是haproxy守护进程的systemd服务单元。\nsystemctl enable --now keepalived.service\n# 启用并立即启动keepalived.service单元。keepalived.service是keepalived守护进程的systemd服务单元。\nsystemctl status haproxy.service\n# haproxy.service单元的当前状态，包括运行状态、是否启用等信息。\nsystemctl status keepalived.service\n# keepalived.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### **5.2.8测试高可用**\n\n```\n# 能ping同\n[root@k8s-node02 ~]# ping 192.168.1.36\n\n# 能telnet访问\n[root@k8s-node02 ~]# telnet 192.168.1.36 9443\n\n# 关闭主节点，看vip是否漂移到备节点\n```\n\n# 6.k8s组件配置\n\n所有k8s节点创建以下目录\n\n```\nmkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes\n```\n\n## **6.1.创建apiserver（所有master节点）**\n\n### **6.1.1master01节点配置**\n\n```\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n​\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n​\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.31 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n​\n[Install]\nWantedBy=multi-user.target\n​\nEOF\n```\n\n### **6.1.2master02节点配置**\n\n```\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n​\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.32 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n​\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n​\n[Install]\nWantedBy=multi-user.target\n​\nEOF\n```\n\n### **6.1.3master03节点配置**\n\n```\ncat > /usr/lib/systemd/system/kube-apiserver.service  << EOF\n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.33 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n参数\n\n```\n该配置文件是用于定义Kubernetes API Server的systemd服务的配置。systemd是一个用于启动和管理Linux系统服务的守护进程。\n\n[Unit]\n- Description: 服务的描述信息，用于显示在日志和系统管理工具中。\n- Documentation: 提供关于服务的文档链接。\n- After: 规定服务依赖于哪些其他服务或单元。在这个例子中，API Server服务在网络目标启动之后启动。\n\n[Service]\n- ExecStart: 定义服务的命令行参数和命令。这里指定了API Server的启动命令，包括各种参数选项。\n- Restart: 指定当服务退出时应该如何重新启动。在这个例子中，服务在失败时将被重新启动。\n- RestartSec: 指定两次重新启动之间的等待时间。\n- LimitNOFILE: 指定进程可以打开的文件描述符的最大数量。\n\n[Install]\n- WantedBy: 指定服务应该安装到哪个系统目标。在这个例子中，服务将被安装到multi-user.target目标，以便在多用户模式下启动。\n\n上述配置文件中定义的kube-apiserver服务将以指定的参数运行，这些参数包括：\n\n- `--v=2` 指定日志级别为2，打印详细的API Server日志。\n- `--allow-privileged=true` 允许特权容器运行。\n- `--bind-address=0.0.0.0` 绑定API Server监听的IP地址。\n- `--secure-port=6443` 指定API Server监听的安全端口。\n- `--advertise-address=192.168.1.31` 广告API Server的地址。\n- `--service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112` 指定服务CIDR范围。\n- `--service-node-port-range=30000-32767` 指定NodePort的范围。\n- `--etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379` 指定etcd服务器的地址。\n- `--etcd-cafile` 指定etcd服务器的CA证书。\n- `--etcd-certfile` 指定etcd服务器的证书。\n- `--etcd-keyfile` 指定etcd服务器的私钥。\n- `--client-ca-file` 指定客户端CA证书。\n- `--tls-cert-file` 指定服务的证书。\n- `--tls-private-key-file` 指定服务的私钥。\n- `--kubelet-client-certificate` 和 `--kubelet-client-key` 指定与kubelet通信的客户端证书和私钥。\n- `--service-account-key-file` 指定服务账户公钥文件。\n- `--service-account-signing-key-file` 指定服务账户签名密钥文件。\n- `--service-account-issuer` 指定服务账户的发布者。\n- `--kubelet-preferred-address-types` 指定kubelet通信时的首选地址类型。\n- `--enable-admission-plugins` 启用一系列准入插件。\n- `--authorization-mode` 指定授权模式。\n- `--enable-bootstrap-token-auth` 启用引导令牌认证。\n- `--requestheader-client-ca-file` 指定请求头中的客户端CA证书。\n- `--proxy-client-cert-file` 和 `--proxy-client-key-file` 指定代理客户端的证书和私钥。\n- `--requestheader-allowed-names` 指定请求头中允许的名字。\n- `--requestheader-group-headers` 指定请求头中的组头。\n- `--requestheader-extra-headers-prefix` 指定请求头中的额外头前缀。\n- `--requestheader-username-headers` 指定请求头中的用户名头。\n- `--enable-aggregator-routing` 启用聚合路由。\n\n整个配置文件为Kubernetes API Server提供了必要的参数，以便正确地启动和运行。\n```\n\n### **6.1.4启动apiserver（所有master节点）**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-apiserver.service\n# 启用并立即启动kube-apiserver.service单元。kube-apiserver.service是kube-apiserver守护进程的systemd服务单元。\n\nsystemctl restart kube-apiserver.service\n# 重启kube-apiserver.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-apiserver.service\n# kube-apiserver.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## **6.2.配置kube-controller-manager service**\n\n```\n# 所有master节点配置，且配置相同\n# 172.16.0.0/12为pod网段，按需求设置你自己的网段\n\ncat > /usr/lib/systemd/system/kube-controller-manager.service << EOF\n\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\\\n      --leader-elect=true \\\\\n      --use-service-account-credentials=true \\\\\n      --node-monitor-grace-period=40s \\\\\n      --node-monitor-period=5s \\\\\n      --controllers=*,bootstrapsigner,tokencleaner \\\\\n      --allocate-node-cidrs=true \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112 \\\\\n      --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n      --node-cidr-mask-size-ipv4=24 \\\\\n      --node-cidr-mask-size-ipv6=120 \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n参数\n\n```\n这是一个用于启动 Kubernetes 控制器管理器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 控制器管理器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-controller-manager，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--root-ca-file：根证书文件的路径，用于验证其他组件的证书。\n--cluster-signing-cert-file：用于签名集群证书的证书文件路径。\n--cluster-signing-key-file：用于签名集群证书的私钥文件路径。\n--service-account-private-key-file：用于签名服务账户令牌的私钥文件路径。\n--kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个控制器管理器作为 leader 在运行。\n--use-service-account-credentials=true：使用服务账户的凭据进行认证和授权。\n--node-monitor-grace-period=40s：节点监控的优雅退出时间，节点长时间不响应时会触发节点驱逐。\n--node-monitor-period=5s：节点监控的检测周期，用于检测节点是否正常运行。\n--controllers：指定要运行的控制器类型，在这里使用了通配符 *，表示运行所有的控制器，同时还包括了 bootstrapsigner 和 tokencleaner 控制器。\n--allocate-node-cidrs=true：为节点分配 CIDR 子网，用于分配 Pod 网络地址。\n--service-cluster-ip-range：定义 Service 的 IP 范围，这里设置为 10.96.0.0/12 和 fd00::/108。\n--cluster-cidr：定义集群的 CIDR 范围，这里设置为 172.16.0.0/12 和 fc00::/48。\n--node-cidr-mask-size-ipv4：分配给每个节点的 IPv4 子网掩码大小，默认是 24。\n--node-cidr-mask-size-ipv6：分配给每个节点的 IPv6 子网掩码大小，默认是 120。\n--requestheader-client-ca-file：设置请求头中客户端 CA 的证书文件路径，用于认证请求头中的 CA 证书。\n\n这个服务单元文件描述了 Kubernetes 控制器管理器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 控制器管理器组件。\n```\n\n### **6.2.1启动kube-controller-manager，并查看状态**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-controller-manager.service\n# 启用并立即启动kube-controller-manager.service单元。kube-controller-manager.service是kube-controller-manager守护进程的systemd服务单元。\n\nsystemctl restart kube-controller-manager.service\n# 重启kube-controller-manager.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-controller-manager.service\n# kube-controller-manager.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## **6.3.配置kube-scheduler service**\n\n### **6.3.1所有master节点配置，且配置相同**\n\n```\ncat > /usr/lib/systemd/system/kube-scheduler.service << EOF\n\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --leader-elect=true \\\\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n参数\n\n```\n这是一个用于启动 Kubernetes 调度器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 调度器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-scheduler，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个调度器作为 leader 在运行。\n--kubeconfig=/etc/kubernetes/scheduler.kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n\n这个服务单元文件描述了 Kubernetes 调度器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 调度器组件。\n```\n\n### **6.3.2启动并查看服务状态**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-scheduler.service\n# 启用并立即启动kube-scheduler.service单元。kube-scheduler.service是kube-scheduler守护进程的systemd服务单元。\n\nsystemctl restart kube-scheduler.service\n# 重启kube-scheduler.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-scheduler.service\n# kube-scheduler.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 7.TLS Bootstrapping配置\n\n## **7.1在master01上配置**\n\n```\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:8443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\ncd bootstrap\n\nkubectl config set-cluster kubernetes     \\\n--certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n--embed-certs=true     --server=https://127.0.0.1:8443     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置 Kubernetes 集群配置的命令示例。下面是对每个选项的详细解释：\n# \n# config set-cluster kubernetes：指定要设置的集群名称为 \"kubernetes\"，表示要修改名为 \"kubernetes\" 的集群配置。\n# --certificate-authority=/etc/kubernetes/pki/ca.pem：指定证书颁发机构（CA）的证书文件路径，用于验证服务器证书的有效性。\n# --embed-certs=true：将证书文件嵌入到生成的 kubeconfig 文件中。这样可以避免在 kubeconfig 文件中引用外部证书文件。\n# --server=https://127.0.0.1:8443：指定 Kubernetes API 服务器的地址和端口，这里使用的是 https 协议和本地地址（127.0.0.1），端口号为 8443。你可以根据实际环境修改该参数。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"kubernetes\" 的集群配置，并提供 CA 证书、API 服务器地址和端口，并将这些配置信息嵌入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-credentials tls-bootstrap-token-user     \\\n--token=c8ad9c.2e4d610cf3e7426e \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置凭证信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-credentials tls-bootstrap-token-user：指定要设置的凭证名称为 \"tls-bootstrap-token-user\"，表示要修改名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --token=c8ad9c.2e4d610cf3e7426e：指定用户的身份验证令牌（token）。在这个示例中，令牌是 c8ad9c.2e4d610cf3e7426e。你可以根据实际情况修改该令牌。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user\" 的用户凭证，并将令牌信息加入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-context tls-bootstrap-token-user@kubernetes     \\\n--cluster=kubernetes     \\\n--user=tls-bootstrap-token-user     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置上下文信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-context tls-bootstrap-token-user@kubernetes：指定要设置的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要修改名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文配置。\n# --cluster=kubernetes：指定上下文关联的集群名称为 \"kubernetes\"，表示使用名为 \"kubernetes\" 的集群配置。\n# --user=tls-bootstrap-token-user：指定上下文关联的用户凭证名称为 \"tls-bootstrap-token-user\"，表示使用名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文，并将其关联到名为 \"kubernetes\" 的集群配置和名为 \"tls-bootstrap-token-user\" 的用户凭证配置。这样，bootstrap-kubelet.kubeconfig 文件就包含了完整的上下文信息，可以用于指定与 Kubernetes 集群建立连接时要使用的集群和凭证。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config use-context tls-bootstrap-token-user@kubernetes     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置当前上下文的命令示例。下面是对每个选项的详细解释：\n# \n# config use-context tls-bootstrap-token-user@kubernetes：指定要使用的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要将当前上下文切换为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以将当前上下文设置为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。这样，当你执行其他 kubectl 命令时，它们将使用该上下文与 Kubernetes 集群进行交互。请确保路径和文件名与实际环境中的配置相匹配。\n\n\n# token的位置在bootstrap.secret.yaml，如果修改的话到这个文件修改\nmkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n```\n\n## **7.2查看集群状态，没问题的话继续后续操作**\n\n```\n# 1.28 版本只能查看到一个etcd 属于正常现象\n# export ETCDCTL_API=3\n# etcdctl --endpoints=\"192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n\nkubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\nscheduler            Healthy   ok        \ncontroller-manager   Healthy   ok        \netcd-0               Healthy   ok \n\n# 切记执行，别忘记！！！\nkubectl create -f bootstrap.secret.yaml\n```\n\n# 8.node节点配置\n\n## **8.1.在master01上将证书复制到node节点**\n\n```\ncd /etc/kubernetes/\n \nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $NODE mkdir -p /etc/kubernetes/pki; for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig kube-proxy.kubeconfig; do scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n## **8.2.kubelet配置**\n\n**注意 ： 8.2.1 和 8.2.2 需要和 上方 2.1 和 2.2 对应起来**\n\n### **8.2.1当使用docker作为Runtime**\n\n```\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node= \n\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。下面是对每个节（[Unit]、[Service]、[Install]）的详细解释：\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数。这里使用的是 /usr/local/bin/kubelet 命令，并传递了一系列参数来配置 Kubelet 的运行。这些参数包括：\n# --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定了用于引导 kubelet 的 kubeconfig 文件的路径和名称。\n# --kubeconfig=/etc/kubernetes/kubelet.kubeconfig：指定了 kubelet 的 kubeconfig 文件的路径和名称。\n# --config=/etc/kubernetes/kubelet-conf.yml：指定了 kubelet 的配置文件的路径和名称。\n# --container-runtime-endpoint=unix:///run/cri-dockerd.sock：指定了容器运行时接口的端点地址，这里使用的是 Docker 运行时（cri-dockerd）的 UNIX 套接字。\n# --node-labels=node.kubernetes.io/node=：指定了节点的标签。这里的示例只给节点添加了一个简单的标签 node.kubernetes.io/node=。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，指定相关的配置文件和凭证文件，以及定义节点的标签。请确认路径和文件名与你的实际环境中的配置相匹配。\n\n\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=   \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### **8.2.2当使用Containerd作为Runtime （推荐）**\n\n```\nmkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n​\n# 所有k8s节点配置kubelet service\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n​\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n​\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=\n​\n[Install]\nWantedBy=multi-user.target\nEOF\n​\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。与之前相比，添加了 After 和 Requires 字段来指定依赖关系。\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数，与之前的示例相同。\n# --container-runtime-endpoint=unix:///run/containerd/containerd.sock：修改了容器运行时接口的端点地址，将其更改为使用 containerd 运行时（通过 UNIX 套接字）。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，并指定了它依赖的 containerd 服务。确保路径和文件名与你实际环境中的配置相匹配。\n​\n​\n​\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n​\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n​\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=  \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### **8.2.3所有k8s节点创建kubelet的配置文件**\n\n```\ncat > /etc/kubernetes/kubelet-conf.yml <<EOF\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\nEOF\n\n# 这是一个Kubelet的配置文件，用于配置Kubelet的各项参数。\n# \n# - apiVersion: kubelet.config.k8s.io/v1beta1：指定了配置文件的API版本为kubelet.config.k8s.io/v1beta1。\n# - kind: KubeletConfiguration：指定了配置类别为KubeletConfiguration。\n# - address: 0.0.0.0：指定了Kubelet监听的地址为0.0.0.0。\n# - port: 10250：指定了Kubelet监听的端口为10250。\n# - readOnlyPort: 10255：指定了只读端口为10255，用于提供只读的状态信息。\n# - authentication：指定了认证相关的配置信息。\n#   - anonymous.enabled: false：禁用了匿名认证。\n#   - webhook.enabled: true：启用了Webhook认证。\n#   - x509.clientCAFile: /etc/kubernetes/pki/ca.pem：指定了X509证书的客户端CA文件路径。\n# - authorization：指定了授权相关的配置信息。\n#   - mode: Webhook：指定了授权模式为Webhook。\n#   - webhook.cacheAuthorizedTTL: 5m0s：指定了授权缓存时间段为5分钟。\n#   - webhook.cacheUnauthorizedTTL: 30s：指定了未授权缓存时间段为30秒。\n# - cgroupDriver: systemd：指定了Cgroup驱动为systemd。\n# - cgroupsPerQOS: true：启用了每个QoS类别一个Cgroup的设置。\n# - clusterDNS: 指定了集群的DNS服务器地址列表。\n#   - 10.96.0.10：指定了DNS服务器地址为10.96.0.10。\n# - clusterDomain: cluster.local：指定了集群的域名后缀为cluster.local。\n# - containerLogMaxFiles: 5：指定了容器日志文件保留的最大数量为5个。\n# - containerLogMaxSize: 10Mi：指定了容器日志文件的最大大小为10Mi。\n# - contentType: application/vnd.kubernetes.protobuf：指定了内容类型为protobuf。\n# - cpuCFSQuota: true：启用了CPU CFS Quota。\n# - cpuManagerPolicy: none：禁用了CPU Manager。\n# - cpuManagerReconcilePeriod: 10s：指定了CPU管理器的调整周期为10秒。\n# - enableControllerAttachDetach: true：启用了控制器的挂载和拆卸。\n# - enableDebuggingHandlers: true：启用了调试处理程序。\n# - enforceNodeAllocatable: 指定了强制节点可分配资源的列表。\n#   - pods：强制节点可分配pods资源。\n# - eventBurst: 10：指定了事件突发的最大数量为10。\n# - eventRecordQPS: 5：指定了事件记录的最大请求量为5。\n# - evictionHard: 指定了驱逐硬性限制参数的配置信息。\n#   - imagefs.available: 15%：指定了镜像文件系统可用空间的限制为15%。\n#   - memory.available: 100Mi：指定了可用内存的限制为100Mi。\n#   - nodefs.available: 10%：指定了节点文件系统可用空间的限制为10%。\n#   - nodefs.inodesFree: 5%：指定了节点文件系统可用inode的限制为5%。\n# - evictionPressureTransitionPeriod: 5m0s：指定了驱逐压力转换的时间段为5分钟。\n# - failSwapOn: true：指定了在发生OOM时禁用交换分区。\n# - fileCheckFrequency: 20s：指定了文件检查频率为20秒。\n# - hairpinMode: promiscuous-bridge：设置了Hairpin Mode为\"promiscuous-bridge\"。\n# - healthzBindAddress: 127.0.0.1：指定了健康检查的绑定地址为127.0.0.1。\n# - healthzPort: 10248：指定了健康检查的端口为10248。\n# - httpCheckFrequency: 20s：指定了HTTP检查的频率为20秒。\n# - imageGCHighThresholdPercent: 85：指定了镜像垃圾回收的上阈值为85%。\n# - imageGCLowThresholdPercent: 80：指定了镜像垃圾回收的下阈值为80%。\n# - imageMinimumGCAge: 2m0s：指定了镜像垃圾回收的最小时间为2分钟。\n# - iptablesDropBit: 15：指定了iptables的Drop Bit为15。\n# - iptablesMasqueradeBit: 14：指定了iptables的Masquerade Bit为14。\n# - kubeAPIBurst: 10：指定了KubeAPI的突发请求数量为10个。\n# - kubeAPIQPS: 5：指定了KubeAPI的每秒请求频率为5个。\n# - makeIPTablesUtilChains: true：指定了是否使用iptables工具链。\n# - maxOpenFiles: 1000000：指定了最大打开文件数为1000000。\n# - maxPods: 110：指定了最大的Pod数量为110。\n# - nodeStatusUpdateFrequency: 10s：指定了节点状态更新的频率为10秒。\n# - oomScoreAdj: -999：指定了OOM Score Adjustment为-999。\n# - podPidsLimit: -1：指定了Pod的PID限制为-1，表示无限制。\n# - registryBurst: 10：指定了Registry的突发请求数量为10个。\n# - registryPullQPS: 5：指定了Registry的每秒拉取请求数量为5个。\n# - resolvConf: /etc/resolv.conf：指定了resolv.conf的文件路径。\n# - rotateCertificates: true：指定了是否轮转证书。\n# - runtimeRequestTimeout: 2m0s：指定了运行时请求的超时时间为2分钟。\n# - serializeImagePulls: true：指定了是否序列化镜像拉取。\n# - staticPodPath: /etc/kubernetes/manifests：指定了静态Pod的路径。\n# - streamingConnectionIdleTimeout: 4h0m0s：指定了流式连接的空闲超时时间为4小时。\n# - syncFrequency: 1m0s：指定了同步频率为1分钟。\n# - volumeStatsAggPeriod: 1m0s：指定了卷统计聚合周期为1分钟。\n```\n\n### **8.2.4启动kubelet**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kubelet.service\n# 启用并立即启动kubelet.service单元。kubelet.service是kubelet守护进程的systemd服务单元。\n\nsystemctl restart kubelet.service\n# 重启kubelet.service单元，即重新启动kubelet守护进程。\n\nsystemctl status kubelet.service\n# kubelet.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### **8.2.5查看集群**\n\n```\n[root@k8s-master01 ~]# kubectl  get node\nNAME           STATUS   ROLES    AGE   VERSION\nk8s-master01   Ready    <none>   16s   v1.29.2\nk8s-master02   Ready    <none>   13s   v1.29.2\nk8s-master03   Ready    <none>   12s   v1.29.2\nk8s-node01     Ready    <none>   10s   v1.29.2\nk8s-node02     Ready    <none>   9s    v1.29.2\n[root@k8s-master01 ~]#\n```\n\n### **8.2.6查看容器运行时**\n\n```\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n```\n\n## **8.3.kube-proxy配置**\n\n### **8.3.1将kubeconfig发送至其他节点**\n\n```\n# master-1执行\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig; done\n```\n\n### **8.3.2所有k8s节点添加kube-proxy的service文件**\n\n```\ncat >  /usr/lib/systemd/system/kube-proxy.service << EOF\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\\\n  --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n  --v=2\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n\n# 这是一个 systemd 服务单元文件的示例，用于配置 Kubernetes Kube Proxy 服务。下面是对其中一些字段的详细解释：\n# \n# [Unit]\n# \n# Description: 描述了该服务单元的用途，这里是 Kubernetes Kube Proxy。\n# Documentation: 指定了该服务单元的文档地址，即 https://github.com/kubernetes/kubernetes。\n# After: 指定该服务单元应在 network.target（网络目标）之后启动。\n# [Service]\n# \n# ExecStart: 指定了启动 Kube Proxy 服务的命令。通过 /usr/local/bin/kube-proxy 命令启动，并指定了配置文件的路径为 /etc/kubernetes/kube-proxy.yaml，同时指定了日志级别为 2。\n# Restart: 配置了服务在失败或退出后自动重启。\n# RestartSec: 配置了重启间隔，这里是每次重启之间的等待时间为 10 秒。\n# [Install]\n# \n# WantedBy: 指定了该服务单元的安装目标为 multi-user.target（多用户目标），表示该服务将在多用户模式下启动。\n# 通过配置这些字段，你可以启动和管理 Kubernetes Kube Proxy 服务。请注意，你需要根据实际情况修改 ExecStart 中的路径和文件名，确保与你的环境一致。另外，可以根据需求修改其他字段的值，以满足你的特定要求。\n```\n\n### **8.3.3所有k8s节点添加kube-proxy的配置**\n\n```\ncat > /etc/kubernetes/kube-proxy.yaml << EOF\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: \"\"\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/12,fc00:2222::/112\nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: \"\"\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: \"rr\"\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: \"ipvs\"\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: \"\"\nudpIdleTimeout: 250ms\nEOF\n\n# 这是一个Kubernetes的kube-proxy组件配置文件示例。以下是每个配置项的详细解释：\n# \n# 1. apiVersion: kubeproxy.config.k8s.io/v1alpha1\n#    - 指定该配置文件的API版本。\n# \n# 2. bindAddress: 0.0.0.0\n#    - 指定kube-proxy使用的监听地址。0.0.0.0表示监听所有网络接口。\n# \n# 3. clientConnection:\n#    - 客户端连接配置项。\n# \n#    a. acceptContentTypes: \"\"\n#       - 指定接受的内容类型。\n# \n#    b. burst: 10\n#       - 客户端请求超出qps设置时的最大突发请求数。\n# \n#    c. contentType: application/vnd.kubernetes.protobuf\n#       - 指定客户端请求的内容类型。\n# \n#    d. kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n#       - kube-proxy使用的kubeconfig文件路径。\n# \n#    e. qps: 5\n#       - 每秒向API服务器发送的请求数量。\n# \n# 4. clusterCIDR: 172.16.0.0/12,fc00:2222::/112\n#    - 指定集群使用的CIDR范围，用于自动分配Pod IP。\n# \n# 5. configSyncPeriod: 15m0s\n#    - 指定kube-proxy配置同步到节点的频率。\n# \n# 6. conntrack:\n#    - 连接跟踪设置。\n# \n#    a. max: null\n#       - 指定连接跟踪的最大值。\n# \n#    b. maxPerCore: 32768\n#       - 指定每个核心的最大连接跟踪数。\n# \n#    c. min: 131072\n#       - 指定最小的连接跟踪数。\n# \n#    d. tcpCloseWaitTimeout: 1h0m0s\n#       - 指定处于CLOSE_WAIT状态的TCP连接的超时时间。\n# \n#    e. tcpEstablishedTimeout: 24h0m0s\n#       - 指定已建立的TCP连接的超时时间。\n# \n# 7. enableProfiling: false\n#    - 是否启用性能分析。\n# \n# 8. healthzBindAddress: 0.0.0.0:10256\n#    - 指定健康检查监听地址和端口。\n# \n# 9. hostnameOverride: \"\"\n#    - 指定覆盖默认主机名的值。\n# \n# 10. iptables:\n#     - iptables设置。\n# \n#     a. masqueradeAll: false\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. masqueradeBit: 14\n#        - 指定伪装的Bit标记。\n# \n#     c. minSyncPeriod: 0s\n#        - 指定同步iptables规则的最小间隔。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步iptables规则的时间间隔。\n# \n# 11. ipvs:\n#     - ipvs设置。\n# \n#     a. masqueradeAll: true\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. minSyncPeriod: 5s\n#        - 指定同步ipvs规则的最小间隔。\n# \n#     c. scheduler: \"rr\"\n#        - 指定ipvs默认使用的调度算法。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步ipvs规则的时间间隔。\n# \n# 12. kind: KubeProxyConfiguration\n#     - 指定该配置文件的类型。\n# \n# 13. metricsBindAddress: 127.0.0.1:10249\n#     - 指定指标绑定的地址和端口。\n# \n# 14. mode: \"ipvs\"\n#     - 指定kube-proxy的模式。这里指定为ipvs，使用IPVS代理模式。\n# \n# 15. nodePortAddresses: null\n#     - 指定可用于NodePort的网络地址。\n# \n# 16. oomScoreAdj: -999\n#     - 指定kube-proxy的OOM优先级。\n# \n# 17. portRange: \"\"\n#     - 指定可用于服务端口范围。\n# \n# 18. udpIdleTimeout: 250ms\n#     - 指定UDP连接的空闲超时时间。\n```\n\n### **8.3.4启动kube-proxy**\n\n```\n systemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-proxy.service\n# 启用并立即启动kube-proxy.service单元。kube-proxy.service是kube-proxy守护进程的systemd服务单元。\n\nsystemctl restart kube-proxy.service\n# 重启kube-proxy.service单元，即重新启动kube-proxy守护进程。\n\nsystemctl status kube-proxy.service\n# kube-proxy.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 9.安装网络插件\n\n**注意 9.1 和 9.2 二选其一即可，建议在此处创建好快照后在进行操作，后续出问题可以回滚**\n\n**centos7 要升级libseccomp 不然 无法安装网络插件**\n\n```\n# https://github.com/opencontainers/runc/releases\n# 升级runc\n# wget https://mirrors.chenby.cn/https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\n\ninstall -m 755 runc.amd64 /usr/local/sbin/runc\ncp -p /usr/local/sbin/runc  /usr/local/bin/runc\ncp -p /usr/local/sbin/runc  /usr/bin/runc\n\n#下载高于2.4以上的包\nyum -y install http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n# 清华源\nyum -y install https://mirrors.tuna.tsinghua.edu.cn/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n\n#查看当前版本\n[root@k8s-master-1 ~]# rpm -qa | grep libseccomp\nlibseccomp-2.5.1-1.el8.x86_64\n```\n\n## **9.1安装Calico**\n\n### **9.1.1更改calico网段**\n\n```\nwget https://mirrors.chenby.cn/https://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yaml\n\ncp calico-typha.yaml calico.yaml\ncp calico-typha.yaml calico-ipv6.yaml\n\nvim calico.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n# vim calico-ipv6.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n        \"assign_ipv4\": \"true\",\n        \"assign_ipv6\": \"true\"\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: IP6\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n    - name: CALICO_IPV6POOL_CIDR\n      value: \"fc00:2222::/112\"\n\n    - name: FELIX_IPV6SUPPORT\n      value: \"true\"\n\n\n# 若docker镜像拉不下来，可以使用国内的仓库\nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico.yaml \nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico-ipv6.yaml\n\nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico.yaml \nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico-ipv6.yaml\n\n# 本地没有公网 IPv6 使用 calico.yaml\nkubectl apply -f calico.yaml\n\n# 本地有公网 IPv6 使用 calico-ipv6.yaml \n# kubectl apply -f calico-ipv6.yaml \n```\n\n### **9.1.2查看容器状态**\n\n```\n# calico 初始化会很慢 需要耐心等待一下，大约十分钟左右\n[root@k8s-master01 ~]# kubectl  get pod -A\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-6747f75cdc-fbvvc   1/1     Running   0          61s\nkube-system   calico-node-fs7hl                          1/1     Running   0          61s\nkube-system   calico-node-jqz58                          1/1     Running   0          61s\nkube-system   calico-node-khjlg                          1/1     Running   0          61s\nkube-system   calico-node-wmf8q                          1/1     Running   0          61s\nkube-system   calico-node-xc6gn                          1/1     Running   0          61s\nkube-system   calico-typha-6cdc4b4fbc-57snb              1/1     Running   0          61s\n```\n\n## **9.2 安装cilium**\n\n### **9.2.1 安装helm**\n\n```\n# [root@k8s-master01 ~]# curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n# [root@k8s-master01 ~]# chmod 700 get_helm.sh\n# [root@k8s-master01 ~]# ./get_helm.sh\n\n# wget https://mirrors.huaweicloud.com/helm/v3.13.2/helm-v3.13.2-linux-amd64.tar.gz\ntar xvf helm-*-linux-amd64.tar.gz\ncp linux-amd64/helm /usr/local/bin/\n```\n\n### **9.2.2 安装cilium**\n\n```\n# 添加源\nhelm repo add cilium https://helm.cilium.io\n\n# 修改为国内源\nhelm pull cilium/cilium\ntar xvf cilium-*.tgz\ncd cilium/\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  cilium ./cilium/ -n kube-system\n\n# 启用ipv6\n# helm install cilium cilium/cilium --namespace kube-system --set ipv6.enabled=true\n\n# 启用路由信息和监控插件\n# helm install cilium cilium/cilium --namespace kube-system --set hubble.relay.enabled=true --set hubble.ui.enabled=true --set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.enabled=true --set hubble.metrics.enabled=\"{dns,drop,tcp,flow,port-distribution,icmp,http}\" \n```\n\n### **9.2.3 查看**\n\n```\n[root@k8s-master01 ~]# kubectl  get pod -A | grep cil\nkube-system   cilium-gmr6c                       1/1     Running       0             5m3s\nkube-system   cilium-kzgdj                       1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-6pw4k   1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-xzzdk   1/1     Running       0             5m3s\nkube-system   cilium-q2rnr                       1/1     Running       0             5m3s\nkube-system   cilium-smx5v                       1/1     Running       0             5m3s\nkube-system   cilium-tdjq4                       1/1     Running       0             5m3s\n[root@k8s-master01 ~]#\n```\n\n### **9.2.4 下载专属监控面板**\n\n安装时候没有创建 监控可以忽略\n\n```\n[root@k8s-master01 yaml]# wget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/1.12.1/examples/kubernetes/addons/prometheus/monitoring-example.yaml\n\n[root@k8s-master01 yaml]# sed -i \"s#docker.io/#m.daocloud.io/docker.io/#g\" monitoring-example.yaml\n\n[root@k8s-master01 yaml]# kubectl  apply -f monitoring-example.yaml\nnamespace/cilium-monitoring created\nserviceaccount/prometheus-k8s created\nconfigmap/grafana-config created\nconfigmap/grafana-cilium-dashboard created\nconfigmap/grafana-cilium-operator-dashboard created\nconfigmap/grafana-hubble-dashboard created\nconfigmap/prometheus created\nclusterrole.rbac.authorization.k8s.io/prometheus created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus created\nservice/grafana created\nservice/prometheus created\ndeployment.apps/grafana created\ndeployment.apps/prometheus created\n[root@k8s-master01 yaml]#\n```\n\n### **9.2.5 下载部署测试用例**\n\n说明 测试用例 需要在 安装CoreDNS 之后即可完成\n\n```\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/connectivity-check/connectivity-check.yaml\n\nsed -i \"s#google.com#baidu.cn#g\" connectivity-check.yaml\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" connectivity-check.yaml\n\nkubectl  apply -f connectivity-check.yaml\n```\n\n### **9.2.6 查看pod**\n\n```\n[root@k8s-master01 yaml]# kubectl  get pod -A\nNAMESPACE           NAME                                                     READY   STATUS    RESTARTS      AGE\ncilium-monitoring   grafana-59957b9549-6zzqh                                 1/1     Running   0             10m\ncilium-monitoring   prometheus-7c8c9684bb-4v9cl                              1/1     Running   0             10m\ndefault             chenby-75b5d7fbfb-7zjsr                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-hbvr8                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-ppbzg                                  1/1     Running   0             27h\ndefault             echo-a-6799dff547-pnx6w                                  1/1     Running   0             10m\ndefault             echo-b-fc47b659c-4bdg9                                   1/1     Running   0             10m\ndefault             echo-b-host-67fcfd59b7-28r9s                             1/1     Running   0             10m\ndefault             host-to-b-multi-node-clusterip-69c57975d6-z4j2z          1/1     Running   0             10m\ndefault             host-to-b-multi-node-headless-865899f7bb-frrmc           1/1     Running   0             10m\ndefault             pod-to-a-allowed-cnp-5f9d7d4b9d-hcd8x                    1/1     Running   0             10m\ndefault             pod-to-a-denied-cnp-65cc5ff97b-2rzb8                     1/1     Running   0             10m\ndefault             pod-to-a-dfc64f564-p7xcn                                 1/1     Running   0             10m\ndefault             pod-to-b-intra-node-nodeport-677868746b-trk2l            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-clusterip-76bbbc677b-knfq2           1/1     Running   0             10m\ndefault             pod-to-b-multi-node-headless-698c6579fd-mmvd7            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-nodeport-5dc4b8cfd6-8dxmz            1/1     Running   0             10m\ndefault             pod-to-external-1111-8459965778-pjt9b                    1/1     Running   0             10m\ndefault             pod-to-external-fqdn-allow-google-cnp-64df9fb89b-l9l4q   1/1     Running   0             10m\nkube-system         cilium-7rfj6                                             1/1     Running   0             56s\nkube-system         cilium-d4cch                                             1/1     Running   0             56s\nkube-system         cilium-h5x8r                                             1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-flpl5                         1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-gcznc                         1/1     Running   0             56s\nkube-system         cilium-t2xlz                                             1/1     Running   0             56s\nkube-system         cilium-z65z7                                             1/1     Running   0             56s\nkube-system         coredns-665475b9f8-jkqn8                                 1/1     Running   1 (36h ago)   36h\nkube-system         hubble-relay-59d8575-9pl9z                               1/1     Running   0             56s\nkube-system         hubble-ui-64d4995d57-nsv9j                               2/2     Running   0             56s\nkube-system         metrics-server-776f58c94b-c6zgs                          1/1     Running   1 (36h ago)   37h\n[root@k8s-master01 yaml]#\n```\n\n### **9.2.7 修改为NodePort**\n\n安装时候没有创建 监控可以忽略\n\n```\n[root@k8s-master01 yaml]# kubectl  edit svc  -n kube-system hubble-ui\nservice/hubble-ui edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring grafana\nservice/grafana edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring prometheus\nservice/prometheus edited\n[root@k8s-master01 yaml]#\n\ntype: NodePort\n```\n\n### **9.2.8 查看端口**\n\n安装时候没有创建 监控可以忽略\n\n```\n[root@k8s-master01 yaml]# kubectl get svc -A | grep monit\ncilium-monitoring   grafana                NodePort    10.100.250.17    <none>        3000:30707/TCP           15m\ncilium-monitoring   prometheus             NodePort    10.100.131.243   <none>        9090:31155/TCP           15m\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl get svc -A | grep hubble\nkube-system         hubble-metrics         ClusterIP   None             <none>        9965/TCP                 5m12s\nkube-system         hubble-peer            ClusterIP   10.100.150.29    <none>        443/TCP                  5m12s\nkube-system         hubble-relay           ClusterIP   10.109.251.34    <none>        80/TCP                   5m12s\nkube-system         hubble-ui              NodePort    10.102.253.59    <none>        80:31219/TCP             5m12s\n[root@k8s-master01 yaml]#\n```\n\n### **9.2.9 访问**\n\n安装时候没有创建 监控可以忽略\n\n```\nhttp://192.168.1.31:30707\nhttp://192.168.1.31:31155\nhttp://192.168.1.31:31219\n```\n\n# 10.安装CoreDNS\n\n## **10.1以下步骤只在master01操作**\n\n### **10.1.1修改文件**\n\n```\n# 下载tgz包\nhelm repo add coredns https://coredns.github.io/helm\nhelm pull coredns/coredns\ntar xvf coredns-*.tgz\ncd coredns/\n\n# 修改IP地址\nvim values.yaml\ncat values.yaml | grep clusterIP:\nclusterIP: \"10.96.0.10\"\n\n# 示例\n---\nservice:\n# clusterIP: \"\"\n# clusterIPs: []\n# loadBalancerIP: \"\"\n# externalIPs: []\n# externalTrafficPolicy: \"\"\n# ipFamilyPolicy: \"\"\n  # The name of the Service\n  # If not set, a name is generated using the fullname template\n  clusterIP: \"10.96.0.10\"\n  name: \"\"\n  annotations: {}\n---\n\n# 修改为国内源 docker源可选\nsed -i \"s#coredns/#m.daocloud.io/docker.io/coredns/#g\" values.yaml\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  coredns ./coredns/ -n kube-system\n```\n\n# 11.安装Metrics Server\n\n## **11.1以下步骤只在master01操作**\n\n### **11.1.1安装Metrics-server**\n\n在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率\n\n```\n# 下载 \nwget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# 修改配置\nvim components.yaml\n\n---\n# 1\n\t\t\t- args:\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\n        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n        - --requestheader-username-headers=X-Remote-User\n        - --requestheader-group-headers=X-Remote-Group\n        - --requestheader-extra-headers-prefix=X-Remote-Extra-\n\n# 2\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-dir\n        - name: ca-ssl\n          mountPath: /etc/kubernetes/pki\n\n# 3\n      volumes:\n      - emptyDir: {}\n        name: tmp-dir\n      - name: ca-ssl\n        hostPath:\n          path: /etc/kubernetes/pki\n---\n\n\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n\n# 执行部署\nkubectl apply -f components.yaml\n```\n\n### **11.1.2稍等片刻查看状态**\n\n```\nkubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   268m         6%     2318Mi          60%       \nk8s-master02   147m         3%     1802Mi          47%       \nk8s-master03   147m         3%     1820Mi          47%       \nk8s-node01     62m          1%     1152Mi          30%       \nk8s-node02     63m          1%     1114Mi          29%  \n```\n\n# 12.集群验证\n\n## **12.1部署pod资源**\n\n```\ncat<<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: docker.io/library/busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\nEOF\n\n# 查看\nkubectl  get pod\nNAME      READY   STATUS    RESTARTS   AGE\nbusybox   1/1     Running   0          17s\n```\n\n## **12.2用pod解析默认命名空间中的kubernetes**\n\n```\n# 查看name\nkubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   17h\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup kubernetes\n3Server:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes\nAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n```\n\n## **12.3测试跨命名空间是否可以解析**\n\n```\n# 查看有那些name\nkubectl  get svc -A\nNAMESPACE     NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndefault       kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP         76m\nkube-system   calico-typha      ClusterIP   10.105.100.82   <none>        5473/TCP        35m\nkube-system   coredns-coredns   ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP   8m14s\nkube-system   metrics-server    ClusterIP   10.105.60.31    <none>        443/TCP         109s\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup coredns-coredns.kube-system\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n\nName:      coredns-coredns.kube-system\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n[root@k8s-master01 metrics-server]# \n```\n\n## **12.4每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53**\n\n```\ntelnet 10.96.0.1 443\nTrying 10.96.0.1...\nConnected to 10.96.0.1.\nEscape character is '^]'.\n\n telnet 10.96.0.10 53\nTrying 10.96.0.10...\nConnected to 10.96.0.10.\nEscape character is '^]'.\n\ncurl 10.96.0.10:53\ncurl: (52) Empty reply from server\n```\n\n## **12.5Pod和Pod之前要能通**\n\n```\nkubectl get po -owide\nNAME      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nbusybox   1/1     Running   0          17m   172.27.14.193   k8s-node02   <none>           <none>\n\nkubectl get po -n kube-system -owide\nNAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\ncalico-kube-controllers-76754ff848-pw4xg   1/1     Running   0          38m     172.25.244.193   k8s-master01   <none>           <none>\ncalico-node-97m55                          1/1     Running   0          38m     192.168.1.34     k8s-node01     <none>           <none>\ncalico-node-hlz7j                          1/1     Running   0          38m     192.168.1.32     k8s-master02   <none>           <none>\ncalico-node-jtlck                          1/1     Running   0          38m     192.168.1.33     k8s-master03   <none>           <none>\ncalico-node-lxfkf                          1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncalico-node-t667x                          1/1     Running   0          38m     192.168.1.31     k8s-master01   <none>           <none>\ncalico-typha-59d75c5dd4-gbhfp              1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncoredns-coredns-c5c6d4d9b-bd829            1/1     Running   0          10m     172.25.92.65     k8s-master02   <none>           <none>\nmetrics-server-7c8b55c754-w7q8v            1/1     Running   0          3m56s   172.17.125.3     k8s-node01     <none>           <none>\n\n# 进入busybox ping其他节点上的pod\n\nkubectl exec -ti busybox -- sh\n/ # ping 192.168.1.34\nPING 192.168.1.34 (192.168.1.34): 56 data bytes\n64 bytes from 192.168.1.34: seq=0 ttl=63 time=0.358 ms\n64 bytes from 192.168.1.34: seq=1 ttl=63 time=0.668 ms\n64 bytes from 192.168.1.34: seq=2 ttl=63 time=0.637 ms\n64 bytes from 192.168.1.34: seq=3 ttl=63 time=0.624 ms\n64 bytes from 192.168.1.34: seq=4 ttl=63 time=0.907 ms\n\n# 可以连通证明这个pod是可以跨命名空间和跨主机通信的\n```\n\n## **12.6创建三个副本，可以看到3个副本分布在不同的节点上（用完可以删了）**\n\n```\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl  get pod \nNAME                               READY   STATUS    RESTARTS   AGE\nbusybox                            1/1     Running   0          6m25s\nnginx-deployment-9456bbbf9-4bmvk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-9rcdk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-dqv8s   1/1     Running   0          8s\n\n# 删除nginx\n[root@k8s-master01 ~]# kubectl delete deployments nginx-deployment \n```\n\n# 13.安装dashboard\n\n```\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\nhelm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kube-system\n```\n\n## **13.1更改dashboard的svc为NodePort，如果已是请忽略**\n\n```\nkubectl edit svc kubernetes-dashboard -n kube-system\n  type: NodePort\n```\n\n## **13.2查看端口号**\n\n```\nkubectl get svc kubernetes-dashboard -n kube-system\nNAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.108.120.110   <none>        443:30034/TCP   34s\n```\n\n## **13.3创建token**\n\n```\ncat > dashboard-user.yaml << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\nEOF\n\nkubectl  apply -f dashboard-user.yaml\n\n# 创建token\nkubectl -n kube-system create token admin-user\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Im5vZExpNi1tTERLb09ONVM2cEE0SWNCUnA4eTZieE81RnVGb1IwSk5QVFEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzA4MjQ4NjM4LCJpYXQiOjE3MDgyNDUwMzgsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiMTQ1YTdmZTktMTQ0YS00NDZmLWI1M2QtNDk4OGM3YjIyZjgyIn19LCJuYmYiOjE3MDgyNDUwMzgsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.H2Oxxrb5BVLH1iDOA-Uo1I7aiAUZX1wK-xBiV9NJXQ32EDyQvss95yQbCNHtPMhQZ8jFE3NRhyjkgZMZmX7kR9J-89QXLqKhE8Qnihd1mq5HOEVQ8tjZ6ix8ymxs5QkfSvd_OUzILKBtfYAMb4Fer67Dyf14oBHWVKU9LQkCdtFaLxerK--N7gLWeGXzavqzOlEPZR5UZWUPwP5dJmAQtvSToPVMaKiA49LjaGJid0F5Pxnutr80oZRsLfKr0MpoEG6jrow1QeJ2PgVksDTcqMTpye-M6jmIbuxabsRSskTT_zEDT0J86BiLYIHnh79D-P7IUUq6GOp8DgG-wXhICQ\n```\n\n## **13.3登录dashboard**\n\n[*https://192.168.1.31:30034/*](https://192.168.1.31:30034/)\n\n# 14.ingress安装\n\n## **14.1执行部署**\n\n```\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml\n​\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n​\ncat > backend.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: registry.cn-hangzhou.aliyuncs.com/chenby/defaultbackend-amd64:1.5 \n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\nEOF\n​\nkubectl  apply -f deploy.yaml \nkubectl  apply -f backend.yaml \n​\n​\ncat > ingress-demo-app.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello-server\n  template:\n    metadata:\n      labels:\n        app: hello-server\n    spec:\n      containers:\n      - name: hello-server\n        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server\n        ports:\n        - containerPort: 9000\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-demo\n  template:\n    metadata:\n      labels:\n        app: nginx-demo\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  selector:\n    app: nginx-demo\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello-server\n  name: hello-server\nspec:\n  selector:\n    app: hello-server\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 9000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress  \nmetadata:\n  name: ingress-host-bar\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: \"hello.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: hello-server\n            port:\n              number: 8000\n  - host: \"demo.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/nginx\"  \n        backend:\n          service:\n            name: nginx-demo\n            port:\n              number: 8000\nEOF\n​\n# 等创建完成后在执行：\nkubectl  apply -f ingress-demo-app.yaml \n​\nkubectl  get ingress\nNAME               CLASS   HOSTS                            ADDRESS     PORTS   AGE\ningress-host-bar   nginx   hello.chenby.cn,demo.chenby.cn   192.168.1.32   80      7s\n​\n```\n\n## **14.2过滤查看ingress端口**\n\n```\n# 修改为nodeport\nkubectl edit svc -n ingress-nginx   ingress-nginx-controller\ntype: NodePort\n\n[root@hello ~/yaml]# kubectl  get svc -A | grep ingress\ningress-nginx          ingress-nginx-controller             NodePort    10.104.231.36    <none>        80:32636/TCP,443:30579/TCP   104s\ningress-nginx          ingress-nginx-controller-admission   ClusterIP   10.101.85.88     <none>        443/TCP                      105s\n[root@hello ~/yaml]#\n```\n\n# 15.IPv6测试\n\n```\n#部署应用\n​\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chenby\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: chenby\n  template:\n    metadata:\n      labels:\n        app: chenby\n    spec:\n      hostNetwork: true\n      containers:\n      - name: chenby\n        image: docker.io/library/nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: chenby\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n  - IPv6\n  - IPv4\n  type: NodePort\n  selector:\n    app: chenby\n  ports:\n  - port: 80\n    targetPort: 80\nEOF\n​\n​\n#查看端口\n[root@k8s-master01 ~]# kubectl  get svc\nNAME           TYPE        CLUSTER-IP            EXTERNAL-IP   PORT(S)        AGE\nchenby         NodePort    fd00:1111::bc86       <none>        80:31540/TCP   5s\n[root@k8s-master01 ~]# \n​\n[root@localhost yaml]# curl -I http://192.168.1.31:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:59 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n​\n[root@localhost yaml]# \n​\n[root@localhost yaml]# curl -I http://[2409:8a10:9e18:9020::10]:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:54 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n```\n\n# 16.安装命令行自动补全功能\n\n```\nyum install bash-completion -y\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\n```\n\n# 附录\n\n```\n# 镜像加速器可以使用DaoCloud仓库，替换规则如下\ncr.l5d.io/  ===> m.daocloud.io/cr.l5d.io/\ndocker.elastic.co/  ===> m.daocloud.io/docker.elastic.co/\ndocker.io/  ===> m.daocloud.io/docker.io/\ngcr.io/  ===> m.daocloud.io/gcr.io/\nghcr.io/  ===> m.daocloud.io/ghcr.io/\nk8s.gcr.io/  ===> m.daocloud.io/k8s.gcr.io/\nmcr.microsoft.com/  ===> m.daocloud.io/mcr.microsoft.com/\nnvcr.io/  ===> m.daocloud.io/nvcr.io/\nquay.io/  ===> m.daocloud.io/quay.io/\nregistry.jujucharms.com/  ===> m.daocloud.io/registry.jujucharms.com/\nregistry.k8s.io/  ===> m.daocloud.io/registry.k8s.io/\nregistry.opensource.zalan.do/  ===> m.daocloud.io/registry.opensource.zalan.do/\nrocks.canonical.com/  ===> m.daocloud.io/rocks.canonical.com/\n​\n​\n​\n​\n# 镜像版本要自行查看，因为镜像版本是随时更新的，文档无法做到实时更新\n​\n# docker pull 镜像\n​\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/cni:master \ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/node:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/typha:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2\ndocker pull kubernetesui/dashboard:v2.7.0\ndocker pull kubernetesui/metrics-scraper:v1.0.8\ndocker pull quay.io/cilium/cilium:v1.12.6\ndocker pull quay.io/cilium/certgen:v0.1.8\ndocker pull quay.io/cilium/hubble-relay:v1.12.6\ndocker pull quay.io/cilium/hubble-ui-backend:v0.9.2\ndocker pull quay.io/cilium/hubble-ui:v0.9.2\ndocker pull quay.io/cilium/cilium-etcd-operator:v2.0.7\ndocker pull quay.io/cilium/operator:v1.12.6\ndocker pull quay.io/cilium/clustermesh-apiserver:v1.12.6\ndocker pull quay.io/coreos/etcd:v3.5.4\ndocker pull quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26\n​\n# docker 保存镜像\ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/cni:master -o cni.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/node:master -o node.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/typha:master -o typha.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master -o kube-controllers.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0 -o coredns.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6 -o pause.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2 -o metrics-server.tar \ndocker save kubernetesui/dashboard:v2.7.0 -o dashboard.tar \ndocker save kubernetesui/metrics-scraper:v1.0.8 -o metrics-scraper.tar \ndocker save quay.io/cilium/cilium:v1.12.6 -o cilium.tar \ndocker save quay.io/cilium/certgen:v0.1.8 -o certgen.tar \ndocker save quay.io/cilium/hubble-relay:v1.12.6 -o hubble-relay.tar \ndocker save quay.io/cilium/hubble-ui-backend:v0.9.2 -o hubble-ui-backend.tar \ndocker save quay.io/cilium/hubble-ui:v0.9.2 -o hubble-ui.tar \ndocker save quay.io/cilium/cilium-etcd-operator:v2.0.7 -o cilium-etcd-operator.tar \ndocker save quay.io/cilium/operator:v1.12.6 -o operator.tar \ndocker save quay.io/cilium/clustermesh-apiserver:v1.12.6 -o clustermesh-apiserver.tar \ndocker save quay.io/coreos/etcd:v3.5.4 -o etcd.tar \ndocker save quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26 -o startup-script.tar \n​\n# 传输到各个节点\nfor NODE in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp -r images/  $NODE:/root/ ; done\n​\n# 创建命名空间\nctr ns create k8s.io\n​\n# 导入镜像\nctr --namespace k8s.io image import images/cni.tar\nctr --namespace k8s.io image import images/node.tar\nctr --namespace k8s.io image import images/typha.tar\nctr --namespace k8s.io image import images/kube-controllers.tar \nctr --namespace k8s.io image import images/coredns.tar \nctr --namespace k8s.io image import images/pause.tar \nctr --namespace k8s.io image import images/metrics-server.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/cilium.tar \nctr --namespace k8s.io image import images/certgen.tar \nctr --namespace k8s.io image import images/hubble-relay.tar \nctr --namespace k8s.io image import images/hubble-ui-backend.tar \nctr --namespace k8s.io image import images/hubble-ui.tar \nctr --namespace k8s.io image import images/cilium-etcd-operator.tar \nctr --namespace k8s.io image import images/operator.tar \nctr --namespace k8s.io image import images/clustermesh-apiserver.tar \nctr --namespace k8s.io image import images/etcd.tar \nctr --namespace k8s.io image import images/startup-script.tar \n​\n# pull tar包 解压后\nhelm pull cilium/cilium\n​\n# 查看镜像版本\nroot@hello:~/cilium# cat values.yaml| grep tag: -C1\n  repository: \"quay.io/cilium/cilium\"\n  tag: \"v1.12.6\"\n  pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/certgen\"\n    tag: \"v0.1.8@sha256:4a456552a5f192992a6edcec2febb1c54870d665173a33dc7d876129b199ddbd\"\n    pullPolicy: \"IfNotPresent\"\n--\n      repository: \"quay.io/cilium/hubble-relay\"\n      tag: \"v1.12.6\"\n       # hubble-relay-digest\n--\n        repository: \"quay.io/cilium/hubble-ui-backend\"\n        tag: \"v0.9.2@sha256:a3ac4d5b87889c9f7cc6323e86d3126b0d382933bd64f44382a92778b0cde5d7\"\n        pullPolicy: \"IfNotPresent\"\n--\n        repository: \"quay.io/cilium/hubble-ui\"\n        tag: \"v0.9.2@sha256:d3596efc94a41c6b772b9afe6fe47c17417658956e04c3e2a28d293f2670663e\"\n        pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium-etcd-operator\"\n    tag: \"v2.0.7@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/operator\"\n    tag: \"v1.12.6\"\n    # operator-generic-digest\n--\n    repository: \"quay.io/cilium/startup-script\"\n    tag: \"d69851597ea019af980891a4628fb36b7880ec26\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium\"\n    tag: \"v1.12.6\"\n    # cilium-digest\n--\n      repository: \"quay.io/cilium/clustermesh-apiserver\"\n      tag: \"v1.12.6\"\n      # clustermesh-apiserver-digest\n--\n        repository: \"quay.io/coreos/etcd\"\n        tag: \"v3.5.4@sha256:795d8660c48c439a7c3764c2330ed9222ab5db5bb524d8d0607cac76f7ba82a3\"\n        pullPolicy: \"IfNotPresent\"\n​\n```\n","tags":["CNI","Kubernetes","calico"],"categories":["云原生"]},{"title":"【转载】二进制部署 Kubernetes v1.29.2 ipv4/ipv6双栈","url":"/2024/03/16/【转载】二进制部署 Kubernetes v1.29.2 ipv4ipv6双栈/","content":"\n# 二进制部署 Kubernetes v1.29.2 ipv4/ipv6双栈\n\n# 介绍\n\nkubernetes（k8s）二进制高可用安装部署，支持IPv4+IPv6双栈。\n\n我使用IPV6的目的是在公网进行访问，所以我配置了IPV6静态地址。\n\n若您没有IPV6环境，或者不想使用IPv6，不对主机进行配置IPv6地址即可。\n\n不配置IPV6，不影响后续，不过集群依旧是支持IPv6的。为后期留有扩展可能性。\n\n若不要IPv6 ，不给网卡配置IPv6即可，不要对IPv6相关配置删除或操作，否则会出问题。\n\n\n# 1.环境\n\n| 主机名称 | IP地址       | 说明       | 软件                                                         |\n| -------- | ------------ | ---------- | ------------------------------------------------------------ |\n|          | 192.168.1.60 | 外网节点   | 下载各种所需安装包                                           |\n| Master01 | 192.168.1.31 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Master02 | 192.168.1.32 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Master03 | 192.168.1.33 | master节点 | kube-apiserver、kube-controller-manager、kube-scheduler、etcd、<br />kubelet、kube-proxy、nfs-client、haproxy、keepalived、nginx |\n| Node01   | 192.168.1.34 | node节点   | kubelet、kube-proxy、nfs-client、nginx                       |\n| Node02   | 192.168.1.35 | node节点   | kubelet、kube-proxy、nfs-client、nginx                       |\n|          | 192.168.1.36 | VIP        |                                                              |\n\n\n网段\n物理主机：192.168.1.0/24\nservice：10.96.0.0/12\npod：172.16.0.0/12\n\n安装包已经整理好：https://mirrors.chenby.cn/https://github.com/cby-chen/Kubernetes/releases/download/v1.29.2/kubernetes-v1.29.2.tar\n\n\n\n## 1.1.k8s基础系统环境配置\n\n### 1.2.配置IP\n\n```shell\n# 注意！\n# 若虚拟机是进行克隆的那么网卡的UUID会重复\n# 若UUID重复需要重新生成新的UUID\n# UUID重复无法获取到IPV6地址\n# \n# 查看当前的网卡列表和 UUID：\n# nmcli con show\n# 删除要更改 UUID 的网络连接：\n# nmcli con delete uuid <原 UUID>\n# 重新生成 UUID：\n# nmcli con add type ethernet ifname <接口名称> con-name <新名称>\n# 重新启用网络连接：\n# nmcli con up <新名称>\n\n# 更改网卡的UUID\nssh root@192.168.1.31 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.32 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.33 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.34 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\nssh root@192.168.1.35 \"nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44;nmcli con add type ethernet ifname eth0 con-name eth0;nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh ssh root@192.168.1.31\n# 使用SSH登录到IP为192.168.1.31的主机，使用root用户身份。\n# \n# nmcli con delete uuid 708a1497-2192-43a5-9f03-2ab936fb3c44\n# 删除 UUID 为 708a1497-2192-43a5-9f03-2ab936fb3c44 的网络连接，这是 NetworkManager 中一种特定网络配置的唯一标识符。\n# \n# nmcli con add type ethernet ifname eth0 con-name eth0\n# 添加一种以太网连接类型，并指定接口名为 eth0，连接名称也为 eth0。\n# \n# nmcli con up eth0\n# 开启 eth0 这个网络连接。\n# \n# 简单来说，这个命令的作用是删除一个特定的网络连接配置，并添加一个名为 eth0 的以太网连接，然后启用这个新的连接。\n\n# 修改静态的IPv4地址\nssh root@192.168.1.104 \"nmcli con mod eth0 ipv4.addresses 192.168.1.31/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.106 \"nmcli con mod eth0 ipv4.addresses 192.168.1.32/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.107 \"nmcli con mod eth0 ipv4.addresses 192.168.1.33/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.109 \"nmcli con mod eth0 ipv4.addresses 192.168.1.34/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\nssh root@192.168.1.110 \"nmcli con mod eth0 ipv4.addresses 192.168.1.35/24; nmcli con mod eth0 ipv4.gateway  192.168.1.1; nmcli con mod eth0 ipv4.method manual; nmcli con mod eth0 ipv4.dns \"8.8.8.8\"; nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh root@192.168.1.154\n# 使用SSH登录到IP为192.168.1.154的主机，使用root用户身份。\n# \n# \"nmcli con mod eth0 ipv4.addresses 192.168.1.31/24\"\n# 修改eth0网络连接的IPv4地址为192.168.1.31，子网掩码为 24。\n# \n# \"nmcli con mod eth0 ipv4.gateway 192.168.1.1\"\n# 修改eth0网络连接的IPv4网关为192.168.1.1。\n# \n# \"nmcli con mod eth0 ipv4.method manual\"\n# 将eth0网络连接的IPv4配置方法设置为手动。\n# \n# \"nmcli con mod eth0 ipv4.dns \"8.8.8.8\"\n# 将eth0网络连接的IPv4 DNS服务器设置为 8.8.8.8。\n# \n# \"nmcli con up eth0\"\n# 启动eth0网络连接。\n# \n# 总体来说，这条命令是通过SSH远程登录到指定的主机，并使用网络管理命令 (nmcli) 修改eth0网络连接的配置，包括IP地址、网关、配置方法和DNS服务器，并启动该网络连接。\n\n# 没有IPv6选择不配置即可\nssh root@192.168.1.31 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::10; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.32 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::20; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.33 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::30; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.34 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::40; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\nssh root@192.168.1.35 \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::50; nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1; nmcli con mod eth0 ipv6.method manual; nmcli con mod eth0 ipv6.dns \"2400:3200::1\"; nmcli con up eth0\"\n\n# 参数解释\n# \n# ssh root@192.168.1.31\n# 通过SSH连接到IP地址为192.168.1.31的远程主机，使用root用户进行登录。\n# \n# \"nmcli con mod eth0 ipv6.addresses fc00:43f4:1eea:1::10\"\n# 使用nmcli命令修改eth0接口的IPv6地址为fc00:43f4:1eea:1::10。\n# \n# \"nmcli con mod eth0 ipv6.gateway fc00:43f4:1eea:1::1\"\n# 使用nmcli命令修改eth0接口的IPv6网关为fc00:43f4:1eea:1::1。\n# \n# \"nmcli con mod eth0 ipv6.method manual\"\n# 使用nmcli命令将eth0接口的IPv6配置方法修改为手动配置。\n# \n# \"nmcli con mod eth0 ipv6.dns \"2400:3200::1\"\n# 使用nmcli命令设置eth0接口的IPv6 DNS服务器为2400:3200::1。\n# \n# \"nmcli con up eth0\"\n# 使用nmcli命令启动eth0接口。\n# \n# 这个命令的目的是在远程主机上配置eth0接口的IPv6地址、网关、配置方法和DNS服务器，并启动eth0接口。\n\n# 查看网卡配置\n# nmcli device show eth0\n# nmcli con show eth0\n[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 \nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=none\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=no\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=no\nIPV6_ADDR_GEN_MODE=stable-privacy\nNAME=eth0\nUUID=2aaddf95-3f36-4a48-8626-b55ebf7f53e7\nDEVICE=eth0\nONBOOT=yes\nIPADDR=192.168.1.31\nPREFIX=24\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\n[root@localhost ~]# \n\n# 参数解释\n# \n# TYPE=Ethernet\n# 指定连接类型为以太网。\n# \n# PROXY_METHOD=none\n# 指定不使用代理方法。\n# \n# BROWSER_ONLY=no\n# 指定不仅仅在浏览器中使用代理。\n# \n# BOOTPROTO=none\n# 指定自动分配地址的方式为无（即手动配置IP地址）。\n# \n# DEFROUTE=yes\n# 指定默认路由开启。\n# \n# IPV4_FAILURE_FATAL=no\n# 指定IPv4连接失败时不宣告严重错误。\n# \n# IPV6INIT=yes\n# 指定启用IPv6。\n# \n# IPV6_AUTOCONF=no\n# 指定不自动配置IPv6地址。\n# \n# IPV6_DEFROUTE=yes\n# 指定默认IPv6路由开启。\n# \n# IPV6_FAILURE_FATAL=no\n# 指定IPv6连接失败时不宣告严重错误。\n# \n# IPV6_ADDR_GEN_MODE=stable-privacy\n# 指定IPv6地址生成模式为稳定隐私模式。\n# \n# NAME=eth0\n# 指定设备名称为eth0。\n# \n# UUID=424fd260-c480-4899-97e6-6fc9722031e8\n# 指定设备的唯一标识符。\n# \n# DEVICE=eth0\n# 指定设备名称为eth0。\n# \n# ONBOOT=yes\n# 指定开机自动启用这个连接。\n# \n# IPADDR=192.168.1.31\n# 指定IPv4地址为192.168.1.31。\n# \n# PREFIX=24\n# 指定IPv4地址的子网掩码为24。\n# \n# GATEWAY=192.168.8.1\n# 指定IPv4的网关地址为192.168.8.1。\n# \n# DNS1=8.8.8.8\n# 指定首选DNS服务器为8.8.8.8。\n# \n# IPV6ADDR=fc00:43f4:1eea:1::10/128\n# 指定IPv6地址为fc00:43f4:1eea:1::10，子网掩码为128。\n# \n# IPV6_DEFAULTGW=fc00:43f4:1eea:1::1\n# 指定IPv6的默认网关地址为fc00:43f4:1eea:1::1。\n# \n# DNS2=2400:3200::1\n# 指定备用DNS服务器为2400:3200::1。\n```\n\n### 1.3.设置主机名\n\n```shell\nhostnamectl set-hostname k8s-master01\nhostnamectl set-hostname k8s-master02\nhostnamectl set-hostname k8s-master03\nhostnamectl set-hostname k8s-node01\nhostnamectl set-hostname k8s-node02\n\n# 参数解释\n# \n# 参数: set-hostname\n# 解释: 这是hostnamectl命令的一个参数，用于设置系统的主机名。\n# \n# 参数: k8s-master01\n# 解释: 这是要设置的主机名，将系统的主机名设置为\"k8s-master01\"。\n```\n\n\n### 1.4.配置yum源\n\n```shell\n# 其他系统的源地址\n# https://mirrors.tuna.tsinghua.edu.cn/help/\n\n# 对于 Ubuntu\nsed -i 's/cn.archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n# 对于 CentOS 7\nsudo sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n         -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' \\\n         -i.bak \\\n         /etc/yum.repos.d/CentOS-*.repo\n\n# 对于 CentOS 8\nsudo sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n         -e 's|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' \\\n         -i.bak \\\n         /etc/yum.repos.d/CentOS-*.repo\n\n# 对于私有仓库\nsed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/\\$contentdir|baseurl=http://192.168.1.123/centos|g' -i.bak  /etc/yum.repos.d/CentOS-*.repo\n\n# 参数解释\n# \n# 以上命令是用于更改系统软件源的配置，以便从国内镜像站点下载软件包和更新。\n# \n# 对于 Ubuntu 系统，将 /etc/apt/sources.list 文件中的软件源地址 cn.archive.ubuntu.com 替换为 mirrors.ustc.edu.cn。\n# \n# 对于 CentOS 7 系统，将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为 https://mirrors.tuna.tsinghua.edu.cn/centos。\n# \n# 对于 CentOS 8 系统，同样将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为 https://mirrors.tuna.tsinghua.edu.cn/centos。\n# \n# 对于私有仓库，将 /etc/yum.repos.d/CentOS-*.repo 文件中的 mirrorlist 注释掉，并将 baseurl 的值替换为私有仓库地址 http://192.168.1.123/centos。\n# \n# 这些命令通过使用 sed 工具和正则表达式，对相应的配置文件进行批量的替换操作，从而更改系统软件源配置。\n```\n\n### 1.5.安装一些必备工具\n\n```shell\n# 对于 Ubuntu\napt update && apt upgrade -y && apt install -y wget psmisc vim net-tools nfs-kernel-server telnet lvm2 git tar curl\n\n# 对于 CentOS 7\nyum update -y && yum -y install  wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl\n\n# 对于 CentOS 8\nyum update -y && yum -y install wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git network-scripts tar curl\n```\n\n#### 1.5.1 下载离线所需文件(可选)\n\n 在互联网服务器上安装一个一模一样的系统进行下载所需包\n\n##### CentOS7\n```shell\n# 下载必要工具\nyum -y install createrepo yum-utils wget epel*\n\n# 下载全量依赖包\nrepotrack createrepo wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\n\n# 删除libseccomp\nrm -rf libseccomp-*.rpm\n\n# 下载libseccomp\nwget http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n\n# 创建yum源信息\ncreaterepo -u -d /data/centos7/\n\n# 拷贝包到内网机器上\nscp -r /data/centos7/ root@192.168.1.31:\nscp -r /data/centos7/ root@192.168.1.32:\nscp -r /data/centos7/ root@192.168.1.33:\nscp -r /data/centos7/ root@192.168.1.34:\nscp -r /data/centos7/ root@192.168.1.35:\n\n# 在内网机器上创建repo配置文件\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos7/\ngpgcheck=0\nenabled=1\nEOF\n\n# 安装下载好的包\nyum clean all\nyum makecache\nyum install /root/centos7/* --skip-broken -y\n\n#### 备注 #####\n# 安装完成后，可能还会出现yum无法使用那么再次执行\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos7/\ngpgcheck=0\nenabled=1\nEOF\nyum clean all\nyum makecache\nyum install /root/centos7/* --skip-broken -y\n\n#### 备注 #####\n# 安装 chrony 和 libseccomp\n# yum install /root/centos7/libseccomp-2.5.1*.rpm -y\n# yum install /root/centos7/chrony-*.rpm -y\n```\n##### CentOS8\n```shell\n# 下载必要工具\nyum -y install createrepo yum-utils wget epel*\n\n# 下载全量依赖包\nrepotrack wget psmisc vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git network-scripts tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\n\n# 创建yum源信息\ncreaterepo -u -d /data/centos8/\n\n# 拷贝包到内网机器上\nscp -r centos8/ root@192.168.1.31:\nscp -r centos8/ root@192.168.1.32:\nscp -r centos8/ root@192.168.1.33:\nscp -r centos8/ root@192.168.1.34:\nscp -r centos8/ root@192.168.1.35:\n\n# 在内网机器上创建repo配置文件\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos8/\ngpgcheck=0\nenabled=1\nEOF\n\n# 安装下载好的包\nyum clean all\nyum makecache\nyum install /root/centos8/* --skip-broken -y\n\n#### 备注 #####\n# 安装完成后，可能还会出现yum无法使用那么再次执行\nrm -rf /etc/yum.repos.d/*\ncat > /etc/yum.repos.d/123.repo  << EOF \n[cby]\nname=CentOS-$releasever - Media\nbaseurl=file:///root/centos8/\ngpgcheck=0\nenabled=1\nEOF\nyum clean all\nyum makecache\nyum install /root/centos8/* --skip-broken -y\n```\n\n##### Ubuntu 下载包和依赖\n```shell\n#!/bin/bash\n\nlogfile=123.log\nret=\"\"\nfunction getDepends()\n{\n   echo \"fileName is\" $1>>$logfile\n   # use tr to del < >\n   ret=`apt-cache depends $1|grep Depends |cut -d: -f2 |tr -d \"<>\"`\n   echo $ret|tee  -a $logfile\n}\n# 需要获取其所依赖包的包\nlibs=\"wget psmisc vim net-tools nfs-kernel-server telnet lvm2 git tar curl gcc keepalived haproxy bash-completion chrony sshpass ipvsadm ipset sysstat conntrack libseccomp\"\n\n# download libs dependen. deep in 3\ni=0\nwhile [ $i -lt 3 ] ;\ndo\n    let i++\n    echo $i\n    # download libs\n    newlist=\" \"\n    for j in $libs\n    do\n        added=\"$(getDepends $j)\"\n        newlist=\"$newlist $added\"\n        apt install $added --reinstall -d -y\n    done\n\n    libs=$newlist\ndone\n\n# 创建源信息\napt install dpkg-dev\nsudo cp /var/cache/apt/archives/*.deb /data/ubuntu/ -r\ndpkg-scanpackages . /dev/null |gzip > /data/ubuntu/Packages.gz -r\n\n# 拷贝包到内网机器上\nscp -r ubuntu/ root@192.168.1.31:\nscp -r ubuntu/ root@192.168.1.32:\nscp -r ubuntu/ root@192.168.1.33:\nscp -r ubuntu/ root@192.168.1.34:\nscp -r ubuntu/ root@192.168.1.35:\n\n# 在内网机器上配置apt源\nvim /etc/apt/sources.list\ncat /etc/apt/sources.list\ndeb file:////root/ ubuntu/\n\n# 安装deb包\napt install ./*.deb\n\n```\n\n\n### 1.6.选择性下载需要工具\n\n```shell\n#!/bin/bash\n\n# 查看版本地址：\n# \n# https://github.com/containernetworking/plugins/releases/\n# https://github.com/containerd/containerd/releases/\n# https://github.com/kubernetes-sigs/cri-tools/releases/\n# https://github.com/Mirantis/cri-dockerd/releases/\n# https://github.com/etcd-io/etcd/releases/\n# https://github.com/cloudflare/cfssl/releases/\n# https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG\n# https://download.docker.com/linux/static/stable/x86_64/\n# https://github.com/opencontainers/runc/releases/\n# https://mirrors.tuna.tsinghua.edu.cn/elrepo/kernel/el7/x86_64/RPMS/\n# https://github.com/helm/helm/tags\n# http://nginx.org/download/\n\n# Version numbers\ncni_plugins_version='v1.4.0'\ncri_containerd_cni_version='1.7.13'\ncrictl_version='v1.29.0'\ncri_dockerd_version='0.3.10'\netcd_version='v3.5.12'\ncfssl_version='1.6.4'\nkubernetes_server_version='1.29.2'\ndocker_version='25.0.3'\nrunc_version='1.1.12'\nkernel_version='5.4.268'\nhelm_version='3.14.1'\nnginx_version='1.25.4'\n\n# URLs \nbase_url='https://mirrors.chenby.cn/https://github.com'\nkernel_url=\"http://mirrors.tuna.tsinghua.edu.cn/elrepo/kernel/el7/x86_64/RPMS/kernel-lt-${kernel_version}-1.el7.elrepo.x86_64.rpm\"\nrunc_url=\"${base_url}/opencontainers/runc/releases/download/v${runc_version}/runc.amd64\"\ndocker_url=\"https://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-${docker_version}.tgz\"\ncni_plugins_url=\"${base_url}/containernetworking/plugins/releases/download/${cni_plugins_version}/cni-plugins-linux-amd64-${cni_plugins_version}.tgz\"\ncri_containerd_cni_url=\"${base_url}/containerd/containerd/releases/download/v${cri_containerd_cni_version}/cri-containerd-cni-${cri_containerd_cni_version}-linux-amd64.tar.gz\"\ncrictl_url=\"${base_url}/kubernetes-sigs/cri-tools/releases/download/${crictl_version}/crictl-${crictl_version}-linux-amd64.tar.gz\"\ncri_dockerd_url=\"${base_url}/Mirantis/cri-dockerd/releases/download/v${cri_dockerd_version}/cri-dockerd-${cri_dockerd_version}.amd64.tgz\"\netcd_url=\"${base_url}/etcd-io/etcd/releases/download/${etcd_version}/etcd-${etcd_version}-linux-amd64.tar.gz\"\ncfssl_url=\"${base_url}/cloudflare/cfssl/releases/download/v${cfssl_version}/cfssl_${cfssl_version}_linux_amd64\"\ncfssljson_url=\"${base_url}/cloudflare/cfssl/releases/download/v${cfssl_version}/cfssljson_${cfssl_version}_linux_amd64\"\nhelm_url=\"https://mirrors.huaweicloud.com/helm/v${helm_version}/helm-v${helm_version}-linux-amd64.tar.gz\"\nkubernetes_server_url=\"https://storage.googleapis.com/kubernetes-release/release/v${kubernetes_server_version}/kubernetes-server-linux-amd64.tar.gz\"\nnginx_url=\"http://nginx.org/download/nginx-${nginx_version}.tar.gz\"\n\n# Download packages\npackages=(\n  $kernel_url\n  $runc_url\n  $docker_url\n  $cni_plugins_url\n  $cri_containerd_cni_url\n  $crictl_url\n  $cri_dockerd_url\n  $etcd_url\n  $cfssl_url\n  $cfssljson_url\n  $helm_url\n  $kubernetes_server_url\n  $nginx_url\n)\n\nfor package_url in \"${packages[@]}\"; do\n  filename=$(basename \"$package_url\")\n  if curl --parallel --parallel-immediate -k -L -C - -o \"$filename\" \"$package_url\"; then\n    echo \"Downloaded $filename\"\n  else\n    echo \"Failed to download $filename\"\n    exit 1\n  fi\ndone\n```\n\n### 1.7.关闭防火墙\n\n```shell\n# Ubuntu忽略，CentOS执行\nsystemctl disable --now firewalld\n```\n\n### 1.8.关闭SELinux\n\n```shell\n# Ubuntu忽略，CentOS执行\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n\n# 参数解释\n# \n# setenforce 0\n# 此命令用于设置 SELinux 的执行模式。0 表示关闭 SELinux。\n# \n# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n# 该命令使用 sed 工具来编辑 /etc/selinux/config 文件。其中 '-i' 参数表示直接修改原文件，而不是输出到终端或另一个文件。's#SELINUX=enforcing#SELINUX=disabled#g' 是 sed 的替换命令，它将文件中所有的 \"SELINUX=enforcing\" 替换为 \"SELINUX=disabled\"。这里的 '#' 是分隔符，用于替代传统的 '/' 分隔符，以避免与路径中的 '/' 冲突。\n```\n\n### 1.9.关闭交换分区\n\n```shell\nsed -ri 's/.*swap.*/#&/' /etc/fstab\nswapoff -a && sysctl -w vm.swappiness=0\n\ncat /etc/fstab\n# /dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n# 参数解释：\n# \n# -ri: 这个参数用于在原文件中替换匹配的模式。-r表示扩展正则表达式，-i允许直接修改文件。\n# 's/.*swap.*/#&/': 这是一个sed命令，用于在文件/etc/fstab中找到包含swap的行，并在行首添加#来注释掉该行。\n# /etc/fstab: 这是一个文件路径，即/etc/fstab文件，用于存储文件系统表。\n# swapoff -a: 这个命令用于关闭所有启用的交换分区。\n# sysctl -w vm.swappiness=0: 这个命令用于修改vm.swappiness参数的值为0，表示系统在物理内存充足时更倾向于使用物理内存而非交换分区。\n```\n\n### 1.10.网络配置（俩种方式二选一）\n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 方式一\n# systemctl disable --now NetworkManager\n# systemctl start network && systemctl enable network\n\n# 方式二\ncat > /etc/NetworkManager/conf.d/calico.conf << EOF \n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*\nEOF\nsystemctl restart NetworkManager\n\n# 参数解释\n#\n# 这个参数用于指定不由 NetworkManager 管理的设备。它由以下两个部分组成\n# \n# interface-name:cali*\n# 表示以 \"cali\" 开头的接口名称被排除在 NetworkManager 管理之外。例如，\"cali0\", \"cali1\" 等接口不受 NetworkManager 管理。\n# \n# interface-name:tunl*\n# 表示以 \"tunl\" 开头的接口名称被排除在 NetworkManager 管理之外。例如，\"tunl0\", \"tunl1\" 等接口不受 NetworkManager 管理。\n# \n# 通过使用这个参数，可以将特定的接口排除在 NetworkManager 的管理范围之外，以便其他工具或进程可以独立地管理和配置这些接口。\n```\n\n### 1.11.进行时间同步 \n\n```shell\n# 服务端\n# apt install chrony -y\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \npool ntp.aliyun.com iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nallow 192.168.1.0/24\nlocal stratum 10\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n\nsystemctl restart chronyd ; systemctl enable chronyd\n\n# 客户端\n# apt install chrony -y\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \npool 192.168.1.31 iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nkeyfile /etc/chrony.keys\nleapsectz right/UTC\nlogdir /var/log/chrony\nEOF\n\nsystemctl restart chronyd ; systemctl enable chronyd\n\n#使用客户端进行验证\nchronyc sources -v\n\n# 参数解释\n#\n# pool ntp.aliyun.com iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度。\n# \n# driftfile /var/lib/chrony/drift\n# 指定用于保存时钟漂移信息的文件路径。\n# \n# makestep 1.0 3\n# 设置当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整。\n# \n# rtcsync\n# 启用硬件时钟同步功能，可以提高时钟的准确性。\n# \n# allow 192.168.0.0/24\n# 允许192.168.0.0/24网段范围内的主机与chrony进行时间同步。\n# \n# local stratum 10\n# 将本地时钟设为stratum 10，stratum值表示时钟的准确度，值越小表示准确度越高。\n# \n# keyfile /etc/chrony.keys\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证。\n# \n# leapsectz right/UTC\n# 指定时区为UTC。\n# \n# logdir /var/log/chrony\n# 指定日志文件存放目录。\n```\n\n### 1.12.配置ulimit\n\n```shell\nulimit -SHn 65535\ncat >> /etc/security/limits.conf <<EOF\n* soft nofile 655360\n* hard nofile 131072\n* soft nproc 655350\n* hard nproc 655350\n* seft memlock unlimited\n* hard memlock unlimitedd\nEOF\n\n# 参数解释\n#\n# soft nofile 655360\n# soft表示软限制，nofile表示一个进程可打开的最大文件数，默认值为1024。这里的软限制设置为655360，即一个进程可打开的最大文件数为655360。\n#\n# hard nofile 131072\n# hard表示硬限制，即系统设置的最大值。nofile表示一个进程可打开的最大文件数，默认值为4096。这里的硬限制设置为131072，即系统设置的最大文件数为131072。\n#\n# soft nproc 655350\n# soft表示软限制，nproc表示一个用户可创建的最大进程数，默认值为30720。这里的软限制设置为655350，即一个用户可创建的最大进程数为655350。\n#\n# hard nproc 655350\n# hard表示硬限制，即系统设置的最大值。nproc表示一个用户可创建的最大进程数，默认值为4096。这里的硬限制设置为655350，即系统设置的最大进程数为655350。\n#\n# seft memlock unlimited\n# seft表示软限制，memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的软限制设置为unlimited，即一个进程可锁定的最大内存为无限制。\n#\n# hard memlock unlimited\n# hard表示硬限制，即系统设置的最大值。memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的硬限制设置为unlimited，即系统设置的最大内存锁定为无限制。\n```\n\n### 1.13.配置免密登录\n\n```shell\n# apt install -y sshpass\nyum install -y sshpass\nssh-keygen -f /root/.ssh/id_rsa -P ''\nexport IP=\"192.168.1.31 192.168.1.32 192.168.1.33 192.168.1.34 192.168.1.35\"\nexport SSHPASS=123123\nfor HOST in $IP;do\n     sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $HOST\ndone\n\n# 这段脚本的作用是在一台机器上安装sshpass工具，并通过sshpass自动将本机的SSH公钥复制到多个远程主机上，以实现无需手动输入密码的SSH登录。\n# \n# 具体解释如下：\n# \n# 1. `apt install -y sshpass` 或 `yum install -y sshpass`：通过包管理器（apt或yum）安装sshpass工具，使得后续可以使用sshpass命令。\n# \n# 2. `ssh-keygen -f /root/.ssh/id_rsa -P ''`：生成SSH密钥对。该命令会在/root/.ssh目录下生成私钥文件id_rsa和公钥文件id_rsa.pub，同时不设置密码（即-P参数后面为空），方便后续通过ssh-copy-id命令自动复制公钥。\n# \n# 3. `export IP=\"192.168.1.31 192.168.1.32 192.168.1.33 192.168.1.34 192.168.1.35\"`：设置一个包含多个远程主机IP地址的环境变量IP，用空格分隔开，表示要将SSH公钥复制到这些远程主机上。\n# \n# 4. `export SSHPASS=123123`：设置环境变量SSHPASS，将sshpass所需的SSH密码（在这里是\"123123\"）赋值给它，这样sshpass命令可以自动使用这个密码进行登录。\n# \n# 5. `for HOST in $IP;do`：遍历环境变量IP中的每个IP地址，并将当前IP地址赋值给变量HOST。\n# \n# 6. `sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $HOST`：使用sshpass工具复制本机的SSH公钥到远程主机。其中，-e选项表示使用环境变量中的密码（即SSHPASS）进行登录，-o StrictHostKeyChecking=no选项表示连接时不检查远程主机的公钥，以避免交互式确认。\n# \n# 通过这段脚本，可以方便地将本机的SSH公钥复制到多个远程主机上，实现无需手动输入密码的SSH登录。\n```\n\n### 1.14.添加启用源 \n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 为 RHEL-8或 CentOS-8配置源\nyum install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm -y \nsed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo \nsed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo \n\n# 为 RHEL-7 SL-7 或 CentOS-7 安装 ELRepo \nyum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y \nsed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo \nsed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo \n\n# 查看可用安装包\nyum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available\n```\n\n### 1.15.升级内核至4.18版本以上 \n\n```shell\n# Ubuntu忽略，CentOS执行\n\n# 安装最新的内核\n# 我这里选择的是稳定版kernel-ml   如需更新长期维护版本kernel-lt  \nyum -y --enablerepo=elrepo-kernel  install  kernel-ml\n\n# 查看已安装那些内核\nrpm -qa | grep kernel\n\n# 查看默认内核\ngrubby --default-kernel\n\n# 若不是最新的使用命令设置\ngrubby --set-default $(ls /boot/vmlinuz-* | grep elrepo)\n\n# 重启生效\nreboot\n\n# v8 整合命令为：\nyum install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm -y ; sed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo ; sed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo ; yum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available -y ; yum  --enablerepo=elrepo-kernel  install kernel-lt -y ; grubby --default-kernel ; reboot \n\n# v7 整合命令为：\nyum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y ; sed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo ; sed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo ; yum  --disablerepo=\"*\"  --enablerepo=\"elrepo-kernel\"  list  available -y ; yum  --enablerepo=elrepo-kernel  install  kernel-lt -y ; grubby --set-default $(ls /boot/vmlinuz-* | grep elrepo) ; grubby --default-kernel ; reboot \n\n# 离线版本 \nyum install -y /root/cby/kernel-lt-*-1.el7.elrepo.x86_64.rpm ; grubby --set-default $(ls /boot/vmlinuz-* | grep elrepo) ; grubby --default-kernel ; reboot \n```\n\n### 1.16.安装ipvsadm \n\n```shell\n# 对于CentOS7离线安装\n# yum install /root/centos7/ipset-*.el7.x86_64.rpm /root/centos7/lm_sensors-libs-*.el7.x86_64.rpm  /root/centos7/ipset-libs-*.el7.x86_64.rpm /root/centos7/sysstat-*.el7_9.x86_64.rpm  /root/centos7/ipvsadm-*.el7.x86_64.rpm  -y\n\n# 对于 Ubuntu\n# apt install ipvsadm ipset sysstat conntrack -y\n\n# 对于 CentOS\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\ncat >> /etc/modules-load.d/ipvs.conf <<EOF \nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\nEOF\n\nsystemctl restart systemd-modules-load.service\n\nlsmod | grep -e ip_vs -e nf_conntrack\nip_vs_sh               16384  0\nip_vs_wrr              16384  0\nip_vs_rr               16384  0\nip_vs                 180224  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr\nnf_conntrack          176128  1 ip_vs\nnf_defrag_ipv6         24576  2 nf_conntrack,ip_vs\nnf_defrag_ipv4         16384  1 nf_conntrack\nlibcrc32c              16384  3 nf_conntrack,xfs,ip_vs\n\n# 参数解释\n#\n# ip_vs\n# IPVS 是 Linux 内核中的一个模块，用于实现负载均衡和高可用性。它通过在前端代理服务器上分发传入请求到后端实际服务器上，提供了高性能和可扩展的网络服务。\n#\n# ip_vs_rr\n# IPVS 的一种调度算法之一，使用轮询方式分发请求到后端服务器，每个请求按顺序依次分发。\n#\n# ip_vs_wrr\n# IPVS 的一种调度算法之一，使用加权轮询方式分发请求到后端服务器，每个请求按照指定的权重比例分发。\n#\n# ip_vs_sh\n# IPVS 的一种调度算法之一，使用哈希方式根据源 IP 地址和目标 IP 地址来分发请求。\n#\n# nf_conntrack\n# 这是一个内核模块，用于跟踪和管理网络连接，包括 TCP、UDP 和 ICMP 等协议。它是实现防火墙状态跟踪的基础。\n#\n# ip_tables\n# 这是一个内核模块，提供了对 Linux 系统 IP 数据包过滤和网络地址转换（NAT）功能的支持。\n#\n# ip_set\n# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的 IP 地址集合操作。\n#\n# xt_set\n# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的数据包匹配和操作。\n#\n# ipt_set\n# 这是一个用户空间工具，用于配置和管理 xt_set 内核模块。\n#\n# ipt_rpfilter\n# 这是一个内核模块，用于实现反向路径过滤，用于防止 IP 欺骗和 DDoS 攻击。\n#\n# ipt_REJECT\n# 这是一个 iptables 目标，用于拒绝 IP 数据包，并向发送方发送响应，指示数据包被拒绝。\n#\n# ipip\n# 这是一个内核模块，用于实现 IP 封装在 IP（IP-over-IP）的隧道功能。它可以在不同网络之间创建虚拟隧道来传输 IP 数据包。\n```\n\n### 1.17.修改内核参数 \n\n```shell\ncat <<EOF > /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nfs.may_detach_mounts = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\n\nnet.ipv6.conf.all.disable_ipv6 = 0\nnet.ipv6.conf.default.disable_ipv6 = 0\nnet.ipv6.conf.lo.disable_ipv6 = 0\nnet.ipv6.conf.all.forwarding = 1\nEOF\n\nsysctl --system\n\n# 这些是Linux系统的一些参数设置，用于配置和优化网络、文件系统和虚拟内存等方面的功能。以下是每个参数的详细解释：\n# \n# 1. net.ipv4.ip_forward = 1\n#    - 这个参数启用了IPv4的IP转发功能，允许服务器作为网络路由器转发数据包。\n# \n# 2. net.bridge.bridge-nf-call-iptables = 1\n#    - 当使用网络桥接技术时，将数据包传递到iptables进行处理。\n#   \n# 3. fs.may_detach_mounts = 1\n#    - 允许在挂载文件系统时，允许被其他进程使用。\n#   \n# 4. vm.overcommit_memory=1\n#    - 该设置允许原始的内存过量分配策略，当系统的内存已经被完全使用时，系统仍然会分配额外的内存。\n# \n# 5. vm.panic_on_oom=0\n#    - 当系统内存不足（OOM）时，禁用系统崩溃和重启。\n# \n# 6. fs.inotify.max_user_watches=89100\n#    - 设置系统允许一个用户的inotify实例可以监控的文件数目的上限。\n# \n# 7. fs.file-max=52706963\n#    - 设置系统同时打开的文件数的上限。\n# \n# 8. fs.nr_open=52706963\n#    - 设置系统同时打开的文件描述符数的上限。\n# \n# 9. net.netfilter.nf_conntrack_max=2310720\n#    - 设置系统可以创建的网络连接跟踪表项的最大数量。\n# \n# 10. net.ipv4.tcp_keepalive_time = 600\n#     - 设置TCP套接字的空闲超时时间（秒），超过该时间没有活动数据时，内核会发送心跳包。\n# \n# 11. net.ipv4.tcp_keepalive_probes = 3\n#     - 设置未收到响应的TCP心跳探测次数。\n# \n# 12. net.ipv4.tcp_keepalive_intvl = 15\n#     - 设置TCP心跳探测的时间间隔（秒）。\n# \n# 13. net.ipv4.tcp_max_tw_buckets = 36000\n#     - 设置系统可以使用的TIME_WAIT套接字的最大数量。\n# \n# 14. net.ipv4.tcp_tw_reuse = 1\n#     - 启用TIME_WAIT套接字的重新利用，允许新的套接字使用旧的TIME_WAIT套接字。\n# \n# 15. net.ipv4.tcp_max_orphans = 327680\n#     - 设置系统可以同时存在的TCP套接字垃圾回收包裹数的最大数量。\n# \n# 16. net.ipv4.tcp_orphan_retries = 3\n#     - 设置系统对于孤立的TCP套接字的重试次数。\n# \n# 17. net.ipv4.tcp_syncookies = 1\n#     - 启用TCP SYN cookies保护，用于防止SYN洪泛攻击。\n# \n# 18. net.ipv4.tcp_max_syn_backlog = 16384\n#     - 设置新的TCP连接的半连接数（半连接队列）的最大长度。\n# \n# 19. net.ipv4.ip_conntrack_max = 65536\n#     - 设置系统可以创建的网络连接跟踪表项的最大数量。\n# \n# 20. net.ipv4.tcp_timestamps = 0\n#     - 关闭TCP时间戳功能，用于提供更好的安全性。\n# \n# 21. net.core.somaxconn = 16384\n#     - 设置系统核心层的连接队列的最大值。\n# \n# 22. net.ipv6.conf.all.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 23. net.ipv6.conf.default.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 24. net.ipv6.conf.lo.disable_ipv6 = 0\n#     - 启用IPv6协议。\n# \n# 25. net.ipv6.conf.all.forwarding = 1\n#     - 允许IPv6数据包转发。\n```\n\n### 1.18.所有节点配置hosts本地解析\n\n```shell\ncat > /etc/hosts <<EOF\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n192.168.1.31 k8s-master01\n192.168.1.32 k8s-master02\n192.168.1.33 k8s-master03\n192.168.1.34 k8s-node01\n192.168.1.35 k8s-node02\n192.168.1.36 lb-vip\nEOF\n```\n\n# 2.k8s基本组件安装\n\n**注意 ：  2.1 和 2.2 二选其一即可**\n\n## 2.1.安装Containerd作为Runtime （推荐）\n\n```shell\n# https://github.com/containernetworking/plugins/releases/\n# wget https://mirrors.chenby.cn/https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz\n\ncd cby/\n\n#创建cni插件所需目录\nmkdir -p /etc/cni/net.d /opt/cni/bin \n#解压cni二进制包\ntar xf cni-plugins-linux-amd64-v*.tgz -C /opt/cni/bin/\n\n# https://github.com/containerd/containerd/releases/\n# wget https://mirrors.chenby.cn/https://github.com/containerd/containerd/releases/download/v1.7.13/cri-containerd-cni-1.7.13-linux-amd64.tar.gz\n\n#解压\ntar -xzf cri-containerd-cni-*-linux-amd64.tar.gz -C /\n\n#创建服务启动文件\ncat > /etc/systemd/system/containerd.service <<EOF\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target local-fs.target\n\n[Service]\nExecStartPre=-/sbin/modprobe overlay\nExecStart=/usr/local/bin/containerd\nType=notify\nDelegate=yes\nKillMode=process\nRestart=always\nRestartSec=5\nLimitNPROC=infinity\nLimitCORE=infinity\nLimitNOFILE=infinity\nTasksMax=infinity\nOOMScoreAdjust=-999\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n# 参数解释：\n#\n# 这是一个用于启动containerd容器运行时的systemd unit文件。下面是对该文件不同部分的详细解释：\n# \n# [Unit]\n# Description=containerd container runtime\n# 描述该unit的作用是作为containerd容器运行时。\n# \n# Documentation=https://containerd.io\n# 指向容器运行时的文档的URL。\n# \n# After=network.target local-fs.target\n# 定义了在哪些依赖项之后该unit应该被启动。在网络和本地文件系统加载完成后启动，确保了容器运行时在这些依赖项可用时才会启动。\n# \n# [Service]\n# ExecStartPre=-/sbin/modprobe overlay\n# 在启动containerd之前执行的命令。这里的命令是尝试加载内核的overlay模块，如果失败则忽略错误继续执行下面的命令。\n# \n# ExecStart=/usr/local/bin/containerd\n# 实际执行的命令，用于启动containerd容器运行时。\n# \n# Type=notify\n# 指定服务的通知类型。这里使用notify类型，表示当服务就绪时会通过通知的方式告知systemd。\n# \n# Delegate=yes\n# 允许systemd对此服务进行重启和停止操作。\n# \n# KillMode=process\n# 在终止容器运行时时使用的kill模式。这里使用process模式，表示通过终止进程来停止容器运行时。\n# \n# Restart=always\n# 定义了当容器运行时终止后的重启策略。这里设置为always，表示无论何时终止容器运行时，都会自动重新启动。\n# \n# RestartSec=5\n# 在容器运行时终止后重新启动之前等待的秒数。\n# \n# LimitNPROC=infinity\n# 指定容器运行时可以使用的最大进程数量。这里设置为无限制。\n# \n# LimitCORE=infinity\n# 指定容器运行时可以使用的最大CPU核心数量。这里设置为无限制。\n# \n# LimitNOFILE=infinity\n# 指定容器运行时可以打开的最大文件数。这里设置为无限制。\n# \n# TasksMax=infinity\n# 指定容器运行时可以创建的最大任务数。这里设置为无限制。\n# \n# OOMScoreAdjust=-999\n# 指定容器运行时的OOM（Out-Of-Memory）分数调整值。负数值表示容器运行时的优先级较高。\n# \n# [Install]\n# WantedBy=multi-user.target\n# 定义了服务的安装位置。这里指定为multi-user.target，表示将服务安装为多用户模式下的启动项。\n```\n\n### 2.1.1配置Containerd所需的模块\n\n```shell\ncat <<EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\n# 参数解释：\n#\n# containerd是一个容器运行时，用于管理和运行容器。它支持多种不同的参数配置来自定义容器运行时的行为和功能。\n# \n# 1. overlay：overlay是容器d默认使用的存储驱动，它提供了一种轻量级的、可堆叠的、逐层增量的文件系统。它通过在现有文件系统上叠加文件系统层来创建容器的文件系统视图。每个容器可以有自己的一组文件系统层，这些层可以共享基础镜像中的文件，并在容器内部进行修改。使用overlay可以有效地使用磁盘空间，并使容器更加轻量级。\n# \n# 2. br_netfilter：br_netfilter是Linux内核提供的一个网络过滤器模块，用于在容器网络中进行网络过滤和NAT转发。当容器和主机之间的网络通信需要进行DNAT或者SNAT时，br_netfilter模块可以将IP地址进行转换。它还可以提供基于iptables规则的网络过滤功能，用于限制容器之间或容器与外部网络之间的通信。\n# \n# 这些参数可以在containerd的配置文件或者命令行中指定。例如，可以通过设置--storage-driver参数来选择使用overlay作为存储驱动，通过设置--iptables参数来启用或禁用br_netfilter模块。具体的使用方法和配置细节可以参考containerd的官方文档。\n```\n\n### 2.1.2加载模块\n\n```shell\nsystemctl restart systemd-modules-load.service\n\n# 参数解释：\n# - `systemctl`: 是Linux系统管理服务的命令行工具，可以管理systemd init系统。\n# - `restart`: 是systemctl命令的一个选项，用于重新启动服务。\n# - `systemd-modules-load.service`: 是一个系统服务，用于加载内核模块。\n# \n# 将上述参数结合在一起来解释`systemctl restart systemd-modules-load.service`的含义：\n# 这个命令用于重新启动系统服务`systemd-modules-load.service`，它是负责加载内核模块的服务。在重新启动该服务后，系统会重新加载所有的内核模块。\n```\n\n### 2.1.3配置Containerd所需的内核\n\n```shell\ncat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\n# 加载内核\nsysctl --system\n\n# 参数解释：\n# \n# 这些参数是Linux操作系统中用于网络和网络桥接设置的参数。\n# \n# - net.bridge.bridge-nf-call-iptables：这个参数控制网络桥接设备是否调用iptables规则处理网络数据包。当该参数设置为1时，网络数据包将被传递到iptables进行处理；当该参数设置为0时，网络数据包将绕过iptables直接传递。默认情况下，这个参数的值是1，即启用iptables规则处理网络数据包。\n# \n# - net.ipv4.ip_forward：这个参数用于控制是否启用IP转发功能。IP转发使得操作系统可以将接收到的数据包从一个网络接口转发到另一个网络接口。当该参数设置为1时，启用IP转发功能；当该参数设置为0时，禁用IP转发功能。在网络环境中，通常需要启用IP转发功能来实现不同网络之间的通信。默认情况下，这个参数的值是0，即禁用IP转发功能。\n# \n# - net.bridge.bridge-nf-call-ip6tables：这个参数与net.bridge.bridge-nf-call-iptables类似，但是它用于IPv6数据包的处理。当该参数设置为1时，IPv6数据包将被传递到ip6tables进行处理；当该参数设置为0时，IPv6数据包将绕过ip6tables直接传递。默认情况下，这个参数的值是1，即启用ip6tables规则处理IPv6数据包。\n# \n# 这些参数的值可以通过修改操作系统的配置文件（通常是'/etc/sysctl.conf'）来进行设置。修改完成后，需要使用'sysctl -p'命令重载配置文件使参数生效。\n```\n\n### 2.1.4创建Containerd的配置文件\n\n```shell\n# 参数解释：\n# \n# 这段代码是用于修改并配置containerd的参数。\n# \n# 1. 首先使用命令`mkdir -p /etc/containerd`创建/etc/containerd目录，如果该目录已存在，则不进行任何操作。\n# 2. 使用命令`containerd config default | tee /etc/containerd/config.toml`创建默认配置文件，并将输出同时传递给/etc/containerd/config.toml文件。\n# 3. 使用sed命令修改/etc/containerd/config.toml文件，将SystemdCgroup参数的值从false改为true。-i参数表示直接在原文件中进行编辑。\n# 4. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中SystemdCgroup参数的值是否已修改为true。\n# 5. 使用sed命令修改/etc/containerd/config.toml文件，将registry.k8s.io的地址替换为m.daocloud.io/registry.k8s.io。-i参数表示直接在原文件中进行编辑。\n# 6. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中sandbox_image参数的值是否已修改为m.daocloud.io/registry.k8s.io。\n# 7. 使用sed命令修改/etc/containerd/config.toml文件，将config_path参数的值从\"\"改为\"/etc/containerd/certs.d\"。-i参数表示直接在原文件中进行编辑。\n# 8. 使用cat命令结合grep命令查看/etc/containerd/config.toml文件中certs.d参数的值是否已修改为/etc/containerd/certs.d。\n# 9. 使用mkdir命令创建/etc/containerd/certs.d/docker.io目录，如果目录已存在，则不进行任何操作。-p参数表示创建目录时，如果父级目录不存在，则自动创建父级目录。\n# \n# 最后，使用cat重定向操作符将内容写入/etc/containerd/certs.d/docker.io/hosts.toml文件。该文件会配置加速器，其中server参数设置为\"https://docker.io\"，host参数设置为\"https://hub-mirror.c.163.com\"，并添加capabilities参数。\n\n# 创建默认配置文件\nmkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml\n\n# 修改Containerd的配置文件\nsed -i \"s#SystemdCgroup\\ \\=\\ false#SystemdCgroup\\ \\=\\ true#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep SystemdCgroup\nsed -i \"s#registry.k8s.io#m.daocloud.io/registry.k8s.io#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep sandbox_image\nsed -i \"s#config_path\\ \\=\\ \\\"\\\"#config_path\\ \\=\\ \\\"/etc/containerd/certs.d\\\"#g\" /etc/containerd/config.toml\ncat /etc/containerd/config.toml | grep certs.d\n\n# 配置加速器\nmkdir /etc/containerd/certs.d/docker.io -pv\ncat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF\nserver = \"https://docker.io\"\n[host.\"https://docker.mirrors.ustc.edu.cn\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n\n# 注意！\n# SystemdCgroup参数是containerd中的一个配置参数，用于设置containerd在运行过程中使用的Cgroup（控制组）路径。Containerd使用SystemdCgroup参数来指定应该使用哪个Cgroup来跟踪和管理容器的资源使用。\n# \n# Cgroup是Linux内核提供的一种资源隔离和管理机制，可以用于限制、分配和监控进程组的资源使用。使用Cgroup，可以将容器的资源限制和隔离，以防止容器之间的资源争用和不公平的竞争。\n# \n# 通过设置SystemdCgroup参数，可以确保containerd能够找到正确的Cgroup路径，并正确地限制和隔离容器的资源使用，确保容器可以按照预期的方式运行。如果未正确设置SystemdCgroup参数，可能会导致容器无法正确地使用资源，或者无法保证资源的公平分配和隔离。\n# \n# 总而言之，SystemdCgroup参数的作用是为了确保containerd能够正确地管理容器的资源使用，以实现资源的限制、隔离和公平分配。\n```\n\n### 2.1.5启动并设置为开机启动\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now containerd.service\n# 启用并立即启动docker.service单元。docker.service是Docker守护进程的systemd服务单元。\n\nsystemctl stop containerd.service\n# 停止运行中的docker.service单元，即停止Docker守护进程。\n\nsystemctl start containerd.service\n# 启动docker.service单元，即启动Docker守护进程。\n\nsystemctl restart containerd.service\n# 重启docker.service单元，即重新启动Docker守护进程。\n\nsystemctl status containerd.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 2.1.6配置crictl客户端连接的运行时位置\n\n```shell\n# https://github.com/kubernetes-sigs/cri-tools/releases/\n# wget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.29.0/crictl-v1.29.0-linux-amd64.tar.gz\n\n#解压\ntar xf crictl-v*-linux-amd64.tar.gz -C /usr/bin/\n#生成配置文件\ncat > /etc/crictl.yaml <<EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n\n#测试\nsystemctl restart  containerd\ncrictl info\n\n# 注意！\n# 下面是参数`crictl`的详细解释\n# \n# `crictl`是一个用于与容器运行时通信的命令行工具。它是容器运行时接口（CRI）工具的一个实现，可以对容器运行时进行管理和操作。\n# \n# 1. `runtime-endpoint: unix:///run/containerd/containerd.sock`\n# 指定容器运行时的终端套接字地址。在这个例子中，指定的地址是`unix:///run/containerd/containerd.sock`，这是一个Unix域套接字地址。\n# \n# 2. `image-endpoint: unix:///run/containerd/containerd.sock`\n# 指定容器镜像服务的终端套接字地址。在这个例子中，指定的地址是`unix:///run/containerd/containerd.sock`，这是一个Unix域套接字地址。\n# \n# 3. `timeout: 10`\n# 设置与容器运行时通信的超时时间，单位是秒。在这个例子中，超时时间被设置为10秒。\n# \n# 4. `debug: false`\n# 指定是否开启调式模式。在这个例子中，调式模式被设置为关闭，即`false`。如果设置为`true`，则会输出更详细的调试信息。\n# \n# 这些参数可以根据需要进行修改，以便与容器运行时进行有效的通信和管理。\n```\n\n## 2.2 安装docker作为Runtime \n\n### 2.2.1 解压docker程序\n\n```shell\n# 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/\n# wget https://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-25.0.3.tgz\n\n#解压\ntar xf docker-*.tgz \n#拷贝二进制文件\ncp docker/* /usr/bin/\n```\n\n### 2.2.2 创建containerd的service文件\n\n```shell\n#创建containerd的service文件,并且启动\ncat >/etc/systemd/system/containerd.service <<EOF\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target local-fs.target\n\n[Service]\nExecStartPre=-/sbin/modprobe overlay\nExecStart=/usr/bin/containerd\nType=notify\nDelegate=yes\nKillMode=process\nRestart=always\nRestartSec=5\nLimitNPROC=infinity\nLimitCORE=infinity\nLimitNOFILE=1048576\nTasksMax=infinity\nOOMScoreAdjust=-999\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 参数解释：\n# \n# [Unit]\n# - Description=containerd container runtime：指定服务的描述信息。\n# - Documentation=https://containerd.io：指定服务的文档链接。\n# - After=network.target local-fs.target：指定服务的启动顺序，在网络和本地文件系统启动之后再启动该服务。\n# \n# [Service]\n# - ExecStartPre=-/sbin/modprobe overlay：在启动服务之前执行的命令，使用`-`表示忽略错误。\n# - ExecStart=/usr/bin/containerd：指定服务的启动命令。\n# - Type=notify：指定服务的类型，`notify`表示服务会在启动完成后向systemd发送通知。\n# - Delegate=yes：允许服务代理其他服务的应答，例如收到关机命令后终止其他服务。\n# - KillMode=process：指定服务终止时的行为，`process`表示终止服务进程。\n# - Restart=always：指定服务终止后是否自动重启，`always`表示总是自动重启。\n# - RestartSec=5：指定服务重启的时间间隔，单位为秒。\n# - LimitNPROC=infinity：限制服务的最大进程数，`infinity`表示没有限制。\n# - LimitCORE=infinity：限制服务的最大核心数，`infinity`表示没有限制。\n# - LimitNOFILE=1048576：限制服务的最大文件数，指定为1048576。\n# - TasksMax=infinity：限制服务的最大任务数，`infinity`表示没有限制。\n# - OOMScoreAdjust=-999：指定服务的OOM（Out of Memory）得分，负数表示降低被终止的概率。\n# \n# [Install]\n# - WantedBy=multi-user.target：指定服务的安装方式，`multi-user.target`表示该服务在多用户模式下安装。\n\n\n# 设置开机自启\nsystemctl enable --now containerd.service\n```\n\n### 2.2.3 准备docker的service文件\n\n```shell\n#准备docker的service文件\ncat > /etc/systemd/system/docker.service <<EOF\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\nOOMScoreAdjust=-500\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 参数解释：\n# \n# [Unit]\n# - Description: 描述服务的作用，这里是Docker Application Container Engine，即Docker应用容器引擎。\n# - Documentation: 提供关于此服务的文档链接，这里是Docker官方文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# \n# [Service]\n# - Type: 服务类型，这里是notify，表示服务在启动完成时发送通知。\n# - ExecStart: 命令，启动该服务时会执行的命令，这里是/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock，即启动dockerd并指定一些参数，其中-H指定dockerd的监听地址为fd://，--containerd指定containerd的sock文件位置。\n# - ExecReload: 重载命令，当接收到HUP信号时执行的命令，这里是/bin/kill -s HUP $MAINPID，即发送HUP信号给主进程ID。\n# - TimeoutSec: 服务超时时间，这里是0，表示没有超时限制。\n# - RestartSec: 重启间隔时间，这里是2秒，表示重启失败后等待2秒再重启。\n# - Restart: 重启策略，这里是always，表示总是重启。\n# - StartLimitBurst: 启动限制次数，这里是3，表示在启动失败后最多重试3次。\n# - StartLimitInterval: 启动限制时间间隔，这里是60秒，表示两次启动之间最少间隔60秒。\n# - LimitNOFILE: 文件描述符限制，这里是infinity，表示没有限制。\n# - LimitNPROC: 进程数限制，这里是infinity，表示没有限制。\n# - LimitCORE: 核心转储限制，这里是infinity，表示没有限制。\n# - TasksMax: 最大任务数，这里是infinity，表示没有限制。\n# - Delegate: 修改权限，这里是yes，表示启用权限修改。\n# - KillMode: 杀死模式，这里是process，表示杀死整个进程组。\n# - OOMScoreAdjust: 用于调整进程在系统内存紧张时的优先级调整，这里是-500，表示将OOM分数降低500。\n# \n# [Install]\n# - WantedBy: 安装目标，这里是multi-user.target，表示在多用户模式下安装。\n#      在WantedBy参数中，我们可以使用以下参数：\n#      1. multi-user.target：指定该服务应该在多用户模式下启动。\n#      2. graphical.target：指定该服务应该在图形化界面模式下启动。\n#      3. default.target：指定该服务应该在系统的默认目标（runlevel）下启动。\n#      4. rescue.target：指定该服务应该在系统救援模式下启动。\n#      5. poweroff.target：指定该服务应该在关机时启动。\n#      6. reboot.target：指定该服务应该在重启时启动。\n#      7. halt.target：指定该服务应该在停止时启动。\n#      8. shutdown.target：指定该服务应该在系统关闭时启动。\n#      这些参数可以根据需要选择一个或多个，以告知系统在何时启动该服务。\n\n```\n\n### 2.2.4 准备docker的socket文件\n\n```shell\n#准备docker的socket文件\ncat > /etc/systemd/system/docker.socket <<EOF\n[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=/var/run/docker.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n\n[Install]\nWantedBy=sockets.target\nEOF\n\n# 这是一个用于Docker API的socket配置文件，包含了以下参数：\n# \n# [Unit]\n# - Description：描述了该socket的作用，即为Docker API的socket。\n# \n# [Socket]\n# - ListenStream：指定了socket的监听地址，该socket会监听在/var/run/docker.sock上，即Docker守护程序使用的默认sock文件。\n# - SocketMode：指定了socket文件的权限模式，此处为0660，即用户和用户组有读写权限，其他用户无权限。\n# - SocketUser：指定了socket文件的所有者，此处为root用户。\n# - SocketGroup：指定了socket文件的所属用户组，此处为docker用户组。\n# \n# [Install]\n# - WantedBy：指定了该socket被启用时的目标，此处为sockets.target，表示当sockets.target启动时启用该socket。\n# \n# 该配置文件的作用是为Docker提供API访问的通道，它监听在/var/run/docker.sock上，具有root用户权限，但只接受docker用户组的成员的连接，并且其他用户无法访问。这样，只有docker用户组的成员可以通过该socket与Docker守护进程进行通信。\n\n```\n\n### 2.2.5 配置加速器\n\n```shell\n# 配置加速器\nmkdir /etc/docker/ -pv\ncat >/etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"registry-mirrors\": [\n    \"https://docker.mirrors.ustc.edu.cn\"\n  ],\n  \"max-concurrent-downloads\": 10,\n  \"log-driver\": \"json-file\",\n  \"log-level\": \"warn\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n    },\n  \"data-root\": \"/var/lib/docker\"\n}\nEOF\n\n\n# 该参数文件中包含以下参数：\n# \n# 1. exec-opts: 用于设置Docker守护进程的选项，native.cgroupdriver=systemd表示使用systemd作为Cgroup驱动程序。\n# 2. registry-mirrors: 用于指定Docker镜像的镜像注册服务器。在这里有三个镜像注册服务器：https://docker.m.daocloud.io、https://docker.mirrors.ustc.edu.cn和http://hub-mirror.c.163.com。\n# 3. max-concurrent-downloads: 用于设置同时下载镜像的最大数量，默认值为3，这里设置为10。\n# 4. log-driver: 用于设置Docker守护进程的日志驱动程序，这里设置为json-file。\n# 5. log-level: 用于设置日志的级别，这里设置为warn。\n# 6. log-opts: 用于设置日志驱动程序的选项，这里有两个选项：max-size和max-file。max-size表示每个日志文件的最大大小，这里设置为10m，max-file表示保存的最大日志文件数量，这里设置为3。\n# 7. data-root: 用于设置Docker守护进程的数据存储根目录，默认为/var/lib/docker，这里设置为/var/lib/docker。\n\n```\n\n### 2.2.6 启动docker\n\n```shell\ngroupadd docker\n#创建docker组\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now docker.socket\n# 启用并立即启动docker.socket单元。docker.socket是一个systemd的socket单元，用于接收来自网络的Docker API请求。\n\nsystemctl enable --now docker.service\n# 启用并立即启动docker.service单元。docker.service是Docker守护进程的systemd服务单元。\n\nsystemctl stop docker.service\n# 停止运行中的docker.service单元，即停止Docker守护进程。\n\nsystemctl start docker.service\n# 启动docker.service单元，即启动Docker守护进程。\n\nsystemctl restart docker.service\n# 重启docker.service单元，即重新启动Docker守护进程。\n\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n\ndocker info\n#验证\n```\n\n### 2.2.7 解压cri-docker\n\n```shell\n# 由于1.24以及更高版本不支持docker所以安装cri-docker\n# 下载cri-docker \n# https://github.com/Mirantis/cri-dockerd/releases/\n# wget  https://mirrors.chenby.cn/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd-0.3.10.amd64.tgz\n\n# 解压cri-docker\ntar xvf cri-dockerd-*.amd64.tgz \ncp -r cri-dockerd/  /usr/bin/\nchmod +x /usr/bin/cri-dockerd/cri-dockerd\n```\n\n### 2.2.8 写入启动cri-docker配置文件\n\n```shell\n# 写入启动配置文件\ncat >  /usr/lib/systemd/system/cri-docker.service <<EOF\n[Unit]\nDescription=CRI Interface for Docker Application Container Engine\nDocumentation=https://docs.mirantis.com\nAfter=network-online.target firewalld.service docker.service\nWants=network-online.target\nRequires=cri-docker.socket\n\n[Service]\nType=notify\nExecStart=/usr/bin/cri-dockerd/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n# [Unit]\n# - Description：该参数用于描述该单元的功能，这里描述的是CRI与Docker应用容器引擎的接口。\n# - Documentation：该参数指定了相关文档的网址，供用户参考。\n# - After：该参数指定了此单元应该在哪些其他单元之后启动，确保在网络在线、防火墙和Docker服务启动之后再启动此单元。\n# - Wants：该参数指定了此单元希望也启动的所有单元，此处是希望在网络在线之后启动。\n# - Requires：该参数指定了此单元需要依赖的单元，此处是cri-docker.socket单元。\n# \n# [Service]\n# - Type：该参数指定了服务的类型，这里是notify，表示当服务启动完成时向系统发送通知。\n# - ExecStart：该参数指定了将要运行的命令和参数，此处是执行/usr/bin/cri-dockerd/cri-dockerd命令，并指定了网络插件为cni和Pod基础设施容器的镜像为registry.aliyuncs.com/google_containers/pause:3.7。\n# - ExecReload：该参数指定在服务重载时运行的命令，此处是发送HUP信号给主进程。\n# - TimeoutSec：该参数指定了服务启动的超时时间，此处为0，表示无限制。\n# - RestartSec：该参数指定了自动重启服务的时间间隔，此处为2秒。\n# - Restart：该参数指定了在服务发生错误时自动重启，此处是始终重启。\n# - StartLimitBurst：该参数指定了在给定时间间隔内允许的启动失败次数，此处为3次。\n# - StartLimitInterval：该参数指定启动失败的时间间隔，此处为60秒。\n# - LimitNOFILE：该参数指定了允许打开文件的最大数量，此处为无限制。\n# - LimitNPROC：该参数指定了允许同时运行的最大进程数，此处为无限制。\n# - LimitCORE：该参数指定了允许生成的core文件的最大大小，此处为无限制。\n# - TasksMax：该参数指定了此服务的最大任务数，此处为无限制。\n# - Delegate：该参数指定了是否将控制权委托给指定服务，此处为是。\n# - KillMode：该参数指定了在终止服务时如何处理进程，此处是通过终止进程来终止服务。\n# \n# [Install]\n# - WantedBy：该参数指定了希望这个单元启动的多用户目标。在这里，这个单元希望在multi-user.target启动。\n```\n\n### 2.2.9 写入cri-docker的socket配置文件\n\n```shell\n# 写入socket配置文件\ncat > /usr/lib/systemd/system/cri-docker.socket <<EOF\n[Unit]\nDescription=CRI Docker Socket for the API\nPartOf=cri-docker.service\n\n[Socket]\nListenStream=%t/cri-dockerd.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n\n[Install]\nWantedBy=sockets.target\nEOF\n\n\n# 该配置文件是用于systemd的单元配置文件(unit file)，用于定义一个socket单元。\n# \n# [Unit]\n# - Description：表示该单元的描述信息。\n# - PartOf：表示该单元是cri-docker.service的一部分。\n# \n# [Socket]\n# - ListenStream：指定了该socket要监听的地址和端口，这里使用了%t占位符，表示根据单元的类型来决定路径。%t/cri-dockerd.sock表示将监听Unix域套接字cri-dockerd.sock。Unix域套接字用于在同一台主机上的进程之间通信。\n# - SocketMode：指定了socket文件的权限模式，此处为0660，即用户和用户组有读写权限，其他用户无权限。\n# - SocketUser：指定了socket文件的所有者，此处为root用户。\n# - SocketGroup：指定了socket文件的所属用户组，此处为docker用户组。\n# \n# [Install]\n# - WantedBy：部分定义了该单元的安装配置信息。WantedBy=sockets.target表示当sockets.target单元启动时，自动启动该socket单元。sockets.target是一个系统服务，用于管理所有的socket单元。\n```\n\n### 2.2.10 启动cri-docker\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now cri-docker.service\n# 启用并立即启动cri-docker.service单元。cri-docker.service是cri-docker守护进程的systemd服务单元。\n\nsystemctl restart cri-docker.service\n# 重启cri-docker.service单元，即重新启动cri-docker守护进程。\n\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n\n\n## 2.3.k8s与etcd下载及安装（仅在master01操作）\n\n### 2.3.1解压k8s安装包\n\n```shell\n# 下载安装包\n# wget https://mirrors.chenby.cn/https://github.com/etcd-io/etcd/releases/download/v3.5.12/etcd-v3.5.12-linux-amd64.tar.gz\n# wget https://dl.k8s.io/v1.29.2/kubernetes-server-linux-amd64.tar.gz\n\n# 解压k8s安装文件\ncd cby\ntar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}\n\n# 这是一个tar命令，用于解压指定的kubernetes-server-linux-amd64.tar.gz文件，并将其中的特定文件提取到/usr/local/bin目录下。\n# \n# 命令的解释如下：\n# - tar：用于处理tar压缩文件的命令。\n# - -xf：表示解压操作。\n# - kubernetes-server-linux-amd64.tar.gz：要解压的文件名。\n# - --strip-components=3：表示解压时忽略压缩文件中的前3级目录结构，提取文件时直接放到目标目录中。\n# - -C /usr/local/bin：指定提取文件的目标目录为/usr/local/bin。\n# - kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}：要解压和提取的文件名模式，用花括号括起来表示模式中的多个可能的文件名。\n# \n# 总的来说，这个命令的作用是将kubernetes-server-linux-amd64.tar.gz文件中的kubelet、kubectl、kube-apiserver、kube-controller-manager、kube-scheduler和kube-proxy六个文件提取到/usr/local/bin目录下，同时忽略文件路径中的前三级目录结构。\n\n\n# 解压etcd安装文件\ntar -xf etcd*.tar.gz && mv etcd-*/etcd /usr/local/bin/ && mv etcd-*/etcdctl /usr/local/bin/\n\n# 这是一个将文件解压并移动到特定目录的命令。这是一个用于 Linux 系统中的命令。\n# \n# - tar -xf etcd*.tar.gz：这个命令将解压以 etcd 开头并以.tar.gz 结尾的文件。`-xf` 是使用 `tar` 命令的选项，它表示解压文件并展开其中的内容。\n# - mv etcd-*/etcd /usr/local/bin/：这个命令将 etcd 文件移动到 /usr/local/bin 目录。`mv` 是移动命令，它将 etcd-*/etcd 路径下的 etcd 文件移动到了 /usr/local/bin 目录。\n# - mv etcd-*/etcdctl /usr/local/bin/：这个命令将 etcdctl 文件移动到 /usr/local/bin 目录，和上一条命令类似。\n# \n# 总结起来，以上命令将从名为 etcd*.tar.gz 的压缩文件中解压出 etcd 和 etcdctl 文件，并将它们移动到 /usr/local/bin 目录中。\n\n# 查看/usr/local/bin下内容\nls /usr/local/bin/\ncontainerd               crictl       etcdctl                  kube-proxy\ncontainerd-shim          critest      kube-apiserver           kube-scheduler\ncontainerd-shim-runc-v1  ctd-decoder  kube-controller-manager\ncontainerd-shim-runc-v2  ctr          kubectl\ncontainerd-stress        etcd         kubelet\n```\n\n### 2.3.2查看版本\n\n```shell\n[root@k8s-master01 ~]#  kubelet --version\nKubernetes v1.29.2\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.12\nAPI version: 3.5\n[root@k8s-master01 ~]# \n```\n\n### 2.3.3将组件发送至其他k8s节点\n\n```shell\nMaster='k8s-master02 k8s-master03'\nWork='k8s-node01 k8s-node02'\n\n# 拷贝master组件\nfor NODE in $Master; do echo $NODE; scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\n\n# 该命令是一个for循环，对于在$Master变量中的每个节点，执行以下操作：\n# \n# 1. 打印出节点的名称。\n# 2. 使用scp命令将/usr/local/bin/kubelet、kubectl、kube-apiserver、kube-controller-manager、kube-scheduler和kube-proxy文件复制到节点的/usr/local/bin/目录下。\n# 3. 使用scp命令将/usr/local/bin/etcd*文件复制到节点的/usr/local/bin/目录下。\n\n\n# 拷贝work组件\nfor NODE in $Work; do echo $NODE; scp /usr/local/bin/kube{let,-proxy} $NODE:/usr/local/bin/ ; done\n# 该命令是一个for循环，对于在$Master变量中的每个节点，执行以下操作：\n# \n# 1. 打印出节点的名称。\n# 2. 使用scp命令将/usr/local/bin/kubelet和kube-proxy文件复制到节点的/usr/local/bin/目录下。\n\n# 所有节点执行\nmkdir -p /opt/cni/bin\n```\n\n## 2.3创建证书相关文件\n\n```shell\n# 请查看Github仓库 或者进行获取已经打好的包\nhttps://github.com/cby-chen/Kubernetes/\nhttps://github.com/cby-chen/Kubernetes/tags\nhttps://github.com/cby-chen/Kubernetes/releases/download/v1.29.2/kubernetes-v1.29.2.tar\n```\n\n# 3.相关证书生成\n\n```shell\n# master01节点下载证书生成工具\n# wget \"https://mirrors.chenby.cn/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssl_1.6.4_linux_amd64\" -O /usr/local/bin/cfssl\n# wget \"https://mirrors.chenby.cn/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssljson_1.6.4_linux_amd64\" -O /usr/local/bin/cfssljson\n\n# 软件包内有\ncp cfssl_*_linux_amd64 /usr/local/bin/cfssl\ncp cfssljson_*_linux_amd64 /usr/local/bin/cfssljson\n\n# 添加执行权限\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n```\n\n## 3.1.生成etcd证书\n\n特别说明除外，以下操作在所有master节点操作\n\n### 3.1.1所有master节点创建证书存放目录\n\n```shell\nmkdir /etc/etcd/ssl -p\n```\n\n### 3.1.2master01节点生成etcd证书\n\n```shell\n# 写入生成证书所需的配置文件\ncat > ca-config.json << EOF \n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"876000h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"876000h\"\n      }\n    }\n  }\n}\nEOF\n# 这段配置文件是用于配置加密和认证签名的一些参数。\n# \n# 在这里，有两个部分：`signing`和`profiles`。\n# \n# `signing`包含了默认签名配置和配置文件。\n# 默认签名配置`default`指定了证书的过期时间为`876000h`。`876000h`表示证书有效期为100年。\n# \n# `profiles`部分定义了不同的证书配置文件。\n# 在这里，只有一个配置文件`kubernetes`。它包含了以下`usages`和过期时间`expiry`：\n# \n# 1. `signing`：用于对其他证书进行签名\n# 2. `key encipherment`：用于加密和解密传输数据\n# 3. `server auth`：用于服务器身份验证\n# 4. `client auth`：用于客户端身份验证\n# \n# 对于`kubernetes`配置文件，证书的过期时间也是`876000h`，即100年。\n\ncat > etcd-ca-csr.json  << EOF \n{\n  \"CN\": \"etcd\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"etcd\",\n      \"OU\": \"Etcd Security\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n# 这是一个用于生成证书签名请求（Certificate Signing Request，CSR）的JSON配置文件。JSON配置文件指定了生成证书签名请求所需的数据。\n# \n# - \"CN\": \"etcd\" 指定了希望生成的证书的CN字段（Common Name），即证书的主题，通常是该证书标识的实体的名称。\n# - \"key\": {} 指定了生成证书所使用的密钥的配置信息。\"algo\": \"rsa\" 指定了密钥的算法为RSA，\"size\": 2048 指定了密钥的长度为2048位。\n# - \"names\": [] 包含了生成证书时所需的实体信息。在这个例子中，只包含了一个实体，其相关信息如下：\n#   - \"C\": \"CN\" 指定了实体的国家/地区代码，这里是中国。\n#   - \"ST\": \"Beijing\" 指定了实体所在的省/州。\n#   - \"L\": \"Beijing\" 指定了实体所在的城市。\n#   - \"O\": \"etcd\" 指定了实体的组织名称。\n#   - \"OU\": \"Etcd Security\" 指定了实体所属的组织单位。\n# - \"ca\": {} 指定了生成证书时所需的CA（Certificate Authority）配置信息。\n#   - \"expiry\": \"876000h\" 指定了证书的有效期，这里是876000小时。\n# \n# 生成证书签名请求时，可以使用这个JSON配置文件作为输入，根据配置文件中的信息生成相应的CSR文件。然后，可以将CSR文件发送给CA进行签名，以获得有效的证书。\n\n# 生成etcd证书和etcd证书的key（如果你觉得以后可能会扩容，可以在ip那多写几个预留出来）\n# 若没有IPv6 可删除可保留 \n\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。etcd-ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/etcd/ssl/etcd-ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/etcd/ssl/etcd-ca路径下。\n\ncat > etcd-csr.json << EOF \n{\n  \"CN\": \"etcd\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"etcd\",\n      \"OU\": \"Etcd Security\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于生成一个证书签名请求（Certificate Signing Request，CSR）。\n# \n# 首先，\"CN\"字段指定了该证书的通用名称（Common Name），这里设为\"etcd\"。\n# \n# 接下来，\"key\"字段指定了密钥的算法（\"algo\"字段）和长度（\"size\"字段），此处使用的是RSA算法，密钥长度为2048位。\n# \n# 最后，\"names\"字段是一个数组，其中包含了一个名字对象，用于指定证书中的一些其他信息。这个名字对象包含了以下字段：\n# - \"C\"字段指定了国家代码（Country），这里设置为\"CN\"。\n# - \"ST\"字段指定了省份（State）或地区，这里设置为\"Beijing\"。\n# - \"L\"字段指定了城市（Locality），这里设置为\"Beijing\"。\n# - \"O\"字段指定了组织（Organization），这里设置为\"etcd\"。\n# - \"OU\"字段指定了组织单元（Organizational Unit），这里设置为\"Etcd Security\"。\n# \n# 这些字段将作为证书的一部分，用于标识和验证证书的使用范围和颁发者等信息。\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.31,192.168.1.32,192.168.1.33,fc00:43f4:1eea:1::10,fc00:43f4:1eea:1::20,fc00:43f4:1eea:1::30,::1 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n# 这是一条使用cfssl生成etcd证书的命令，下面是各个参数的解释：\n# \n# -ca=/etc/etcd/ssl/etcd-ca.pem：指定用于签名etcd证书的CA文件的路径。\n# -ca-key=/etc/etcd/ssl/etcd-ca-key.pem：指定用于签名etcd证书的CA私钥文件的路径。\n# -config=ca-config.json：指定CA配置文件的路径，该文件定义了证书的有效期、加密算法等设置。\n# -hostname=xxxx：指定要为etcd生成证书的主机名和IP地址列表。\n# -profile=kubernetes：指定使用的证书配置文件，该文件定义了证书的用途和扩展属性。\n# etcd-csr.json：指定etcd证书请求的JSON文件的路径，该文件包含了证书请求的详细信息。\n# | cfssljson -bare /etc/etcd/ssl/etcd：通过管道将cfssl命令的输出传递给cfssljson命令，并使用-bare参数指定输出文件的前缀路径，这里将生成etcd证书的.pem和-key.pem文件。\n# \n# 这条命令的作用是使用指定的CA证书和私钥，根据证书请求的JSON文件和配置文件生成etcd的证书文件。\n```\n\n### 3.1.3将证书复制到其他节点\n\n```shell\nMaster='k8s-master02 k8s-master03'\nfor NODE in $Master; do ssh $NODE \"mkdir -p /etc/etcd/ssl\"; for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do scp /etc/etcd/ssl/${FILE} $NODE:/etc/etcd/ssl/${FILE}; done; done\n\n# 这个命令是一个简单的for循环，在一个由`$Master`存储的主机列表中迭代执行。对于每个主机，它使用`ssh`命令登录到主机，并在远程主机上创建一个名为`/etc/etcd/ssl`的目录（如果不存在）。接下来，它使用`scp`将本地主机上`/etc/etcd/ssl`目录中的四个文件（`etcd-ca-key.pem`，`etcd-ca.pem`，`etcd-key.pem`和`etcd.pem`）复制到远程主机的`/etc/etcd/ssl`目录中。最终的结果是，远程主机上的`/etc/etcd/ssl`目录中包含与本地主机上相同的四个文件的副本。\n```\n\n## 3.2.生成k8s相关证书\n\n特别说明除外，以下操作在所有master节点操作\n\n### 3.2.1 所有k8s节点创建证书存放目录\n\n```shell\nmkdir -p /etc/kubernetes/pki\n```\n\n### 3.2.2 master01节点生成k8s证书\n\n```shell\n# 写入生成证书所需的配置文件\ncat > ca-csr.json   << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n# 这是一个用于生成 Kubernetes 相关证书的配置文件。该配置文件中包含以下信息：\n# \n# - CN：CommonName，即用于标识证书的通用名称。在此配置中，CN 设置为 \"kubernetes\"，表示该证书是用于 Kubernetes。\n# - key：用于生成证书的算法和大小。在此配置中，使用的算法是 RSA，大小是 2048 位。\n# - names：用于证书中的名称字段的详细信息。在此配置中，有以下字段信息：\n#   - C：Country，即国家。在此配置中，设置为 \"CN\"。\n#   - ST：State，即省/州。在此配置中，设置为 \"Beijing\"。\n#   - L：Locality，即城市。在此配置中，设置为 \"Beijing\"。\n#   - O：Organization，即组织。在此配置中，设置为 \"Kubernetes\"。\n#   - OU：Organization Unit，即组织单位。在此配置中，设置为 \"Kubernetes-manual\"。\n# - ca：用于证书签名的证书颁发机构（CA）的配置信息。在此配置中，设置了证书的有效期为 876000 小时。\n# \n# 这个配置文件可以用于生成 Kubernetes 相关的证书，以确保集群中的通信安全性。\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n\n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/kubernetes/pki/ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/kubernetes/pki/ca路径下。\n\ncat > apiserver-csr.json << EOF \n{\n  \"CN\": \"kube-apiserver\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n\n# 这是一个用于生成 Kubernetes 相关证书的配置文件。该配置文件中包含以下信息：\n# \n# - `CN` 字段指定了证书的通用名称 (Common Name)，这里设置为 \"kube-apiserver\"，表示该证书用于 Kubernetes API Server。\n# - `key` 字段指定了生成证书时所选用的加密算法和密钥长度。这里选用了 RSA 算法，密钥长度为 2048 位。\n# - `names` 字段包含了一组有关证书持有者信息的项。这里使用了以下信息：\n#   - `C` 表示国家代码 (Country)，这里设置为 \"CN\" 表示中国。\n#   - `ST` 表示州或省份 (State)，这里设置为 \"Beijing\" 表示北京市。\n#   - `L` 表示城市或地区 (Location)，这里设置为 \"Beijing\" 表示北京市。\n#   - `O` 表示组织名称 (Organization)，这里设置为 \"Kubernetes\" 表示 Kubernetes。\n#   - `OU` 表示组织单位 (Organizational Unit)，这里设置为 \"Kubernetes-manual\" 表示手动管理的 Kubernetes 集群。\n# \n# 这个配置文件可以用于生成 Kubernetes 相关的证书，以确保集群中的通信安全性。\n\n\n# 生成一个根证书 ，多写了一些IP作为预留IP，为将来添加node做准备\n# 10.96.0.1是service网段的第一个地址，需要计算，192.168.1.36为高可用vip地址\n# 若没有IPv6 可删除可保留 \n\ncfssl gencert   \\\n-ca=/etc/kubernetes/pki/ca.pem   \\\n-ca-key=/etc/kubernetes/pki/ca-key.pem   \\\n-config=ca-config.json   \\\n-hostname=10.96.0.1,192.168.1.36,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,x.oiox.cn,k.oiox.cn,l.oiox.cn,o.oiox.cn,192.168.1.31,192.168.1.32,192.168.1.33,192.168.1.34,192.168.1.35,192.168.1.36,192.168.0.37,192.168.0.38,192.168.0.39,192.168.1.70,fc00:43f4:1eea:1::10,fc00:43f4:1eea:1::20,fc00:43f4:1eea:1::30,fc00:43f4:1eea:1::40,fc00:43f4:1eea:1::50,fc00:43f4:1eea:1::60,fc00:43f4:1eea:1::70,fc00:43f4:1eea:1::80,fc00:43f4:1eea:1::90,fc00:43f4:1eea:1::100,::1   \\\n-profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n\n# 这个命令是使用cfssl工具生成Kubernetes API Server的证书。\n# \n# 命令的参数解释如下：\n# - `-ca=/etc/kubernetes/pki/ca.pem`：指定证书的颁发机构（CA）文件路径。\n# - `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定证书的颁发机构（CA）私钥文件路径。\n# - `-config=ca-config.json`：指定证书生成的配置文件路径，配置文件中包含了证书的有效期、加密算法等信息。\n# - `-hostname=10.96.0.1,192.168.1.36,127.0.0.1,fc00:43f4:1eea:1::10`：指定证书的主机名或IP地址列表。\n# - `-profile=kubernetes`：指定证书生成的配置文件中的配置文件名。\n# - `apiserver-csr.json`：API Server的证书签名请求配置文件路径。\n# - `| cfssljson -bare /etc/kubernetes/pki/apiserver`：通过管道将生成的证书输出到cfssljson工具，将其转换为PEM编码格式，并保存到 `/etc/kubernetes/pki/apiserver.pem` 和 `/etc/kubernetes/pki/apiserver-key.pem` 文件中。\n# \n# 最终，这个命令将会生成API Server的证书和私钥，并保存到指定的文件中。\n\n```\n\n### 3.2.3 生成apiserver聚合证书\n\n```shell\ncat > front-proxy-ca-csr.json  << EOF \n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  },\n  \"ca\": {\n    \"expiry\": \"876000h\"\n  }\n}\nEOF\n\n# 这个JSON文件表示了生成一个名为\"kubernetes\"的证书的配置信息。这个证书是用来进行Kubernetes集群的身份验证和安全通信。\n# \n# 配置信息包括以下几个部分：\n# \n# 1. \"CN\": \"kubernetes\"：这表示了证书的通用名称（Common Name），也就是证书所代表的实体的名称。在这里，证书的通用名称被设置为\"kubernetes\"，表示这个证书是用来代表Kubernetes集群。\n# \n# 2. \"key\"：这是用来生成证书的密钥相关的配置。在这里，配置使用了RSA算法，并且设置了密钥的大小为2048位。\n# \n# 3. \"ca\"：这个字段指定了证书的颁发机构（Certificate Authority）相关的配置。在这里，配置指定了证书的有效期为876000小时，即100年。这意味着该证书在100年内将被视为有效，过期后需要重新生成。\n# \n# 总之，这个JSON文件中的配置信息描述了如何生成一个用于Kubernetes集群的证书，包括证书的通用名称、密钥算法和大小以及证书的有效期。\n\ncfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n# 具体的解释如下：\n# \n# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。\n# \n# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。front-proxy-ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。\n# \n# | 符号表示将上一个命令的输出作为下一个命令的输入。\n# \n# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。/etc/kubernetes/pki/front-proxy-ca是指定生成的证书文件的路径和名称。\n# \n# 所以，这条命令的含义是使用cfssl工具根据配置文件ca-csr.json生成一个CA证书，并将证书文件保存在/etc/kubernetes/pki/front-proxy-ca路径下。\n\ncat > front-proxy-client-csr.json  << EOF \n{\n  \"CN\": \"front-proxy-client\",\n  \"key\": {\n     \"algo\": \"rsa\",\n     \"size\": 2048\n  }\n}\nEOF\n\n# 这是一个JSON格式的配置文件，用于描述一个名为\"front-proxy-client\"的配置。配置包括两个字段：CN和key。\n# \n# - CN（Common Name）字段表示证书的通用名称，这里为\"front-proxy-client\"。\n# - key字段描述了密钥的算法和大小。\"algo\"表示使用RSA算法，\"size\"表示密钥大小为2048位。\n# \n# 该配置文件用于生成一个SSL证书，用于在前端代理客户端进行认证和数据传输的加密。这个证书中的通用名称是\"front-proxy-client\"，使用RSA算法生成，密钥大小为2048位。\n\ncfssl gencert  \\\n-ca=/etc/kubernetes/pki/front-proxy-ca.pem   \\\n-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   \\\n-config=ca-config.json   \\\n-profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n\n# 这个命令使用cfssl工具生成一个用于Kubernetes的front-proxy-client证书。\n# \n# 主要参数解释如下：\n# - `-ca=/etc/kubernetes/pki/front-proxy-ca.pem`: 指定用于签署证书的根证书文件路径。\n# - `-ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem`: 指定用于签署证书的根证书的私钥文件路径。\n# - `-config=ca-config.json`: 指定用于配置证书签署的配置文件路径。该配置文件描述了证书生成的一些规则，如加密算法和有效期等。\n# - `-profile=kubernetes`: 指定生成证书时使用的配置文件中定义的profile，其中包含了一些默认的参数。\n# - `front-proxy-client-csr.json`: 指定用于生成证书的CSR文件路径，该文件包含了证书请求的相关信息。\n# - `| cfssljson -bare /etc/kubernetes/pki/front-proxy-client`: 通过管道将生成的证书输出到cfssljson工具进行解析，并通过`-bare`参数将证书和私钥分别保存到指定路径。\n# \n# 这个命令的作用是根据提供的CSR文件和配置信息，使用指定的根证书和私钥生成一个前端代理客户端的证书，并将证书和私钥分别保存到`/etc/kubernetes/pki/front-proxy-client.pem`和`/etc/kubernetes/pki/front-proxy-client-key.pem`文件中。\n```\n\n### 3.2.4 生成controller-manage的证书\n\n在《5.高可用配置》选择使用那种高可用方案\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```shell\ncat > manager-csr.json << EOF \n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这是一个用于生成密钥对（公钥和私钥）的JSON配置文件。下面是针对该文件中每个字段的详细解释：\n# \n# - \"CN\": 值为\"system:kube-controller-manager\"，代表通用名称（Common Name），是此密钥对的主题（subject）。\n# - \"key\": 这个字段用来定义密钥算法和大小。\n#   - \"algo\": 值为\"rsa\"，表示使用RSA算法。\n#   - \"size\": 值为2048，表示生成的密钥大小为2048位。\n# - \"names\": 这个字段用来定义密钥对的各个名称字段。\n#   - \"C\": 值为\"CN\"，表示国家（Country）名称是\"CN\"（中国）。\n#   - \"ST\": 值为\"Beijing\"，表示省/州（State/Province）名称是\"Beijing\"（北京）。\n#   - \"L\": 值为\"Beijing\"，表示城市（Locality）名称是\"Beijing\"（北京）。\n#   - \"O\": 值为\"system:kube-controller-manager\"，表示组织（Organization）名称是\"system:kube-controller-manager\"。\n#   - \"OU\": 值为\"Kubernetes-manual\"，表示组织单位（Organizational Unit）名称是\"Kubernetes-manual\"。\n# \n# 这个JSON配置文件基本上是告诉生成密钥对的工具，生成一个带有特定名称和属性的密钥对。\n\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n# 这是一个命令行操作，使用cfssl工具生成证书。\n# \n# 1. `cfssl gencert` 是cfssl工具的命令，用于生成证书。\n# 2. `-ca` 指定根证书的路径和文件名，这里是`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key` 指定根证书的私钥的路径和文件名，这里是`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config` 指定配置文件的路径和文件名，这里是`ca-config.json`。\n# 5. `-profile` 指定证书使用的配置文件中的配置模板，这里是`kubernetes`。\n# 6. `manager-csr.json` 是证书签发请求的配置文件，用于生成证书签发请求。\n# 7. `|` 管道操作符，将前一条命令的输出作为后一条命令的输入。\n# 8. `cfssljson -bare` 是 cfssl 工具的命令，作用是将证书签发请求的输出转换为PKCS＃1、PKCS＃8和x509 PEM文件。\n# 9. `/etc/kubernetes/pki/controller-manager` 是转换后的 PEM 文件的存储位置和文件名。\n# \n# 这个命令的作用是根据根证书和私钥、配置文件以及证书签发请求的配置文件，生成经过签发的控制器管理器证书和私钥，并将转换后的 PEM 文件保存到指定的位置。\n\n\n# 设置一个集群项\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# kubectl config set-cluster命令用于配置集群信息。\n# --certificate-authority选项指定了集群的证书颁发机构（CA）的路径，这个CA会验证kube-apiserver提供的证书是否合法。\n# --embed-certs选项用于将证书嵌入到生成的kubeconfig文件中，这样就不需要在kubeconfig文件中单独指定证书文件路径。\n# --server选项指定了kube-apiserver的地址，这里使用的是127.0.0.1:8443，表示使用本地主机上的kube-apiserver，默认端口为8443。\n# --kubeconfig选项指定了生成的kubeconfig文件的路径和名称，这里指定为/etc/kubernetes/controller-manager.kubeconfig。\n# 综上所述，kubectl config set-cluster命令的作用是在kubeconfig文件中设置集群信息，包括证书颁发机构、证书、kube-apiserver地址等。\n\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 这个命令用于配置 Kubernetes 控制器管理器的上下文信息。下面是各个参数的详细解释：\n# 1. `kubectl config set-context system:kube-controller-manager@kubernetes`: 设置上下文的名称为 `system:kube-controller-manager@kubernetes`，这是一个标识符，用于唯一标识该上下文。\n# 2. `--cluster=kubernetes`: 指定集群的名称为 `kubernetes`，这是一个现有集群的标识符，表示要管理的 Kubernetes 集群。\n# 3. `--user=system:kube-controller-manager`: 指定使用的用户身份为 `system:kube-controller-manager`。这是一个特殊的用户身份，具有控制 Kubernetes 控制器管理器的权限。\n# 4. `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`: 指定 kubeconfig 文件的路径为 `/etc/kubernetes/controller-manager.kubeconfig`。kubeconfig 文件是一个用于管理 Kubernetes 配置的文件，包含了集群、用户和上下文的相关信息。\n# 通过运行这个命令，可以将这些配置信息保存到 `/etc/kubernetes/controller-manager.kubeconfig` 文件中，以便在后续的操作中使用。\n\n\n\n  # 设置一个用户项\n  kubectl config set-credentials system:kube-controller-manager \\\n       --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n       --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n       --embed-certs=true \\\n       --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 上述命令是用于设置 Kubernetes 的 controller-manager 组件的客户端凭据。下面是每个参数的详细解释：\n# \n# - `kubectl config`: 是使用 kubectl 命令行工具的配置子命令。\n# - `set-credentials`: 是定义一个新的用户凭据配置的子命令。\n# - `system:kube-controller-manager`: 是设置用户凭据的名称，`system:` 是 Kubernetes API Server 内置的身份验证器使用的用户标识符前缀，它表示是一个系统用户，在本例中是 kube-controller-manager 组件使用的身份。\n# - `--client-certificate=/etc/kubernetes/pki/controller-manager.pem`: 指定 controller-manager.pem 客户端证书的路径。\n# - `--client-key=/etc/kubernetes/pki/controller-manager-key.pem`: 指定 controller-manager-key.pem 客户端私钥的路径。\n# - `--embed-certs=true`: 表示将证书和私钥直接嵌入到生成的 kubeconfig 文件中，而不是通过引用外部文件。\n# - `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`: 指定生成的 kubeconfig 文件的路径和文件名，即 controller-manager.kubeconfig。\n# \n# 通过运行上述命令，将根据提供的证书和私钥信息，为 kube-controller-manager 创建一个 kubeconfig 文件，以便后续使用该文件进行身份验证和访问 Kubernetes API。\n\n\n# 设置默认环境\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n# 这个命令是用来指定kubectl使用指定的上下文环境来执行操作。上下文环境是kubectl用来确定要连接到哪个Kubernetes集群以及使用哪个身份验证信息的配置。\n# \n# 在这个命令中，`kubectl config use-context`是用来设置当前上下文环境的命令。 `system:kube-controller-manager@kubernetes`是指定的上下文名称，它告诉kubectl要使用的Kubernetes集群和身份验证信息。 \n# `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`是用来指定使用的kubeconfig文件的路径。kubeconfig文件是存储集群连接和身份验证信息的配置文件。\n# 通过执行这个命令，kubectl将使用指定的上下文来执行后续的操作，包括部署和管理Kubernetes资源。\n```\n\n### 3.2.5 生成kube-scheduler的证书\n\n```shell\ncat > scheduler-csr.json << EOF \n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这个命令是用来创建一个叫做scheduler-csr.json的文件，并将其中的内容赋值给该文件。\n# \n# 文件内容是一个JSON格式的文本，包含了一个描述证书请求的结构。\n# \n# 具体内容如下：\n# \n# - \"CN\": \"system:kube-scheduler\"：Common Name字段，表示该证书的名称为system:kube-scheduler。\n# - \"key\": {\"algo\": \"rsa\", \"size\": 2048}：key字段指定生成证书时使用的加密算法是RSA，并且密钥的长度为2048位。\n# - \"names\": [...]：names字段定义了证书中的另外一些标识信息。\n# - \"C\": \"CN\"：Country字段，表示国家/地区为中国。\n# - \"ST\": \"Beijing\"：State字段，表示省/市为北京。\n# - \"L\": \"Beijing\"：Locality字段，表示所在城市为北京。\n# - \"O\": \"system:kube-scheduler\"：Organization字段，表示组织为system:kube-scheduler。\n# - \"OU\": \"Kubernetes-manual\"：Organizational Unit字段，表示组织单元为Kubernetes-manual。\n# \n# 而EOF是一个占位符，用于标记开始和结束的位置。在开始的EOF之后到结束的EOF之间的内容将会被写入到scheduler-csr.json文件中。\n# \n# 总体来说，这个命令用于生成一个描述kube-scheduler证书请求的JSON文件。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n# 上述命令是使用cfssl工具生成Kubernetes Scheduler的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `scheduler-csr.json`：指定Scheduler的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`scheduler-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/scheduler`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/scheduler.pem`（包含了证书）、`/etc/kubernetes/pki/scheduler-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n\n\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://127.0.0.1:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/scheduler.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 这段命令是用于设置 kube-scheduler 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials system:kube-scheduler`：设置 `system:kube-scheduler` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/scheduler.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 kube-scheduler 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/scheduler-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 kube-scheduler 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 该命令用于设置一个名为\"system:kube-scheduler@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=system:kube-scheduler: 指定用户的名称为\"system:kube-scheduler\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权kube-scheduler组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/scheduler.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/scheduler.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `system:kube-scheduler@kubernetes`是一个上下文名称。它指定了使用`kube-scheduler`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/scheduler.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n### 3.2.6 生成admin的证书配置\n\n```shell\ncat > admin-csr.json << EOF \n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于创建和配置一个名为\"admin\"的Kubernetes凭证。\n# \n# 这个凭证包含以下字段：\n# \n# - \"CN\": \"admin\": 这是凭证的通用名称，表示这是一个管理员凭证。\n# - \"key\": 这是一个包含证书密钥相关信息的对象。\n#   - \"algo\": \"rsa\"：这是使用的加密算法类型，这里是RSA加密算法。\n#   - \"size\": 2048：这是密钥的大小，这里是2048位。\n# - \"names\": 这是一个包含证书名称信息的数组。\n#   - \"C\": \"CN\"：这是证书的国家/地区字段，这里是中国。\n#   - \"ST\": \"Beijing\"：这是证书的省/州字段，这里是北京。\n#   - \"L\": \"Beijing\"：这是证书的城市字段，这里是北京。\n#   - \"O\": \"system:masters\"：这是证书的组织字段，这里是system:masters，表示系统的管理员组。\n#   - \"OU\": \"Kubernetes-manual\"：这是证书的部门字段，这里是Kubernetes-manual。\n# \n# 通过这个配置文件创建的凭证将具有管理员权限，并且可以用于管理Kubernetes集群。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n# 上述命令是使用cfssl工具生成Kubernetes admin的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `admin-csr.json`：指定admin的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`admin-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/admin`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/admin.pem`（包含了证书）、`/etc/kubernetes/pki/admin-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/scheduler.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials kubernetes-admin  \\\n  --client-certificate=/etc/kubernetes/pki/admin.pem     \\\n  --client-key=/etc/kubernetes/pki/admin-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 这段命令是用于设置 kubernetes-admin 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials kubernetes-admin`：设置 `kubernetes-admin` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/admin.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 admin 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/admin-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 admin 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\n\nkubectl config set-context kubernetes-admin@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kubernetes-admin     \\\n  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 该命令用于设置一个名为\"kubernetes-admin@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=kubernetes-admin: 指定用户的名称为\"kubernetes-admin\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权admin组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/admin.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/admin.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\n\nkubectl config use-context kubernetes-admin@kubernetes  --kubeconfig=/etc/kubernetes/admin.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `kubernetes-admin@kubernetes`是一个上下文名称。它指定了使用`kubernetes-admin`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/admin.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n### 3.2.7 创建kube-proxy证书\n\n在《5.高可用配置》选择使用那种高可用方案\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\n```shell\ncat > kube-proxy-csr.json  << EOF \n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"Beijing\",\n      \"L\": \"Beijing\",\n      \"O\": \"system:kube-proxy\",\n      \"OU\": \"Kubernetes-manual\"\n    }\n  ]\n}\nEOF\n# 这段代码是一个JSON格式的配置文件，用于创建和配置一个名为\"kube-proxy-csr\"的Kubernetes凭证。\n# \n# 这个凭证包含以下字段：\n# \n# - \"CN\": \"system:kube-proxy\": 这是凭证的通用名称，表示这是一个管理员凭证。\n# - \"key\": 这是一个包含证书密钥相关信息的对象。\n#   - \"algo\": \"rsa\"：这是使用的加密算法类型，这里是RSA加密算法。\n#   - \"size\": 2048：这是密钥的大小，这里是2048位。\n# - \"names\": 这是一个包含证书名称信息的数组。\n#   - \"C\": \"CN\"：这是证书的国家/地区字段，这里是中国。\n#   - \"ST\": \"Beijing\"：这是证书的省/州字段，这里是北京。\n#   - \"L\": \"Beijing\"：这是证书的城市字段，这里是北京。\n#   - \"O\": \"system:kube-proxy\"：这是证书的组织字段，这里是system:kube-proxy。\n#   - \"OU\": \"Kubernetes-manual\"：这是证书的部门字段，这里是Kubernetes-manual。\n# \n# 通过这个配置文件创建的凭证将具有管理员权限，并且可以用于管理Kubernetes集群。\n\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n# 上述命令是使用cfssl工具生成Kubernetes admin的证书。\n# \n# 具体解释如下：\n# \n# 1. `cfssl gencert`：使用cfssl工具生成证书。\n# 2. `-ca=/etc/kubernetes/pki/ca.pem`：指定根证书文件的路径。在这里，是指定根证书的路径为`/etc/kubernetes/pki/ca.pem`。\n# 3. `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定根证书私钥文件的路径。在这里，是指定根证书私钥的路径为`/etc/kubernetes/pki/ca-key.pem`。\n# 4. `-config=ca-config.json`：指定证书配置文件的路径。在这里，是指定证书配置文件的路径为`ca-config.json`。\n# 5. `-profile=kubernetes`：指定证书的配置文件中的一个配置文件模板。在这里，是指定配置文件中的`kubernetes`配置模板。\n# 6. `kube-proxy-csr.json`：指定admin的证书签名请求文件（CSR）的路径。在这里，是指定请求文件的路径为`kube-proxy-csr.json`。\n# 7. `|`（管道符号）：将前一个命令的输出作为下一个命令的输入。\n# 8. `cfssljson`：将cfssl工具生成的证书签名请求(CSR)进行解析。\n# 9. `-bare /etc/kubernetes/pki/kube-proxy`：指定输出路径和前缀。在这里，是将解析的证书签名请求生成以下文件：`/etc/kubernetes/pki/kube-proxy.pem`（包含了证书）、`/etc/kubernetes/pki/kube-proxy-key.pem`（包含了私钥）。\n# \n# 总结来说，这个命令的目的是根据根证书、根证书私钥、证书配置文件、CSR文件等生成Kubernetes Scheduler的证书和私钥文件。\n\n   \n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\nkubectl config set-cluster kubernetes     \\\n  --certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n  --embed-certs=true     \\\n  --server=https://127.0.0.1:8443     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 该命令用于配置一个名为\"kubernetes\"的集群，并将其应用到/etc/kubernetes/kube-proxy.kubeconfig文件中。\n# \n# 该命令的解释如下：\n# - `kubectl config set-cluster kubernetes`: 设置一个集群并命名为\"kubernetes\"。\n# - `--certificate-authority=/etc/kubernetes/pki/ca.pem`: 指定集群使用的证书授权机构的路径。\n# - `--embed-certs=true`: 该标志指示将证书嵌入到生成的kubeconfig文件中。\n# - `--server=https://127.0.0.1:8443`: 指定集群的 API server 位置。\n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`: 指定要保存 kubeconfig 文件的路径和名称。\n\nkubectl config set-credentials kube-proxy  \\\n  --client-certificate=/etc/kubernetes/pki/kube-proxy.pem     \\\n  --client-key=/etc/kubernetes/pki/kube-proxy-key.pem     \\\n  --embed-certs=true     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 这段命令是用于设置 kube-proxy 组件的身份验证凭据，并生成相应的 kubeconfig 文件。\n# \n# 解释每个选项的含义如下：\n# - `kubectl config set-credentials kube-proxy`：设置 `kube-proxy` 用户的身份验证凭据。\n# - `--client-certificate=/etc/kubernetes/pki/kube-proxy.pem`：指定一个客户端证书文件，用于基于证书的身份验证。在这种情况下，指定了 kube-proxy 组件的证书文件路径。\n# - `--client-key=/etc/kubernetes/pki/kube-proxy-key.pem`：指定与客户端证书相对应的客户端私钥文件。\n# - `--embed-certs=true`：将客户端证书和私钥嵌入到生成的 kubeconfig 文件中。\n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`：指定生成的 kubeconfig 文件的路径和名称。\n# \n# 该命令的目的是为 kube-proxy 组件生成一个 kubeconfig 文件，以便进行身份验证和访问集群资源。kubeconfig 文件是一个包含了连接到 Kubernetes 集群所需的所有配置信息的文件，包括服务器地址、证书和秘钥等。\n\nkubectl config set-context kube-proxy@kubernetes    \\\n  --cluster=kubernetes     \\\n  --user=kube-proxy     \\\n  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 该命令用于设置一个名为\"kube-proxy@kubernetes\"的上下文，具体配置如下：\n# \n# 1. --cluster=kubernetes: 指定集群的名称为\"kubernetes\"，这个集群是在当前的kubeconfig文件中已经定义好的。\n# 2. --user=kube-proxy: 指定用户的名称为\"kube-proxy\"，这个用户也是在当前的kubeconfig文件中已经定义好的。这个用户用于认证和授权kube-proxy组件访问Kubernetes集群的权限。\n# 3. --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig: 指定kubeconfig文件的路径为\"/etc/kubernetes/kube-proxy.kubeconfig\"，这个文件将被用来保存上下文的配置信息。\n# \n# 这个命令的作用是将上述的配置信息保存到指定的kubeconfig文件中，以便后续使用该文件进行认证和授权访问Kubernetes集群。\n\nkubectl config use-context kube-proxy@kubernetes  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n# 上述命令是使用`kubectl`命令来配置Kubernetes集群中的调度器组件。\n# \n# `kubectl config use-context`命令用于切换`kubectl`当前使用的上下文。上下文是Kubernetes集群、用户和命名空间的组合，用于确定`kubectl`的连接目标。下面解释这个命令的不同部分：\n# \n# - `kube-proxy@kubernetes`是一个上下文名称。它指定了使用`kube-proxy`用户和`kubernetes`命名空间的系统级别上下文。系统级别上下文用于操作Kubernetes核心组件。\n# \n# - `--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig`用于指定Kubernetes配置文件的路径。Kubernetes配置文件包含连接到Kubernetes集群所需的身份验证和连接信息。\n# \n# 通过运行以上命令，`kubectl`将使用指定的上下文和配置文件，以便在以后的命令中能正确地与Kubernetes集群中的调度器组件进行交互。\n```\n\n\n\n### 3.2.8 创建ServiceAccount Key ——secret\n\n```shell\nopenssl genrsa -out /etc/kubernetes/pki/sa.key 2048\nopenssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n\n# 这两个命令是使用OpenSSL工具生成RSA密钥对。\n# \n# 命令1：openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n# 该命令用于生成私钥文件。具体解释如下：\n# - openssl：openssl命令行工具。\n# - genrsa：生成RSA密钥对。\n# - -out /etc/kubernetes/pki/sa.key：指定输出私钥文件的路径和文件名。\n# - 2048：指定密钥长度为2048位。\n# \n# 命令2：openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n# 该命令用于从私钥中导出公钥。具体解释如下：\n# - openssl：openssl命令行工具。\n# - rsa：与私钥相关的RSA操作。\n# - -in /etc/kubernetes/pki/sa.key：指定输入私钥文件的路径和文件名。\n# - -pubout：指定输出公钥。\n# - -out /etc/kubernetes/pki/sa.pub：指定输出公钥文件的路径和文件名。\n# \n# 总结：通过以上两个命令，我们可以使用OpenSSL工具生成一个RSA密钥对，并将私钥保存在/etc/kubernetes/pki/sa.key文件中，将公钥保存在/etc/kubernetes/pki/sa.pub文件中。\n```\n\n### 3.2.9 将证书发送到其他master节点\n\n```shell\n#其他节点创建目录\n# mkdir  /etc/kubernetes/pki/ -p\n\nfor NODE in k8s-master02 k8s-master03; do  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do  scp /etc/kubernetes/pki/${FILE} $NODE:/etc/kubernetes/pki/${FILE}; done;  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do  scp /etc/kubernetes/${FILE} $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n### 3.2.10 查看证书\n\n```shell\nls /etc/kubernetes/pki/\nadmin.csr          controller-manager.csr      kube-proxy.csr\nadmin-key.pem      controller-manager-key.pem  kube-proxy-key.pem\nadmin.pem          controller-manager.pem      kube-proxy.pem\napiserver.csr      front-proxy-ca.csr          sa.key\napiserver-key.pem  front-proxy-ca-key.pem      sa.pub\napiserver.pem      front-proxy-ca.pem          scheduler.csr\nca.csr             front-proxy-client.csr      scheduler-key.pem\nca-key.pem         front-proxy-client-key.pem  scheduler.pem\nca.pem             front-proxy-client.pem\n\n# 一共26个就对了\nls /etc/kubernetes/pki/ |wc -l\n26\n```\n\n# 4.k8s系统组件配置\n\n## 4.1.etcd配置\n\n```shell\n这个配置文件是用于 etcd 集群的配置，其中包含了一些重要的参数和选项：\n\n- `name`：指定了当前节点的名称，用于集群中区分不同的节点。\n- `data-dir`：指定了 etcd 数据的存储目录。\n- `wal-dir`：指定了 etcd 数据写入磁盘的目录。\n- `snapshot-count`：指定了触发快照的事务数量。\n- `heartbeat-interval`：指定了 etcd 集群中节点之间的心跳间隔。\n- `election-timeout`：指定了选举超时时间。\n- `quota-backend-bytes`：指定了存储的限额，0 表示无限制。\n- `listen-peer-urls`：指定了节点之间通信的 URL，使用 HTTPS 协议。\n- `listen-client-urls`：指定了客户端访问 etcd 集群的 URL，同时提供了本地访问的 URL。\n- `max-snapshots`：指定了快照保留的数量。\n- `max-wals`：指定了日志保留的数量。\n- `initial-advertise-peer-urls`：指定了节点之间通信的初始 URL。\n- `advertise-client-urls`：指定了客户端访问 etcd 集群的初始 URL。\n- `discovery`：定义了 etcd 集群发现相关的选项。\n- `initial-cluster`：指定了 etcd 集群的初始成员。\n- `initial-cluster-token`：指定了集群的 token。\n- `initial-cluster-state`：指定了集群的初始状态。\n- `strict-reconfig-check`：指定了严格的重新配置检查选项。\n- `enable-v2`：启用了 v2 API。\n- `enable-pprof`：启用了性能分析。\n- `proxy`：设置了代理模式。\n- `client-transport-security`：客户端的传输安全配置。\n- `peer-transport-security`：节点之间的传输安全配置。\n- `debug`：是否启用调试模式。\n- `log-package-levels`：日志的输出级别。\n- `log-outputs`：指定了日志的输出类型。\n- `force-new-cluster`：是否强制创建一个新的集群。\n\n这些参数和选项可以根据实际需求进行调整和配置。\n```\n\n### 4.1.1master01配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master01'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.31:2380'\nlisten-client-urls: 'https://192.168.1.31:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.31:2380'\nadvertise-client-urls: 'https://192.168.1.31:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n### 4.1.2master02配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master02'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.32:2380'\nlisten-client-urls: 'https://192.168.1.32:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.32:2380'\nadvertise-client-urls: 'https://192.168.1.32:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n### 4.1.3master03配置\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\ncat > /etc/etcd/etcd.config.yml << EOF \nname: 'k8s-master03'\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.33:2380'\nlisten-client-urls: 'https://192.168.1.33:2379,http://127.0.0.1:2379'\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.33:2380'\nadvertise-client-urls: 'https://192.168.1.33:2379'\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.31:2380,k8s-master02=https://192.168.1.32:2380,k8s-master03=https://192.168.1.33:2380'\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\nEOF\n```\n\n## 4.2.创建service（所有master节点操作）\n\n### 4.2.1创建etcd.service并启动\n\n```shell\ncat > /usr/lib/systemd/system/etcd.service << EOF\n\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n\nEOF\n# 这是一个系统服务配置文件，用于启动和管理Etcd服务。\n# \n# [Unit] 部分包含了服务的一些基本信息，它定义了服务的描述和文档链接，并指定了服务应在网络连接之后启动。\n# \n# [Service] 部分定义了服务的具体配置。在这里，服务的类型被设置为notify，意味着当服务成功启动时，它将通知系统。ExecStart 指定了启动服务时要执行的命令，这里是运行 /usr/local/bin/etcd 命令并传递一个配置文件 /etc/etcd/etcd.config.yml。Restart 设置为 on-failure，意味着当服务失败时将自动重启，并且在10秒后进行重启。LimitNOFILE 指定了服务的最大文件打开数。\n# \n# [Install] 部分定义了服务的安装配置。WantedBy 指定了服务应该被启动的目标，这里是 multi-user.target，表示在系统进入多用户模式时启动。Alias 定义了一个别名，可以通过etcd3.service来引用这个服务。\n# \n# 这个配置文件描述了如何启动和管理Etcd服务，并将其安装到系统中。通过这个配置文件，可以确保Etcd服务在系统启动后自动启动，并在出现问题时进行重启。\n```\n\n### 4.2.2创建etcd证书目录\n\n```shell\nmkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now etcd.service\n# 启用并立即启动etcd.service单元。etcd.service是etcd守护进程的systemd服务单元。\n\nsystemctl restart etcd.service\n# 重启etcd.service单元，即重新启动etcd守护进程。\n\nsystemctl status etcd.service\n# etcd.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 4.2.3查看etcd状态\n\n```shell\n# 如果要用IPv6那么把IPv4地址修改为IPv6即可\nexport ETCDCTL_API=3\netcdctl --endpoints=\"192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n| 192.168.1.33:2379 |  8065f2e59c8d68c |  3.5.12 |   20 kB |     false |      false |         4 |         14 |                 14 |        |\n| 192.168.1.32:2379 | b7b7ad6bf4db3f28 |  3.5.12 |   20 kB |      true |      false |         4 |         14 |                 14 |        |\n| 192.168.1.31:2379 | bf047bcfe3b9bf27 |  3.5.12 |   20 kB |     false |      false |         4 |         14 |                 14 |        |\n+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\n\n# 这个命令是使用etcdctl工具，用于查看指定etcd集群的健康状态。下面是每个参数的详细解释：\n# \n# - `--endpoints`：指定要连接的etcd集群节点的地址和端口。在这个例子中，指定了3个节点的地址和端口，分别是`192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379`。\n# - `--cacert`：指定用于验证etcd服务器证书的CA证书的路径。在这个例子中，指定了CA证书的路径为`/etc/kubernetes/pki/etcd/etcd-ca.pem`。CA证书用于验证etcd服务器证书的有效性。\n# - `--cert`：指定用于与etcd服务器进行通信的客户端证书的路径。在这个例子中，指定了客户端证书的路径为`/etc/kubernetes/pki/etcd/etcd.pem`。客户端证书用于在与etcd服务器建立安全通信时进行身份验证。\n# - `--key`：指定与客户端证书配对的私钥的路径。在这个例子中，指定了私钥的路径为`/etc/kubernetes/pki/etcd/etcd-key.pem`。私钥用于对通信进行加密解密和签名验证。\n# - `endpoint status`：子命令，用于检查etcd集群节点的健康状态。\n# - `--write-out`：指定输出的格式。在这个例子中，指定以表格形式输出。\n# \n# 通过执行这个命令，可以获取到etcd集群节点的健康状态，并以表格形式展示。\n```\n\n# 5.高可用配置（在Master服务器上操作）\n\n**注意* 5.1.1 和5.1.2 二选一即可**\n\n选择使用那种高可用方案，同时可以俩种都选用，实现内外兼顾的效果，比如：\n5.1 的 NGINX方案实现集群内的高可用\n5.2 的 haproxy、keepalived 方案实现集群外访问\n\n在《3.2.生成k8s相关证书》\n\n若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:9443`\n\n## 5.1 NGINX高可用方案 \n### 5.1.1 进行编译\n```shell\n# 安装编译环境\nyum install gcc -y\n\n# 下载解压nginx二进制文件\n# wget http://nginx.org/download/nginx-1.25.3.tar.gz\ntar xvf nginx-*.tar.gz\ncd nginx-*\n\n# 进行编译\n./configure --with-stream --without-http --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module\nmake && make install \n\n# 拷贝编译好的nginx\nnode='k8s-master02 k8s-master03 k8s-node01 k8s-node02'\nfor NODE in $node; do scp -r /usr/local/nginx/ $NODE:/usr/local/nginx/; done\n\n# 这是一系列命令行指令，用于编译和安装软件。\n# \n# 1. `./configure` 是用于配置软件的命令。在这个例子中，配置的软件是一个Web服务器，指定了一些选项来启用流模块，并禁用了HTTP、uwsgi、scgi和fastcgi模块。\n# 2. `--with-stream` 指定启用流模块。流模块通常用于代理TCP和UDP流量。\n# 3. `--without-http` 指定禁用HTTP模块。这意味着编译的软件将没有HTTP服务器功能。\n# 4. `--without-http_uwsgi_module` 指定禁用uwsgi模块。uwsgi是一种Web服务器和应用服务器之间的通信协议。\n# 5. `--without-http_scgi_module` 指定禁用scgi模块。scgi是一种用于将Web服务器请求传递到应用服务器的协议。\n# 6. `--without-http_fastcgi_module` 指定禁用fastcgi模块。fastcgi是一种用于在Web服务器和应用服务器之间交换数据的协议。\n# 7. `make` 是用于编译软件的命令。该命令将根据之前的配置生成可执行文件。\n# 8. `make install` 用于安装软件。该命令将生成的可执行文件和其他必要文件复制到系统的适当位置，以便可以使用该软件。\n# \n# 总之，这个命令序列用于编译一个配置了特定选项的Web服务器，并将其安装到系统中。\n```\n\n### 5.1.2 写入启动配置\n在所有主机上执行\n```shell\n# 写入nginx配置文件\ncat > /usr/local/nginx/conf/kube-nginx.conf <<EOF\nworker_processes 1;\nevents {\n    worker_connections  1024;\n}\nstream {\n    upstream backend {\n    \tleast_conn;\n        hash $remote_addr consistent;\n        server 192.168.1.31:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.32:6443        max_fails=3 fail_timeout=30s;\n        server 192.168.1.33:6443        max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 127.0.0.1:8443;\n        proxy_connect_timeout 1s;\n        proxy_pass backend;\n    }\n}\nEOF\n# 这段配置是一个nginx的stream模块的配置，用于代理TCP和UDP流量。\n# \n# 首先，`worker_processes 1;`表示启动一个worker进程用于处理流量。\n# 接下来，`events { worker_connections 1024; }`表示每个worker进程可以同时处理最多1024个连接。\n# 在stream块里面，定义了一个名为`backend`的upstream，用于负载均衡和故障转移。\n# `least_conn`表示使用最少连接算法进行负载均衡。\n# `hash $remote_addr consistent`表示用客户端的IP地址进行哈希分配请求，保持相同IP的请求始终访问同一台服务器。\n# `server`指令用于定义后端的服务器，每个服务器都有一个IP地址和端口号，以及一些可选的参数。\n# `max_fails=3`表示当一个服务器连续失败3次时将其标记为不可用。\n# `fail_timeout=30s`表示如果一个服务器被标记为不可用，nginx将在30秒后重新尝试。\n# 在server块内部，定义了一个监听地址为127.0.0.1:8443的服务器。\n# `proxy_connect_timeout 1s`表示与后端服务器建立连接的超时时间为1秒。\n# `proxy_pass backend`表示将流量代理到名为backend的上游服务器组。\n# \n# 总结起来，这段配置将流量代理到一个包含3个后端服务器的上游服务器组中，使用最少连接算法进行负载均衡，并根据客户端的IP地址进行哈希分配请求。如果一个服务器连续失败3次，则将其标记为不可用，并在30秒后重新尝试。\n\n\n# 写入启动配置文件\ncat > /etc/systemd/system/kube-nginx.service <<EOF\n[Unit]\nDescription=kube-apiserver nginx proxy\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=forking\nExecStartPre=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -t\nExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx\nExecReload=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -s reload\nPrivateTmp=true\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nLimitNOFILE=65536\n \n[Install]\nWantedBy=multi-user.target\nEOF\n# 这是一个用于kube-apiserver的NGINX代理的systemd单位文件。\n# \n# [Unit]部分包含了单位的描述和依赖关系。它指定了在network.target和network-online.target之后启动，并且需要network-online.target。\n# \n# [Service]部分定义了如何运行该服务。Type指定了服务进程的类型（forking表示主进程会派生一个子进程）。ExecStartPre指定了在服务启动之前需要运行的命令，用于检查NGINX配置文件的语法是否正确。ExecStart指定了启动服务所需的命令。ExecReload指定了在重新加载配置文件时运行的命令。PrivateTmp设置为true表示将为服务创建一个私有的临时文件系统。Restart和RestartSec用于设置服务的自动重启机制。StartLimitInterval设置为0表示无需等待，可以立即重启服务。LimitNOFILE指定了服务的文件描述符的限制。\n# \n# [Install]部分指定了在哪些target下该单位应该被启用。\n# \n# 综上所述，此单位文件用于启动和管理kube-apiserver的NGINX代理服务。它通过NGINX来反向代理和负载均衡kube-apiserver的请求。该服务会在系统启动时自动启动，并具有自动重启的机制。\n\n\n# 设置开机自启\n\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now kube-nginx.service\n# 启用并立即启动kube-nginx.service单元。kube-nginx.service是kube-nginx守护进程的systemd服务单元。\nsystemctl restart kube-nginx.service\n# 重启kube-nginx.service单元，即重新启动kube-nginx守护进程。\nsystemctl status kube-nginx.service\n# kube-nginx.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 5.2 keepalived和haproxy 高可用方案 \n\n### 5.2.1安装keepalived和haproxy服务\n\n```shell\nsystemctl disable --now firewalld\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nyum -y install keepalived haproxy\n```\n\n### 5.2.2修改haproxy配置文件（配置文件一样）\n\n```shell\n# cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n\ncat >/etc/haproxy/haproxy.cfg<<\"EOF\"\nglobal\n maxconn 2000\n ulimit-n 16384\n log 127.0.0.1 local0 err\n stats timeout 30s\n\ndefaults\n log global\n mode http\n option httplog\n timeout connect 5000\n timeout client 50000\n timeout server 50000\n timeout http-request 15s\n timeout http-keep-alive 15s\n\n\nfrontend monitor-in\n bind *:33305\n mode http\n option httplog\n monitor-uri /monitor\n\nfrontend k8s-master\n bind 0.0.0.0:9443\n bind 127.0.0.1:9443\n mode tcp\n option tcplog\n tcp-request inspect-delay 5s\n default_backend k8s-master\n\n\nbackend k8s-master\n mode tcp\n option tcplog\n option tcp-check\n balance roundrobin\n default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n server  k8s-master01  192.168.1.31:6443 check\n server  k8s-master02  192.168.1.32:6443 check\n server  k8s-master03  192.168.1.33:6443 check\nEOF\n```\n参数\n\n```shell\n这段配置代码是指定了一个HAProxy负载均衡器的配置。下面对各部分进行详细解释：\n1. global:\n   - maxconn 2000: 设置每个进程的最大连接数为2000。\n   - ulimit-n 16384: 设置每个进程的最大文件描述符数为16384。\n   - log 127.0.0.1 local0 err: 指定日志的输出地址为本地主机的127.0.0.1，并且只记录错误级别的日志。\n   - stats timeout 30s: 设置查看负载均衡器统计信息的超时时间为30秒。\n\n2. defaults:\n   - log global: 使默认日志与global部分相同。\n   - mode http: 设定负载均衡器的工作模式为HTTP模式。\n   - option httplog: 使负载均衡器记录HTTP协议的日志。\n   - timeout connect 5000: 设置与后端服务器建立连接的超时时间为5秒。\n   - timeout client 50000: 设置与客户端的连接超时时间为50秒。\n   - timeout server 50000: 设置与后端服务器连接的超时时间为50秒。\n   - timeout http-request 15s: 设置处理HTTP请求的超时时间为15秒。\n   - timeout http-keep-alive 15s: 设置保持HTTP连接的超时时间为15秒。\n\n3. frontend monitor-in:\n   - bind *:33305: 监听所有IP地址的33305端口。\n   - mode http: 设定frontend的工作模式为HTTP模式。\n   - option httplog: 记录HTTP协议的日志。\n   - monitor-uri /monitor: 设置监控URI为/monitor。\n\n4. frontend k8s-master:\n   - bind 0.0.0.0:9443: 监听所有IP地址的9443端口。\n   - bind 127.0.0.1:9443: 监听本地主机的9443端口。\n   - mode tcp: 设定frontend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - tcp-request inspect-delay 5s: 设置在接收到请求后延迟5秒进行检查。\n   - default_backend k8s-master: 设置默认的后端服务器组为k8s-master。\n\n5. backend k8s-master:\n   - mode tcp: 设定backend的工作模式为TCP模式。\n   - option tcplog: 记录TCP协议的日志。\n   - option tcp-check: 启用TCP检查功能。\n   - balance roundrobin: 使用轮询算法进行负载均衡。\n   - default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100: 设置默认的服务器参数。\n   - server k8s-master01 192.168.1.31:6443 check: 增加一个名为k8s-master01的服务器，IP地址为192.168.1.31，端口号为6443，并对其进行健康检查。\n   - server k8s-master02 192.168.1.32:6443 check: 增加一个名为k8s-master02的服务器，IP地址为192.168.1.32，端口号为6443，并对其进行健康检查。\n   - server k8s-master03 192.168.1.33:6443 check: 增加一个名为k8s-master03的服务器，IP地址为192.168.1.33，端口号为6443，并对其进行健康检查。\n\n以上就是这段配置代码的详细解释。它主要定义了全局配置、默认配置、前端监听和后端服务器组的相关参数和设置。通过这些配置，可以实现负载均衡和监控功能。\n```\n\n### 5.2.3Master01配置keepalived master节点\n\n```shell\n#cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n}\nvrrp_instance VI_1 {\n    state MASTER\n    # 注意网卡名\n    interface eth0 \n    mcast_src_ip 192.168.1.31\n    virtual_router_id 51\n    priority 100\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n\n### 5.2.4Master02配置keepalived backup节点\n\n```shell\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.32\n    virtual_router_id 51\n    priority 80\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n\n### 5.2.5Master03配置keepalived backup节点\n\n```shell\n# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\n\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n\nglobal_defs {\n    router_id LVS_DEVEL\n}\nvrrp_script chk_apiserver {\n    script \"/etc/keepalived/check_apiserver.sh\"\n    interval 5 \n    weight -5\n    fall 2\n    rise 1\n\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    # 注意网卡名\n    interface eth0\n    mcast_src_ip 192.168.1.33\n    virtual_router_id 51\n    priority 50\n    nopreempt\n    advert_int 2\n    authentication {\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    }\n    virtual_ipaddress {\n        192.168.1.36\n    }\n    track_script {\n      chk_apiserver \n} }\n\nEOF\n```\n参数\n\n```shell\n这是一个用于配置keepalived的配置文件。下面是对每个部分的详细解释：\n\n- `global_defs`部分定义了全局参数。\n- `router_id`参数指定了当前路由器的标识，这里设置为\"LVS_DEVEL\"。\n\n- `vrrp_script`部分定义了一个VRRP脚本。`chk_apiserver`是脚本的名称，\n    - `script`参数指定了脚本的路径。该脚本每5秒执行一次，返回值为0表示服务正常，返回值为1表示服务异常。\n    - `weight`参数指定了根据脚本返回的值来调整优先级，这里设置为-5。\n    - `fall`参数指定了失败阈值，当连续2次脚本返回值为1时认为服务异常。\n    - `rise`参数指定了恢复阈值，当连续1次脚本返回值为0时认为服务恢复正常。\n\n- `vrrp_instance`部分定义了一个VRRP实例。`VI_1`是实例的名称。\n    - `state`参数指定了当前实例的状态，这里设置为MASTER表示当前实例是主节点。\n    - `interface`参数指定了要监听的网卡，这里设置为eth0。\n    - `mcast_src_ip`参数指定了VRRP报文的源IP地址，这里设置为192.168.1.31。\n    - `virtual_router_id`参数指定了虚拟路由器的ID，这里设置为51。\n    - `priority`参数指定了实例的优先级，优先级越高（数值越大）越有可能被选为主节点。\n    - `nopreempt`参数指定了当主节点失效后不要抢占身份，即不要自动切换为主节点。\n    - `advert_int`参数指定了发送广播的间隔时间，这里设置为2秒。\n    - `authentication`部分指定了认证参数\n    \t- `auth_type`参数指定了认证类型，这里设置为PASS表示使用密码认证，\n    \t- `auth_pass`参数指定了认证密码，这里设置为K8SHA_KA_AUTH。\n    - `virtual_ipaddress`部分指定了虚拟IP地址，这里设置为192.168.1.36。\n    - `track_script`部分指定了要跟踪的脚本，这里跟踪了chk_apiserver脚本。\n```\n\n\n### 5.2.6健康检查脚本配置（lb主机）\n\n```shell\ncat >  /etc/keepalived/check_apiserver.sh << EOF\n#!/bin/bash\n\nerr=0\nfor k in \\$(seq 1 3)\ndo\n    check_code=\\$(pgrep haproxy)\n    if [[ \\$check_code == \"\" ]]; then\n        err=\\$(expr \\$err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ \\$err != \"0\" ]]; then\n    echo \"systemctl stop keepalived\"\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\nEOF\n\n# 给脚本授权\n\nchmod +x /etc/keepalived/check_apiserver.sh\n\n# 这段脚本是一个简单的bash脚本，主要用来检查是否有名为haproxy的进程正在运行。\n# \n# 脚本的主要逻辑如下：\n# 1. 首先设置一个变量err为0，用来记录错误次数。\n# 2. 使用一个循环，在循环内部执行以下操作：\n#    a. 使用pgrep命令检查是否有名为haproxy的进程在运行。如果不存在该进程，将err加1，并暂停1秒钟，然后继续下一次循环。\n#    b. 如果存在haproxy进程，将err重置为0，并跳出循环。\n# 3. 检查err的值，如果不为0，表示检查失败，输出一条错误信息并执行“systemctl stop keepalived”命令停止keepalived进程，并退出脚本返回1。\n# 4. 如果err的值为0，表示检查成功，退出脚本返回0。\n# \n# 该脚本的主要作用是检查是否存在运行中的haproxy进程，如果无法检测到haproxy进程，将停止keepalived进程并返回错误状态。如果haproxy进程存在，则返回成功状态。这个脚本可能是作为一个健康检查脚本的一部分，在确保haproxy服务可用的情况下，才继续运行其他操作。\n```\n\n### 5.2.7启动服务\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\nsystemctl enable --now haproxy.service\n# 启用并立即启动haproxy.service单元。haproxy.service是haproxy守护进程的systemd服务单元。\nsystemctl enable --now keepalived.service\n# 启用并立即启动keepalived.service单元。keepalived.service是keepalived守护进程的systemd服务单元。\nsystemctl status haproxy.service\n# haproxy.service单元的当前状态，包括运行状态、是否启用等信息。\nsystemctl status keepalived.service\n# keepalived.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 5.2.8测试高可用\n\n```shell\n# 能ping同\n[root@k8s-node02 ~]# ping 192.168.1.36\n\n# 能telnet访问\n[root@k8s-node02 ~]# telnet 192.168.1.36 9443\n\n# 关闭主节点，看vip是否漂移到备节点\n```\n\n# 6.k8s组件配置\n\n所有k8s节点创建以下目录\n\n```shell\nmkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes\n```\n\n## 6.1.创建apiserver（所有master节点）\n\n### 6.1.1master01节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.31 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n### 6.1.2master02节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service << EOF\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.32 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n\n### 6.1.3master03节点配置\n```shell\ncat > /usr/lib/systemd/system/kube-apiserver.service  << EOF\n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n      --v=2  \\\\\n      --allow-privileged=true  \\\\\n      --bind-address=0.0.0.0  \\\\\n      --secure-port=6443  \\\\\n      --advertise-address=192.168.1.33 \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112  \\\\\n      --service-node-port-range=30000-32767  \\\\\n      --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\\\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\\\n      --authorization-mode=Node,RBAC  \\\\\n      --enable-bootstrap-token-auth=true  \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\\\n      --requestheader-allowed-names=aggregator  \\\\\n      --requestheader-group-headers=X-Remote-Group  \\\\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\\\n      --requestheader-username-headers=X-Remote-User \\\\\n      --enable-aggregator-routing=true\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n该配置文件是用于定义Kubernetes API Server的systemd服务的配置。systemd是一个用于启动和管理Linux系统服务的守护进程。\n\n[Unit]\n- Description: 服务的描述信息，用于显示在日志和系统管理工具中。\n- Documentation: 提供关于服务的文档链接。\n- After: 规定服务依赖于哪些其他服务或单元。在这个例子中，API Server服务在网络目标启动之后启动。\n\n[Service]\n- ExecStart: 定义服务的命令行参数和命令。这里指定了API Server的启动命令，包括各种参数选项。\n- Restart: 指定当服务退出时应该如何重新启动。在这个例子中，服务在失败时将被重新启动。\n- RestartSec: 指定两次重新启动之间的等待时间。\n- LimitNOFILE: 指定进程可以打开的文件描述符的最大数量。\n\n[Install]\n- WantedBy: 指定服务应该安装到哪个系统目标。在这个例子中，服务将被安装到multi-user.target目标，以便在多用户模式下启动。\n\n上述配置文件中定义的kube-apiserver服务将以指定的参数运行，这些参数包括：\n\n- `--v=2` 指定日志级别为2，打印详细的API Server日志。\n- `--allow-privileged=true` 允许特权容器运行。\n- `--bind-address=0.0.0.0` 绑定API Server监听的IP地址。\n- `--secure-port=6443` 指定API Server监听的安全端口。\n- `--advertise-address=192.168.1.31` 广告API Server的地址。\n- `--service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112` 指定服务CIDR范围。\n- `--service-node-port-range=30000-32767` 指定NodePort的范围。\n- `--etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379` 指定etcd服务器的地址。\n- `--etcd-cafile` 指定etcd服务器的CA证书。\n- `--etcd-certfile` 指定etcd服务器的证书。\n- `--etcd-keyfile` 指定etcd服务器的私钥。\n- `--client-ca-file` 指定客户端CA证书。\n- `--tls-cert-file` 指定服务的证书。\n- `--tls-private-key-file` 指定服务的私钥。\n- `--kubelet-client-certificate` 和 `--kubelet-client-key` 指定与kubelet通信的客户端证书和私钥。\n- `--service-account-key-file` 指定服务账户公钥文件。\n- `--service-account-signing-key-file` 指定服务账户签名密钥文件。\n- `--service-account-issuer` 指定服务账户的发布者。\n- `--kubelet-preferred-address-types` 指定kubelet通信时的首选地址类型。\n- `--enable-admission-plugins` 启用一系列准入插件。\n- `--authorization-mode` 指定授权模式。\n- `--enable-bootstrap-token-auth` 启用引导令牌认证。\n- `--requestheader-client-ca-file` 指定请求头中的客户端CA证书。\n- `--proxy-client-cert-file` 和 `--proxy-client-key-file` 指定代理客户端的证书和私钥。\n- `--requestheader-allowed-names` 指定请求头中允许的名字。\n- `--requestheader-group-headers` 指定请求头中的组头。\n- `--requestheader-extra-headers-prefix` 指定请求头中的额外头前缀。\n- `--requestheader-username-headers` 指定请求头中的用户名头。\n- `--enable-aggregator-routing` 启用聚合路由。\n\n整个配置文件为Kubernetes API Server提供了必要的参数，以便正确地启动和运行。\n```\n\n\n### 6.1.4启动apiserver（所有master节点）\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-apiserver.service\n# 启用并立即启动kube-apiserver.service单元。kube-apiserver.service是kube-apiserver守护进程的systemd服务单元。\n\nsystemctl restart kube-apiserver.service\n# 重启kube-apiserver.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-apiserver.service\n# kube-apiserver.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 6.2.配置kube-controller-manager service\n```shell\n# 所有master节点配置，且配置相同\n# 172.16.0.0/12为pod网段，按需求设置你自己的网段\n\ncat > /usr/lib/systemd/system/kube-controller-manager.service << EOF\n\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\\\n      --leader-elect=true \\\\\n      --use-service-account-credentials=true \\\\\n      --node-monitor-grace-period=40s \\\\\n      --node-monitor-period=5s \\\\\n      --controllers=*,bootstrapsigner,tokencleaner \\\\\n      --allocate-node-cidrs=true \\\\\n      --service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112 \\\\\n      --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n      --node-cidr-mask-size-ipv4=24 \\\\\n      --node-cidr-mask-size-ipv6=120 \\\\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n这是一个用于启动 Kubernetes 控制器管理器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 控制器管理器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-controller-manager，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--root-ca-file：根证书文件的路径，用于验证其他组件的证书。\n--cluster-signing-cert-file：用于签名集群证书的证书文件路径。\n--cluster-signing-key-file：用于签名集群证书的私钥文件路径。\n--service-account-private-key-file：用于签名服务账户令牌的私钥文件路径。\n--kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个控制器管理器作为 leader 在运行。\n--use-service-account-credentials=true：使用服务账户的凭据进行认证和授权。\n--node-monitor-grace-period=40s：节点监控的优雅退出时间，节点长时间不响应时会触发节点驱逐。\n--node-monitor-period=5s：节点监控的检测周期，用于检测节点是否正常运行。\n--controllers：指定要运行的控制器类型，在这里使用了通配符 *，表示运行所有的控制器，同时还包括了 bootstrapsigner 和 tokencleaner 控制器。\n--allocate-node-cidrs=true：为节点分配 CIDR 子网，用于分配 Pod 网络地址。\n--service-cluster-ip-range：定义 Service 的 IP 范围，这里设置为 10.96.0.0/12 和 fd00::/108。\n--cluster-cidr：定义集群的 CIDR 范围，这里设置为 172.16.0.0/12 和 fc00::/48。\n--node-cidr-mask-size-ipv4：分配给每个节点的 IPv4 子网掩码大小，默认是 24。\n--node-cidr-mask-size-ipv6：分配给每个节点的 IPv6 子网掩码大小，默认是 120。\n--requestheader-client-ca-file：设置请求头中客户端 CA 的证书文件路径，用于认证请求头中的 CA 证书。\n\n这个服务单元文件描述了 Kubernetes 控制器管理器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 控制器管理器组件。\n```\n### 6.2.1启动kube-controller-manager，并查看状态\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-controller-manager.service\n# 启用并立即启动kube-controller-manager.service单元。kube-controller-manager.service是kube-controller-manager守护进程的systemd服务单元。\n\nsystemctl restart kube-controller-manager.service\n# 重启kube-controller-manager.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-controller-manager.service\n# kube-controller-manager.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n## 6.3.配置kube-scheduler service\n\n### 6.3.1所有master节点配置，且配置相同\n\n```shell\ncat > /usr/lib/systemd/system/kube-scheduler.service << EOF\n\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\\\n      --v=2 \\\\\n      --bind-address=0.0.0.0 \\\\\n      --leader-elect=true \\\\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n```\n参数\n```shell\n这是一个用于启动 Kubernetes 调度器的 systemd 服务单元文件。下面是对每个部分的详细解释：\n\n[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。\nDescription：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 调度器。\nDocumentation：可选项，提供了关于该服务单元的文档链接。\nAfter：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。\n\n[Service]：定义了服务的运行参数和行为。\nExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-scheduler，并通过后续的行继续传递了一系列的参数设置。\nRestart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。\nRestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。\n\n[Install]：定义了如何安装和启用服务单元。\nWantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。\n\n在 ExecStart 中传递的参数说明如下：\n\n--v=2：设置日志的详细级别为 2。\n--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。\n--leader-elect=true：启用 Leader 选举机制，确保只有一个调度器作为 leader 在运行。\n--kubeconfig=/etc/kubernetes/scheduler.kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。\n\n这个服务单元文件描述了 Kubernetes 调度器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 调度器组件。\n```\n### 6.3.2启动并查看服务状态\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-scheduler.service\n# 启用并立即启动kube-scheduler.service单元。kube-scheduler.service是kube-scheduler守护进程的systemd服务单元。\n\nsystemctl restart kube-scheduler.service\n# 重启kube-scheduler.service单元，即重新启动etcd守护进程。\n\nsystemctl status kube-scheduler.service\n# kube-scheduler.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 7.TLS Bootstrapping配置\n\n## 7.1在master01上配置\n\n```shell\n# 在《5.高可用配置》选择使用那种高可用方案\n# 若使用 haproxy、keepalived 那么为 `--server=https://192.168.1.36:8443`\n# 若使用 nginx方案，那么为 `--server=https://127.0.0.1:8443`\n\ncd bootstrap\n\nkubectl config set-cluster kubernetes     \\\n--certificate-authority=/etc/kubernetes/pki/ca.pem     \\\n--embed-certs=true     --server=https://127.0.0.1:8443     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置 Kubernetes 集群配置的命令示例。下面是对每个选项的详细解释：\n# \n# config set-cluster kubernetes：指定要设置的集群名称为 \"kubernetes\"，表示要修改名为 \"kubernetes\" 的集群配置。\n# --certificate-authority=/etc/kubernetes/pki/ca.pem：指定证书颁发机构（CA）的证书文件路径，用于验证服务器证书的有效性。\n# --embed-certs=true：将证书文件嵌入到生成的 kubeconfig 文件中。这样可以避免在 kubeconfig 文件中引用外部证书文件。\n# --server=https://127.0.0.1:8443：指定 Kubernetes API 服务器的地址和端口，这里使用的是 https 协议和本地地址（127.0.0.1），端口号为 8443。你可以根据实际环境修改该参数。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"kubernetes\" 的集群配置，并提供 CA 证书、API 服务器地址和端口，并将这些配置信息嵌入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-credentials tls-bootstrap-token-user     \\\n--token=c8ad9c.2e4d610cf3e7426e \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置凭证信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-credentials tls-bootstrap-token-user：指定要设置的凭证名称为 \"tls-bootstrap-token-user\"，表示要修改名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --token=c8ad9c.2e4d610cf3e7426e：指定用户的身份验证令牌（token）。在这个示例中，令牌是 c8ad9c.2e4d610cf3e7426e。你可以根据实际情况修改该令牌。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user\" 的用户凭证，并将令牌信息加入到 bootstrap-kubelet.kubeconfig 文件中。这个 kubeconfig 文件可以用于认证和授权 kubelet 组件与 Kubernetes API 服务器之间的通信。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config set-context tls-bootstrap-token-user@kubernetes     \\\n--cluster=kubernetes     \\\n--user=tls-bootstrap-token-user     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置上下文信息的命令示例。下面是对每个选项的详细解释：\n# \n# config set-context tls-bootstrap-token-user@kubernetes：指定要设置的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要修改名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文配置。\n# --cluster=kubernetes：指定上下文关联的集群名称为 \"kubernetes\"，表示使用名为 \"kubernetes\" 的集群配置。\n# --user=tls-bootstrap-token-user：指定上下文关联的用户凭证名称为 \"tls-bootstrap-token-user\"，表示使用名为 \"tls-bootstrap-token-user\" 的用户凭证配置。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以设置名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文，并将其关联到名为 \"kubernetes\" 的集群配置和名为 \"tls-bootstrap-token-user\" 的用户凭证配置。这样，bootstrap-kubelet.kubeconfig 文件就包含了完整的上下文信息，可以用于指定与 Kubernetes 集群建立连接时要使用的集群和凭证。请确保路径和文件名与实际环境中的配置相匹配。\n\nkubectl config use-context tls-bootstrap-token-user@kubernetes     \\\n--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n# 这是一个使用 kubectl 命令设置当前上下文的命令示例。下面是对每个选项的详细解释：\n# \n# config use-context tls-bootstrap-token-user@kubernetes：指定要使用的上下文名称为 \"tls-bootstrap-token-user@kubernetes\"，表示要将当前上下文切换为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。\n# --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定 kubeconfig 文件的路径和名称，这里是 /etc/kubernetes/bootstrap-kubelet.kubeconfig。\n# 通过执行此命令，你可以将当前上下文设置为名为 \"tls-bootstrap-token-user@kubernetes\" 的上下文。这样，当你执行其他 kubectl 命令时，它们将使用该上下文与 Kubernetes 集群进行交互。请确保路径和文件名与实际环境中的配置相匹配。\n\n\n# token的位置在bootstrap.secret.yaml，如果修改的话到这个文件修改\nmkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n```\n\n## 7.2查看集群状态，没问题的话继续后续操作\n\n```shell\n# 1.28 版本只能查看到一个etcd 属于正常现象\n# export ETCDCTL_API=3\n# etcdctl --endpoints=\"192.168.1.33:2379,192.168.1.32:2379,192.168.1.31:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n\nkubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\nscheduler            Healthy   ok        \ncontroller-manager   Healthy   ok        \netcd-0               Healthy   ok \n\n# 切记执行，别忘记！！！\nkubectl create -f bootstrap.secret.yaml\n```\n\n# 8.node节点配置\n\n## 8.1.在master01上将证书复制到node节点\n\n```shell\ncd /etc/kubernetes/\n \nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $NODE mkdir -p /etc/kubernetes/pki; for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig kube-proxy.kubeconfig; do scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/${FILE}; done; done\n```\n\n## 8.2.kubelet配置\n\n**注意 ： 8.2.1 和 8.2.2 需要和 上方 2.1 和 2.2 对应起来**\n\n### 8.2.1当使用docker作为Runtime\n\n```shell\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node= \n\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。下面是对每个节（[Unit]、[Service]、[Install]）的详细解释：\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数。这里使用的是 /usr/local/bin/kubelet 命令，并传递了一系列参数来配置 Kubelet 的运行。这些参数包括：\n# --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig：指定了用于引导 kubelet 的 kubeconfig 文件的路径和名称。\n# --kubeconfig=/etc/kubernetes/kubelet.kubeconfig：指定了 kubelet 的 kubeconfig 文件的路径和名称。\n# --config=/etc/kubernetes/kubelet-conf.yml：指定了 kubelet 的配置文件的路径和名称。\n# --container-runtime-endpoint=unix:///run/cri-dockerd.sock：指定了容器运行时接口的端点地址，这里使用的是 Docker 运行时（cri-dockerd）的 UNIX 套接字。\n# --node-labels=node.kubernetes.io/node=：指定了节点的标签。这里的示例只给节点添加了一个简单的标签 node.kubernetes.io/node=。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，指定相关的配置文件和凭证文件，以及定义节点的标签。请确认路径和文件名与你的实际环境中的配置相匹配。\n\n\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/cri-dockerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=   \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 8.2.2当使用Containerd作为Runtime （推荐）\n\n```shell\nmkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n\n# 所有k8s节点配置kubelet service\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 这是一个表示 Kubernetes Kubelet 服务的 systemd 单位文件示例。与之前相比，添加了 After 和 Requires 字段来指定依赖关系。\n# \n# [Unit]\n# \n# Description=Kubernetes Kubelet：指定了此单位文件对应的服务描述信息为 \"Kubernetes Kubelet\"。\n# Documentation=...：指定了对该服务的文档链接。\n# - After: 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动。\n# - Wants: 说明该服务想要的其他服务，这里是网络在线服务。\n# - Requires: 说明该服务需要的其他服务，这里是docker.socket和containerd.service。\n# [Service]\n# \n# ExecStart=/usr/local/bin/kubelet ...：指定了启动 Kubelet 服务的命令和参数，与之前的示例相同。\n# --container-runtime-endpoint=unix:///run/containerd/containerd.sock：修改了容器运行时接口的端点地址，将其更改为使用 containerd 运行时（通过 UNIX 套接字）。\n# [Install]\n# \n# WantedBy=multi-user.target：指定了在 multi-user.target 被启动时，该服务应该被启用。\n# 通过这个单位文件，你可以配置 Kubelet 服务的启动参数，并指定了它依赖的 containerd 服务。确保路径和文件名与你实际环境中的配置相匹配。\n\n\n\n# IPv6示例\n# 若不使用IPv6那么忽略此项即可\n# 下方 --node-ip 更换为每个节点的IP即可\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n    --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig  \\\\\n    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\\n    --config=/etc/kubernetes/kubelet-conf.yml \\\\\n    --container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\\\\n    --node-labels=node.kubernetes.io/node=  \\\\\n    --node-ip=192.168.1.31,2408:822a:245:8c01::fab\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n\n### 8.2.3所有k8s节点创建kubelet的配置文件\n\n```shell\ncat > /etc/kubernetes/kubelet-conf.yml <<EOF\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\nEOF\n\n# 这是一个Kubelet的配置文件，用于配置Kubelet的各项参数。\n# \n# - apiVersion: kubelet.config.k8s.io/v1beta1：指定了配置文件的API版本为kubelet.config.k8s.io/v1beta1。\n# - kind: KubeletConfiguration：指定了配置类别为KubeletConfiguration。\n# - address: 0.0.0.0：指定了Kubelet监听的地址为0.0.0.0。\n# - port: 10250：指定了Kubelet监听的端口为10250。\n# - readOnlyPort: 10255：指定了只读端口为10255，用于提供只读的状态信息。\n# - authentication：指定了认证相关的配置信息。\n#   - anonymous.enabled: false：禁用了匿名认证。\n#   - webhook.enabled: true：启用了Webhook认证。\n#   - x509.clientCAFile: /etc/kubernetes/pki/ca.pem：指定了X509证书的客户端CA文件路径。\n# - authorization：指定了授权相关的配置信息。\n#   - mode: Webhook：指定了授权模式为Webhook。\n#   - webhook.cacheAuthorizedTTL: 5m0s：指定了授权缓存时间段为5分钟。\n#   - webhook.cacheUnauthorizedTTL: 30s：指定了未授权缓存时间段为30秒。\n# - cgroupDriver: systemd：指定了Cgroup驱动为systemd。\n# - cgroupsPerQOS: true：启用了每个QoS类别一个Cgroup的设置。\n# - clusterDNS: 指定了集群的DNS服务器地址列表。\n#   - 10.96.0.10：指定了DNS服务器地址为10.96.0.10。\n# - clusterDomain: cluster.local：指定了集群的域名后缀为cluster.local。\n# - containerLogMaxFiles: 5：指定了容器日志文件保留的最大数量为5个。\n# - containerLogMaxSize: 10Mi：指定了容器日志文件的最大大小为10Mi。\n# - contentType: application/vnd.kubernetes.protobuf：指定了内容类型为protobuf。\n# - cpuCFSQuota: true：启用了CPU CFS Quota。\n# - cpuManagerPolicy: none：禁用了CPU Manager。\n# - cpuManagerReconcilePeriod: 10s：指定了CPU管理器的调整周期为10秒。\n# - enableControllerAttachDetach: true：启用了控制器的挂载和拆卸。\n# - enableDebuggingHandlers: true：启用了调试处理程序。\n# - enforceNodeAllocatable: 指定了强制节点可分配资源的列表。\n#   - pods：强制节点可分配pods资源。\n# - eventBurst: 10：指定了事件突发的最大数量为10。\n# - eventRecordQPS: 5：指定了事件记录的最大请求量为5。\n# - evictionHard: 指定了驱逐硬性限制参数的配置信息。\n#   - imagefs.available: 15%：指定了镜像文件系统可用空间的限制为15%。\n#   - memory.available: 100Mi：指定了可用内存的限制为100Mi。\n#   - nodefs.available: 10%：指定了节点文件系统可用空间的限制为10%。\n#   - nodefs.inodesFree: 5%：指定了节点文件系统可用inode的限制为5%。\n# - evictionPressureTransitionPeriod: 5m0s：指定了驱逐压力转换的时间段为5分钟。\n# - failSwapOn: true：指定了在发生OOM时禁用交换分区。\n# - fileCheckFrequency: 20s：指定了文件检查频率为20秒。\n# - hairpinMode: promiscuous-bridge：设置了Hairpin Mode为\"promiscuous-bridge\"。\n# - healthzBindAddress: 127.0.0.1：指定了健康检查的绑定地址为127.0.0.1。\n# - healthzPort: 10248：指定了健康检查的端口为10248。\n# - httpCheckFrequency: 20s：指定了HTTP检查的频率为20秒。\n# - imageGCHighThresholdPercent: 85：指定了镜像垃圾回收的上阈值为85%。\n# - imageGCLowThresholdPercent: 80：指定了镜像垃圾回收的下阈值为80%。\n# - imageMinimumGCAge: 2m0s：指定了镜像垃圾回收的最小时间为2分钟。\n# - iptablesDropBit: 15：指定了iptables的Drop Bit为15。\n# - iptablesMasqueradeBit: 14：指定了iptables的Masquerade Bit为14。\n# - kubeAPIBurst: 10：指定了KubeAPI的突发请求数量为10个。\n# - kubeAPIQPS: 5：指定了KubeAPI的每秒请求频率为5个。\n# - makeIPTablesUtilChains: true：指定了是否使用iptables工具链。\n# - maxOpenFiles: 1000000：指定了最大打开文件数为1000000。\n# - maxPods: 110：指定了最大的Pod数量为110。\n# - nodeStatusUpdateFrequency: 10s：指定了节点状态更新的频率为10秒。\n# - oomScoreAdj: -999：指定了OOM Score Adjustment为-999。\n# - podPidsLimit: -1：指定了Pod的PID限制为-1，表示无限制。\n# - registryBurst: 10：指定了Registry的突发请求数量为10个。\n# - registryPullQPS: 5：指定了Registry的每秒拉取请求数量为5个。\n# - resolvConf: /etc/resolv.conf：指定了resolv.conf的文件路径。\n# - rotateCertificates: true：指定了是否轮转证书。\n# - runtimeRequestTimeout: 2m0s：指定了运行时请求的超时时间为2分钟。\n# - serializeImagePulls: true：指定了是否序列化镜像拉取。\n# - staticPodPath: /etc/kubernetes/manifests：指定了静态Pod的路径。\n# - streamingConnectionIdleTimeout: 4h0m0s：指定了流式连接的空闲超时时间为4小时。\n# - syncFrequency: 1m0s：指定了同步频率为1分钟。\n# - volumeStatsAggPeriod: 1m0s：指定了卷统计聚合周期为1分钟。\n```\n\n### 8.2.4启动kubelet\n\n```shell\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kubelet.service\n# 启用并立即启动kubelet.service单元。kubelet.service是kubelet守护进程的systemd服务单元。\n\nsystemctl restart kubelet.service\n# 重启kubelet.service单元，即重新启动kubelet守护进程。\n\nsystemctl status kubelet.service\n# kubelet.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n### 8.2.5查看集群\n\n```shell\n[root@k8s-master01 ~]# kubectl  get node\nNAME           STATUS   ROLES    AGE   VERSION\nk8s-master01   Ready    <none>   16s   v1.29.2\nk8s-master02   Ready    <none>   13s   v1.29.2\nk8s-master03   Ready    <none>   12s   v1.29.2\nk8s-node01     Ready    <none>   10s   v1.29.2\nk8s-node02     Ready    <none>   9s    v1.29.2\n[root@k8s-master01 ~]#\n```\n\n### 8.2.6查看容器运行时\n\n```shell\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n  Container Runtime Version:  containerd://1.7.13\n[root@k8s-master01 ~]# kubectl describe node | grep Runtime\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n  Container Runtime Version:  docker://25.0.3\n\n```\n\n\n## 8.3.kube-proxy配置\n\n### 8.3.1将kubeconfig发送至其他节点\n\n```shell\n# master-1执行\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig; done\n```\n\n### 8.3.2所有k8s节点添加kube-proxy的service文件\n\n```shell\ncat >  /usr/lib/systemd/system/kube-proxy.service << EOF\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\\\n  --cluster-cidr=172.16.0.0/12,fc00:2222::/112 \\\\\n  --v=2\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n\n# 这是一个 systemd 服务单元文件的示例，用于配置 Kubernetes Kube Proxy 服务。下面是对其中一些字段的详细解释：\n# \n# [Unit]\n# \n# Description: 描述了该服务单元的用途，这里是 Kubernetes Kube Proxy。\n# Documentation: 指定了该服务单元的文档地址，即 https://github.com/kubernetes/kubernetes。\n# After: 指定该服务单元应在 network.target（网络目标）之后启动。\n# [Service]\n# \n# ExecStart: 指定了启动 Kube Proxy 服务的命令。通过 /usr/local/bin/kube-proxy 命令启动，并指定了配置文件的路径为 /etc/kubernetes/kube-proxy.yaml，同时指定了日志级别为 2。\n# Restart: 配置了服务在失败或退出后自动重启。\n# RestartSec: 配置了重启间隔，这里是每次重启之间的等待时间为 10 秒。\n# [Install]\n# \n# WantedBy: 指定了该服务单元的安装目标为 multi-user.target（多用户目标），表示该服务将在多用户模式下启动。\n# 通过配置这些字段，你可以启动和管理 Kubernetes Kube Proxy 服务。请注意，你需要根据实际情况修改 ExecStart 中的路径和文件名，确保与你的环境一致。另外，可以根据需求修改其他字段的值，以满足你的特定要求。\n```\n\n### 8.3.3所有k8s节点添加kube-proxy的配置\n```shell\ncat > /etc/kubernetes/kube-proxy.yaml << EOF\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: \"\"\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/12,fc00:2222::/112\nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: \"\"\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: \"rr\"\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: \"ipvs\"\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: \"\"\nudpIdleTimeout: 250ms\nEOF\n\n# 这是一个Kubernetes的kube-proxy组件配置文件示例。以下是每个配置项的详细解释：\n# \n# 1. apiVersion: kubeproxy.config.k8s.io/v1alpha1\n#    - 指定该配置文件的API版本。\n# \n# 2. bindAddress: 0.0.0.0\n#    - 指定kube-proxy使用的监听地址。0.0.0.0表示监听所有网络接口。\n# \n# 3. clientConnection:\n#    - 客户端连接配置项。\n# \n#    a. acceptContentTypes: \"\"\n#       - 指定接受的内容类型。\n# \n#    b. burst: 10\n#       - 客户端请求超出qps设置时的最大突发请求数。\n# \n#    c. contentType: application/vnd.kubernetes.protobuf\n#       - 指定客户端请求的内容类型。\n# \n#    d. kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n#       - kube-proxy使用的kubeconfig文件路径。\n# \n#    e. qps: 5\n#       - 每秒向API服务器发送的请求数量。\n# \n# 4. clusterCIDR: 172.16.0.0/12,fc00:2222::/112\n#    - 指定集群使用的CIDR范围，用于自动分配Pod IP。\n# \n# 5. configSyncPeriod: 15m0s\n#    - 指定kube-proxy配置同步到节点的频率。\n# \n# 6. conntrack:\n#    - 连接跟踪设置。\n# \n#    a. max: null\n#       - 指定连接跟踪的最大值。\n# \n#    b. maxPerCore: 32768\n#       - 指定每个核心的最大连接跟踪数。\n# \n#    c. min: 131072\n#       - 指定最小的连接跟踪数。\n# \n#    d. tcpCloseWaitTimeout: 1h0m0s\n#       - 指定处于CLOSE_WAIT状态的TCP连接的超时时间。\n# \n#    e. tcpEstablishedTimeout: 24h0m0s\n#       - 指定已建立的TCP连接的超时时间。\n# \n# 7. enableProfiling: false\n#    - 是否启用性能分析。\n# \n# 8. healthzBindAddress: 0.0.0.0:10256\n#    - 指定健康检查监听地址和端口。\n# \n# 9. hostnameOverride: \"\"\n#    - 指定覆盖默认主机名的值。\n# \n# 10. iptables:\n#     - iptables设置。\n# \n#     a. masqueradeAll: false\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. masqueradeBit: 14\n#        - 指定伪装的Bit标记。\n# \n#     c. minSyncPeriod: 0s\n#        - 指定同步iptables规则的最小间隔。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步iptables规则的时间间隔。\n# \n# 11. ipvs:\n#     - ipvs设置。\n# \n#     a. masqueradeAll: true\n#        - 是否对所有流量使用IP伪装。\n# \n#     b. minSyncPeriod: 5s\n#        - 指定同步ipvs规则的最小间隔。\n# \n#     c. scheduler: \"rr\"\n#        - 指定ipvs默认使用的调度算法。\n# \n#     d. syncPeriod: 30s\n#        - 指定同步ipvs规则的时间间隔。\n# \n# 12. kind: KubeProxyConfiguration\n#     - 指定该配置文件的类型。\n# \n# 13. metricsBindAddress: 127.0.0.1:10249\n#     - 指定指标绑定的地址和端口。\n# \n# 14. mode: \"ipvs\"\n#     - 指定kube-proxy的模式。这里指定为ipvs，使用IPVS代理模式。\n# \n# 15. nodePortAddresses: null\n#     - 指定可用于NodePort的网络地址。\n# \n# 16. oomScoreAdj: -999\n#     - 指定kube-proxy的OOM优先级。\n# \n# 17. portRange: \"\"\n#     - 指定可用于服务端口范围。\n# \n# 18. udpIdleTimeout: 250ms\n#     - 指定UDP连接的空闲超时时间。\n```\n\n### 8.3.4启动kube-proxy\n\n```shell\n systemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n\nsystemctl enable --now kube-proxy.service\n# 启用并立即启动kube-proxy.service单元。kube-proxy.service是kube-proxy守护进程的systemd服务单元。\n\nsystemctl restart kube-proxy.service\n# 重启kube-proxy.service单元，即重新启动kube-proxy守护进程。\n\nsystemctl status kube-proxy.service\n# kube-proxy.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n\n# 9.安装网络插件\n\n**注意 9.1 和 9.2 二选其一即可，建议在此处创建好快照后在进行操作，后续出问题可以回滚**\n\n** centos7 要升级libseccomp 不然 无法安装网络插件**\n\n```shell\n# https://github.com/opencontainers/runc/releases\n# 升级runc\n# wget https://mirrors.chenby.cn/https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\n\ninstall -m 755 runc.amd64 /usr/local/sbin/runc\ncp -p /usr/local/sbin/runc  /usr/local/bin/runc\ncp -p /usr/local/sbin/runc  /usr/bin/runc\n\n#下载高于2.4以上的包\nyum -y install http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n# 清华源\nyum -y install https://mirrors.tuna.tsinghua.edu.cn/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm\n\n#查看当前版本\n[root@k8s-master-1 ~]# rpm -qa | grep libseccomp\nlibseccomp-2.5.1-1.el8.x86_64\n```\n\n## 9.1安装Calico\n\n### 9.1.1更改calico网段\n\n```shell\nwget https://mirrors.chenby.cn/https://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yaml\n\ncp calico-typha.yaml calico.yaml\ncp calico-typha.yaml calico-ipv6.yaml\n\nvim calico.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n# vim calico-ipv6.yaml\n# calico-config ConfigMap处\n    \"ipam\": {\n        \"type\": \"calico-ipam\",\n        \"assign_ipv4\": \"true\",\n        \"assign_ipv6\": \"true\"\n    },\n    - name: IP\n      value: \"autodetect\"\n\n    - name: IP6\n      value: \"autodetect\"\n\n    - name: CALICO_IPV4POOL_CIDR\n      value: \"172.16.0.0/12\"\n\n    - name: CALICO_IPV6POOL_CIDR\n      value: \"fc00:2222::/112\"\n\n    - name: FELIX_IPV6SUPPORT\n      value: \"true\"\n\n\n# 若docker镜像拉不下来，可以使用国内的仓库\nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico.yaml \nsed -i \"s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g\" calico-ipv6.yaml\n\nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico.yaml \nsed -i \"s#m.daocloud.io/docker.io/calico/#docker.io/calico/#g\" calico-ipv6.yaml\n\n# 本地没有公网 IPv6 使用 calico.yaml\nkubectl apply -f calico.yaml\n\n# 本地有公网 IPv6 使用 calico-ipv6.yaml \n# kubectl apply -f calico-ipv6.yaml \n\n```\n\n### 9.1.2查看容器状态\n\n```shell\n# calico 初始化会很慢 需要耐心等待一下，大约十分钟左右\n[root@k8s-master01 ~]# kubectl  get pod -A\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-6747f75cdc-fbvvc   1/1     Running   0          61s\nkube-system   calico-node-fs7hl                          1/1     Running   0          61s\nkube-system   calico-node-jqz58                          1/1     Running   0          61s\nkube-system   calico-node-khjlg                          1/1     Running   0          61s\nkube-system   calico-node-wmf8q                          1/1     Running   0          61s\nkube-system   calico-node-xc6gn                          1/1     Running   0          61s\nkube-system   calico-typha-6cdc4b4fbc-57snb              1/1     Running   0          61s\n```\n\n## 9.2 安装cilium\n\n### 9.2.1 安装helm\n\n```shell\n# [root@k8s-master01 ~]# curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n# [root@k8s-master01 ~]# chmod 700 get_helm.sh\n# [root@k8s-master01 ~]# ./get_helm.sh\n\n# wget https://mirrors.huaweicloud.com/helm/v3.13.2/helm-v3.13.2-linux-amd64.tar.gz\ntar xvf helm-*-linux-amd64.tar.gz\ncp linux-amd64/helm /usr/local/bin/\n```\n\n### 9.2.2 安装cilium\n\n```shell\n# 添加源\nhelm repo add cilium https://helm.cilium.io\n\n# 修改为国内源\nhelm pull cilium/cilium\ntar xvf cilium-*.tgz\ncd cilium/\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  cilium ./cilium/ -n kube-system\n\n# 启用ipv6\n# helm install cilium cilium/cilium --namespace kube-system --set ipv6.enabled=true\n\n# 启用路由信息和监控插件\n# helm install cilium cilium/cilium --namespace kube-system --set hubble.relay.enabled=true --set hubble.ui.enabled=true --set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.enabled=true --set hubble.metrics.enabled=\"{dns,drop,tcp,flow,port-distribution,icmp,http}\" \n\n```\n\n### 9.2.3 查看\n\n```shell\n[root@k8s-master01 ~]# kubectl  get pod -A | grep cil\nkube-system   cilium-gmr6c                       1/1     Running       0             5m3s\nkube-system   cilium-kzgdj                       1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-6pw4k   1/1     Running       0             5m3s\nkube-system   cilium-operator-69b677f97c-xzzdk   1/1     Running       0             5m3s\nkube-system   cilium-q2rnr                       1/1     Running       0             5m3s\nkube-system   cilium-smx5v                       1/1     Running       0             5m3s\nkube-system   cilium-tdjq4                       1/1     Running       0             5m3s\n[root@k8s-master01 ~]#\n```\n\n### 9.2.4 下载专属监控面板\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# wget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/1.12.1/examples/kubernetes/addons/prometheus/monitoring-example.yaml\n\n[root@k8s-master01 yaml]# sed -i \"s#docker.io/#m.daocloud.io/docker.io/#g\" monitoring-example.yaml\n\n[root@k8s-master01 yaml]# kubectl  apply -f monitoring-example.yaml\nnamespace/cilium-monitoring created\nserviceaccount/prometheus-k8s created\nconfigmap/grafana-config created\nconfigmap/grafana-cilium-dashboard created\nconfigmap/grafana-cilium-operator-dashboard created\nconfigmap/grafana-hubble-dashboard created\nconfigmap/prometheus created\nclusterrole.rbac.authorization.k8s.io/prometheus created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus created\nservice/grafana created\nservice/prometheus created\ndeployment.apps/grafana created\ndeployment.apps/prometheus created\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.5 下载部署测试用例\n\n说明 测试用例 需要在 安装CoreDNS 之后即可完成\n\n```shell\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/connectivity-check/connectivity-check.yaml\n\nsed -i \"s#google.com#baidu.cn#g\" connectivity-check.yaml\nsed -i \"s#quay.io/#m.daocloud.io/quay.io/#g\" connectivity-check.yaml\n\nkubectl  apply -f connectivity-check.yaml\n```\n\n### 9.2.6 查看pod\n\n```shell\n[root@k8s-master01 yaml]# kubectl  get pod -A\nNAMESPACE           NAME                                                     READY   STATUS    RESTARTS      AGE\ncilium-monitoring   grafana-59957b9549-6zzqh                                 1/1     Running   0             10m\ncilium-monitoring   prometheus-7c8c9684bb-4v9cl                              1/1     Running   0             10m\ndefault             chenby-75b5d7fbfb-7zjsr                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-hbvr8                                  1/1     Running   0             27h\ndefault             chenby-75b5d7fbfb-ppbzg                                  1/1     Running   0             27h\ndefault             echo-a-6799dff547-pnx6w                                  1/1     Running   0             10m\ndefault             echo-b-fc47b659c-4bdg9                                   1/1     Running   0             10m\ndefault             echo-b-host-67fcfd59b7-28r9s                             1/1     Running   0             10m\ndefault             host-to-b-multi-node-clusterip-69c57975d6-z4j2z          1/1     Running   0             10m\ndefault             host-to-b-multi-node-headless-865899f7bb-frrmc           1/1     Running   0             10m\ndefault             pod-to-a-allowed-cnp-5f9d7d4b9d-hcd8x                    1/1     Running   0             10m\ndefault             pod-to-a-denied-cnp-65cc5ff97b-2rzb8                     1/1     Running   0             10m\ndefault             pod-to-a-dfc64f564-p7xcn                                 1/1     Running   0             10m\ndefault             pod-to-b-intra-node-nodeport-677868746b-trk2l            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-clusterip-76bbbc677b-knfq2           1/1     Running   0             10m\ndefault             pod-to-b-multi-node-headless-698c6579fd-mmvd7            1/1     Running   0             10m\ndefault             pod-to-b-multi-node-nodeport-5dc4b8cfd6-8dxmz            1/1     Running   0             10m\ndefault             pod-to-external-1111-8459965778-pjt9b                    1/1     Running   0             10m\ndefault             pod-to-external-fqdn-allow-google-cnp-64df9fb89b-l9l4q   1/1     Running   0             10m\nkube-system         cilium-7rfj6                                             1/1     Running   0             56s\nkube-system         cilium-d4cch                                             1/1     Running   0             56s\nkube-system         cilium-h5x8r                                             1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-flpl5                         1/1     Running   0             56s\nkube-system         cilium-operator-5dbddb6dbf-gcznc                         1/1     Running   0             56s\nkube-system         cilium-t2xlz                                             1/1     Running   0             56s\nkube-system         cilium-z65z7                                             1/1     Running   0             56s\nkube-system         coredns-665475b9f8-jkqn8                                 1/1     Running   1 (36h ago)   36h\nkube-system         hubble-relay-59d8575-9pl9z                               1/1     Running   0             56s\nkube-system         hubble-ui-64d4995d57-nsv9j                               2/2     Running   0             56s\nkube-system         metrics-server-776f58c94b-c6zgs                          1/1     Running   1 (36h ago)   37h\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.7 修改为NodePort\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# kubectl  edit svc  -n kube-system hubble-ui\nservice/hubble-ui edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring grafana\nservice/grafana edited\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl  edit svc  -n cilium-monitoring prometheus\nservice/prometheus edited\n[root@k8s-master01 yaml]#\n\ntype: NodePort\n```\n\n### 9.2.8 查看端口\n\n安装时候没有创建 监控可以忽略\n\n```shell\n[root@k8s-master01 yaml]# kubectl get svc -A | grep monit\ncilium-monitoring   grafana                NodePort    10.100.250.17    <none>        3000:30707/TCP           15m\ncilium-monitoring   prometheus             NodePort    10.100.131.243   <none>        9090:31155/TCP           15m\n[root@k8s-master01 yaml]#\n[root@k8s-master01 yaml]# kubectl get svc -A | grep hubble\nkube-system         hubble-metrics         ClusterIP   None             <none>        9965/TCP                 5m12s\nkube-system         hubble-peer            ClusterIP   10.100.150.29    <none>        443/TCP                  5m12s\nkube-system         hubble-relay           ClusterIP   10.109.251.34    <none>        80/TCP                   5m12s\nkube-system         hubble-ui              NodePort    10.102.253.59    <none>        80:31219/TCP             5m12s\n[root@k8s-master01 yaml]#\n```\n\n### 9.2.9 访问\n\n安装时候没有创建 监控可以忽略\n\n```shell\nhttp://192.168.1.31:30707\nhttp://192.168.1.31:31155\nhttp://192.168.1.31:31219\n```\n\n# 10.安装CoreDNS\n\n## 10.1以下步骤只在master01操作\n\n### 10.1.1修改文件\n\n```shell\n# 下载tgz包\nhelm repo add coredns https://coredns.github.io/helm\nhelm pull coredns/coredns\ntar xvf coredns-*.tgz\ncd coredns/\n\n# 修改IP地址\nvim values.yaml\ncat values.yaml | grep clusterIP:\nclusterIP: \"10.96.0.10\"\n\n# 示例\n---\nservice:\n# clusterIP: \"\"\n# clusterIPs: []\n# loadBalancerIP: \"\"\n# externalIPs: []\n# externalTrafficPolicy: \"\"\n# ipFamilyPolicy: \"\"\n  # The name of the Service\n  # If not set, a name is generated using the fullname template\n  clusterIP: \"10.96.0.10\"\n  name: \"\"\n  annotations: {}\n---\n\n# 修改为国内源 docker源可选\nsed -i \"s#coredns/#m.daocloud.io/docker.io/coredns/#g\" values.yaml\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" values.yaml\n\n# 默认参数安装\nhelm install  coredns ./coredns/ -n kube-system\n```\n\n# 11.安装Metrics Server\n\n## 11.1以下步骤只在master01操作\n\n### 11.1.1安装Metrics-server\n\n在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率\n\n```shell\n# 下载 \nwget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# 修改配置\nvim components.yaml\n\n---\n# 1\n\t\t\t- args:\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\n        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem\n        - --requestheader-username-headers=X-Remote-User\n        - --requestheader-group-headers=X-Remote-Group\n        - --requestheader-extra-headers-prefix=X-Remote-Extra-\n\n# 2\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-dir\n        - name: ca-ssl\n          mountPath: /etc/kubernetes/pki\n\n# 3\n      volumes:\n      - emptyDir: {}\n        name: tmp-dir\n      - name: ca-ssl\n        hostPath:\n          path: /etc/kubernetes/pki\n---\n\n\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n\n# 执行部署\nkubectl apply -f components.yaml\n```\n\n### 11.1.2稍等片刻查看状态\n\n```shell\nkubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   268m         6%     2318Mi          60%       \nk8s-master02   147m         3%     1802Mi          47%       \nk8s-master03   147m         3%     1820Mi          47%       \nk8s-node01     62m          1%     1152Mi          30%       \nk8s-node02     63m          1%     1114Mi          29%  \n```\n\n# 12.集群验证\n\n## 12.1部署pod资源\n\n```shell\ncat<<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: docker.io/library/busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\nEOF\n\n# 查看\nkubectl  get pod\nNAME      READY   STATUS    RESTARTS   AGE\nbusybox   1/1     Running   0          17s\n```\n\n## 12.2用pod解析默认命名空间中的kubernetes\n\n```shell\n# 查看name\nkubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   17h\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup kubernetes\n3Server:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes\nAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n```\n\n## 12.3测试跨命名空间是否可以解析\n\n```shell\n# 查看有那些name\nkubectl  get svc -A\nNAMESPACE     NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndefault       kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP         76m\nkube-system   calico-typha      ClusterIP   10.105.100.82   <none>        5473/TCP        35m\nkube-system   coredns-coredns   ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP   8m14s\nkube-system   metrics-server    ClusterIP   10.105.60.31    <none>        443/TCP         109s\n\n# 进行解析\nkubectl exec  busybox -n default -- nslookup coredns-coredns.kube-system\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n\nName:      coredns-coredns.kube-system\nAddress 1: 10.96.0.10 coredns-coredns.kube-system.svc.cluster.local\n[root@k8s-master01 metrics-server]# \n```\n\n## 12.4每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53\n\n```shell\ntelnet 10.96.0.1 443\nTrying 10.96.0.1...\nConnected to 10.96.0.1.\nEscape character is '^]'.\n\n telnet 10.96.0.10 53\nTrying 10.96.0.10...\nConnected to 10.96.0.10.\nEscape character is '^]'.\n\ncurl 10.96.0.10:53\ncurl: (52) Empty reply from server\n```\n\n## 12.5Pod和Pod之前要能通\n\n```shell\nkubectl get po -owide\nNAME      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nbusybox   1/1     Running   0          17m   172.27.14.193   k8s-node02   <none>           <none>\n\nkubectl get po -n kube-system -owide\nNAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\ncalico-kube-controllers-76754ff848-pw4xg   1/1     Running   0          38m     172.25.244.193   k8s-master01   <none>           <none>\ncalico-node-97m55                          1/1     Running   0          38m     192.168.1.34     k8s-node01     <none>           <none>\ncalico-node-hlz7j                          1/1     Running   0          38m     192.168.1.32     k8s-master02   <none>           <none>\ncalico-node-jtlck                          1/1     Running   0          38m     192.168.1.33     k8s-master03   <none>           <none>\ncalico-node-lxfkf                          1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncalico-node-t667x                          1/1     Running   0          38m     192.168.1.31     k8s-master01   <none>           <none>\ncalico-typha-59d75c5dd4-gbhfp              1/1     Running   0          38m     192.168.1.35     k8s-node02     <none>           <none>\ncoredns-coredns-c5c6d4d9b-bd829            1/1     Running   0          10m     172.25.92.65     k8s-master02   <none>           <none>\nmetrics-server-7c8b55c754-w7q8v            1/1     Running   0          3m56s   172.17.125.3     k8s-node01     <none>           <none>\n\n# 进入busybox ping其他节点上的pod\n\nkubectl exec -ti busybox -- sh\n/ # ping 192.168.1.34\nPING 192.168.1.34 (192.168.1.34): 56 data bytes\n64 bytes from 192.168.1.34: seq=0 ttl=63 time=0.358 ms\n64 bytes from 192.168.1.34: seq=1 ttl=63 time=0.668 ms\n64 bytes from 192.168.1.34: seq=2 ttl=63 time=0.637 ms\n64 bytes from 192.168.1.34: seq=3 ttl=63 time=0.624 ms\n64 bytes from 192.168.1.34: seq=4 ttl=63 time=0.907 ms\n\n# 可以连通证明这个pod是可以跨命名空间和跨主机通信的\n```\n\n## 12.6创建三个副本，可以看到3个副本分布在不同的节点上（用完可以删了）\n\n```shell\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl  get pod \nNAME                               READY   STATUS    RESTARTS   AGE\nbusybox                            1/1     Running   0          6m25s\nnginx-deployment-9456bbbf9-4bmvk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-9rcdk   1/1     Running   0          8s\nnginx-deployment-9456bbbf9-dqv8s   1/1     Running   0          8s\n\n# 删除nginx\n[root@k8s-master01 ~]# kubectl delete deployments nginx-deployment \n```\n\n# 13.安装dashboard\n\n```shell\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\nhelm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kube-system\n```\n\n## 13.1更改dashboard的svc为NodePort，如果已是请忽略\n\n```shell\nkubectl edit svc kubernetes-dashboard -n kube-system\n  type: NodePort\n```\n\n## 13.2查看端口号\n\n```shell\nkubectl get svc kubernetes-dashboard -n kube-system\nNAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.108.120.110   <none>        443:30034/TCP   34s\n```\n\n## 13.3创建token\n\n```shell\ncat > dashboard-user.yaml << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\nEOF\n\nkubectl  apply -f dashboard-user.yaml\n\n# 创建token\nkubectl -n kube-system create token admin-user\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Im5vZExpNi1tTERLb09ONVM2cEE0SWNCUnA4eTZieE81RnVGb1IwSk5QVFEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzA4MjQ4NjM4LCJpYXQiOjE3MDgyNDUwMzgsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiMTQ1YTdmZTktMTQ0YS00NDZmLWI1M2QtNDk4OGM3YjIyZjgyIn19LCJuYmYiOjE3MDgyNDUwMzgsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.H2Oxxrb5BVLH1iDOA-Uo1I7aiAUZX1wK-xBiV9NJXQ32EDyQvss95yQbCNHtPMhQZ8jFE3NRhyjkgZMZmX7kR9J-89QXLqKhE8Qnihd1mq5HOEVQ8tjZ6ix8ymxs5QkfSvd_OUzILKBtfYAMb4Fer67Dyf14oBHWVKU9LQkCdtFaLxerK--N7gLWeGXzavqzOlEPZR5UZWUPwP5dJmAQtvSToPVMaKiA49LjaGJid0F5Pxnutr80oZRsLfKr0MpoEG6jrow1QeJ2PgVksDTcqMTpye-M6jmIbuxabsRSskTT_zEDT0J86BiLYIHnh79D-P7IUUq6GOp8DgG-wXhICQ\n```\n\n## 13.3登录dashboard\n\nhttps://192.168.1.31:30034/\n\n# 14.ingress安装\n\n## 14.1执行部署\n\n```shell\nwget https://mirrors.chenby.cn/https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml\n\n# 修改为国内源 docker源可选\nsed -i \"s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g\" *.yaml\n\ncat > backend.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: registry.cn-hangzhou.aliyuncs.com/chenby/defaultbackend-amd64:1.5 \n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\nEOF\n\nkubectl  apply -f deploy.yaml \nkubectl  apply -f backend.yaml \n\n\ncat > ingress-demo-app.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello-server\n  template:\n    metadata:\n      labels:\n        app: hello-server\n    spec:\n      containers:\n      - name: hello-server\n        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server\n        ports:\n        - containerPort: 9000\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-demo\n  template:\n    metadata:\n      labels:\n        app: nginx-demo\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-demo\n  name: nginx-demo\nspec:\n  selector:\n    app: nginx-demo\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello-server\n  name: hello-server\nspec:\n  selector:\n    app: hello-server\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 9000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress  \nmetadata:\n  name: ingress-host-bar\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: \"hello.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: hello-server\n            port:\n              number: 8000\n  - host: \"demo.chenby.cn\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/nginx\"  \n        backend:\n          service:\n            name: nginx-demo\n            port:\n              number: 8000\nEOF\n\n# 等创建完成后在执行：\nkubectl  apply -f ingress-demo-app.yaml \n\nkubectl  get ingress\nNAME               CLASS   HOSTS                            ADDRESS     PORTS   AGE\ningress-host-bar   nginx   hello.chenby.cn,demo.chenby.cn   192.168.1.32   80      7s\n\n```\n\n## 14.2过滤查看ingress端口\n\n```shell\n# 修改为nodeport\nkubectl edit svc -n ingress-nginx   ingress-nginx-controller\ntype: NodePort\n\n[root@hello ~/yaml]# kubectl  get svc -A | grep ingress\ningress-nginx          ingress-nginx-controller             NodePort    10.104.231.36    <none>        80:32636/TCP,443:30579/TCP   104s\ningress-nginx          ingress-nginx-controller-admission   ClusterIP   10.101.85.88     <none>        443/TCP                      105s\n[root@hello ~/yaml]#\n```\n\n# 15.IPv6测试\n\n\n\n```shell\n#部署应用\n\ncat<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chenby\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: chenby\n  template:\n    metadata:\n      labels:\n        app: chenby\n    spec:\n      hostNetwork: true\n      containers:\n      - name: chenby\n        image: docker.io/library/nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: chenby\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n  - IPv6\n  - IPv4\n  type: NodePort\n  selector:\n    app: chenby\n  ports:\n  - port: 80\n    targetPort: 80\nEOF\n\n\n#查看端口\n[root@k8s-master01 ~]# kubectl  get svc\nNAME           TYPE        CLUSTER-IP            EXTERNAL-IP   PORT(S)        AGE\nchenby         NodePort    fd00:1111::bc86       <none>        80:31540/TCP   5s\n[root@k8s-master01 ~]# \n\n[root@localhost yaml]# curl -I http://192.168.1.31:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:59 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n\n[root@localhost yaml]# \n\n[root@localhost yaml]# curl -I http://[2409:8a10:9e18:9020::10]:31540\nHTTP/1.1 200 OK\nServer: nginx/1.21.6\nDate: Thu, 05 May 2022 10:20:54 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 25 Jan 2022 15:03:52 GMT\nConnection: keep-alive\nETag: \"61f01158-267\"\nAccept-Ranges: bytes\n```\n\n# 16.安装命令行自动补全功能\n\n```shell\nyum install bash-completion -y\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\n```\n\n# 附录\n```shell\n# 镜像加速器可以使用DaoCloud仓库，替换规则如下\ncr.l5d.io/  ===> m.daocloud.io/cr.l5d.io/\ndocker.elastic.co/  ===> m.daocloud.io/docker.elastic.co/\ndocker.io/  ===> m.daocloud.io/docker.io/\ngcr.io/  ===> m.daocloud.io/gcr.io/\nghcr.io/  ===> m.daocloud.io/ghcr.io/\nk8s.gcr.io/  ===> m.daocloud.io/k8s.gcr.io/\nmcr.microsoft.com/  ===> m.daocloud.io/mcr.microsoft.com/\nnvcr.io/  ===> m.daocloud.io/nvcr.io/\nquay.io/  ===> m.daocloud.io/quay.io/\nregistry.jujucharms.com/  ===> m.daocloud.io/registry.jujucharms.com/\nregistry.k8s.io/  ===> m.daocloud.io/registry.k8s.io/\nregistry.opensource.zalan.do/  ===> m.daocloud.io/registry.opensource.zalan.do/\nrocks.canonical.com/  ===> m.daocloud.io/rocks.canonical.com/\n\n\n\n\n# 镜像版本要自行查看，因为镜像版本是随时更新的，文档无法做到实时更新\n\n# docker pull 镜像\n\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/cni:master \ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/node:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/typha:master\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6\ndocker pull registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2\ndocker pull kubernetesui/dashboard:v2.7.0\ndocker pull kubernetesui/metrics-scraper:v1.0.8\ndocker pull quay.io/cilium/cilium:v1.12.6\ndocker pull quay.io/cilium/certgen:v0.1.8\ndocker pull quay.io/cilium/hubble-relay:v1.12.6\ndocker pull quay.io/cilium/hubble-ui-backend:v0.9.2\ndocker pull quay.io/cilium/hubble-ui:v0.9.2\ndocker pull quay.io/cilium/cilium-etcd-operator:v2.0.7\ndocker pull quay.io/cilium/operator:v1.12.6\ndocker pull quay.io/cilium/clustermesh-apiserver:v1.12.6\ndocker pull quay.io/coreos/etcd:v3.5.4\ndocker pull quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26\n\n# docker 保存镜像\ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/cni:master -o cni.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/node:master -o node.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/typha:master -o typha.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/kube-controllers:master -o kube-controllers.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/coredns:v1.10.0 -o coredns.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/pause:3.6 -o pause.tar \ndocker save registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.5.2 -o metrics-server.tar \ndocker save kubernetesui/dashboard:v2.7.0 -o dashboard.tar \ndocker save kubernetesui/metrics-scraper:v1.0.8 -o metrics-scraper.tar \ndocker save quay.io/cilium/cilium:v1.12.6 -o cilium.tar \ndocker save quay.io/cilium/certgen:v0.1.8 -o certgen.tar \ndocker save quay.io/cilium/hubble-relay:v1.12.6 -o hubble-relay.tar \ndocker save quay.io/cilium/hubble-ui-backend:v0.9.2 -o hubble-ui-backend.tar \ndocker save quay.io/cilium/hubble-ui:v0.9.2 -o hubble-ui.tar \ndocker save quay.io/cilium/cilium-etcd-operator:v2.0.7 -o cilium-etcd-operator.tar \ndocker save quay.io/cilium/operator:v1.12.6 -o operator.tar \ndocker save quay.io/cilium/clustermesh-apiserver:v1.12.6 -o clustermesh-apiserver.tar \ndocker save quay.io/coreos/etcd:v3.5.4 -o etcd.tar \ndocker save quay.io/cilium/startup-script:d69851597ea019af980891a4628fb36b7880ec26 -o startup-script.tar \n\n# 传输到各个节点\nfor NODE in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp -r images/  $NODE:/root/ ; done\n\n# 创建命名空间\nctr ns create k8s.io\n\n# 导入镜像\nctr --namespace k8s.io image import images/cni.tar\nctr --namespace k8s.io image import images/node.tar\nctr --namespace k8s.io image import images/typha.tar\nctr --namespace k8s.io image import images/kube-controllers.tar \nctr --namespace k8s.io image import images/coredns.tar \nctr --namespace k8s.io image import images/pause.tar \nctr --namespace k8s.io image import images/metrics-server.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/dashboard.tar \nctr --namespace k8s.io image import images/metrics-scraper.tar \nctr --namespace k8s.io image import images/cilium.tar \nctr --namespace k8s.io image import images/certgen.tar \nctr --namespace k8s.io image import images/hubble-relay.tar \nctr --namespace k8s.io image import images/hubble-ui-backend.tar \nctr --namespace k8s.io image import images/hubble-ui.tar \nctr --namespace k8s.io image import images/cilium-etcd-operator.tar \nctr --namespace k8s.io image import images/operator.tar \nctr --namespace k8s.io image import images/clustermesh-apiserver.tar \nctr --namespace k8s.io image import images/etcd.tar \nctr --namespace k8s.io image import images/startup-script.tar \n\n# pull tar包 解压后\nhelm pull cilium/cilium\n\n# 查看镜像版本\nroot@hello:~/cilium# cat values.yaml| grep tag: -C1\n  repository: \"quay.io/cilium/cilium\"\n  tag: \"v1.12.6\"\n  pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/certgen\"\n    tag: \"v0.1.8@sha256:4a456552a5f192992a6edcec2febb1c54870d665173a33dc7d876129b199ddbd\"\n    pullPolicy: \"IfNotPresent\"\n--\n      repository: \"quay.io/cilium/hubble-relay\"\n      tag: \"v1.12.6\"\n       # hubble-relay-digest\n--\n        repository: \"quay.io/cilium/hubble-ui-backend\"\n        tag: \"v0.9.2@sha256:a3ac4d5b87889c9f7cc6323e86d3126b0d382933bd64f44382a92778b0cde5d7\"\n        pullPolicy: \"IfNotPresent\"\n--\n        repository: \"quay.io/cilium/hubble-ui\"\n        tag: \"v0.9.2@sha256:d3596efc94a41c6b772b9afe6fe47c17417658956e04c3e2a28d293f2670663e\"\n        pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium-etcd-operator\"\n    tag: \"v2.0.7@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/operator\"\n    tag: \"v1.12.6\"\n    # operator-generic-digest\n--\n    repository: \"quay.io/cilium/startup-script\"\n    tag: \"d69851597ea019af980891a4628fb36b7880ec26\"\n    pullPolicy: \"IfNotPresent\"\n--\n    repository: \"quay.io/cilium/cilium\"\n    tag: \"v1.12.6\"\n    # cilium-digest\n--\n      repository: \"quay.io/cilium/clustermesh-apiserver\"\n      tag: \"v1.12.6\"\n      # clustermesh-apiserver-digest\n--\n        repository: \"quay.io/coreos/etcd\"\n        tag: \"v3.5.4@sha256:795d8660c48c439a7c3764c2330ed9222ab5db5bb524d8d0607cac76f7ba82a3\"\n        pullPolicy: \"IfNotPresent\"\n\n```\n","tags":["kubernetes"],"categories":["云原生"]},{"title":"【云原生】使用Cilium作为Kubernetes CNI插件","url":"/2024/03/11/【云原生】使用Cilium作为Kubernetes CNI插件/","content":"\n### **前提条件**\n\n* Kubernetes must be configured to use CNI\n\n  * see [Network Plugin Requirements](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements)\n\n* Linux kernel >= 4.9.17\n\n  * 升级内核，原文地址：[yum更新Linux内核 - Elijah Blog (](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)\n\n### **卸载集群中的CNI插件**\n\n原文地址：[CNI插件完全卸载指南（Flannel、Calico、Cilium） - Elijah Blog (](https://sreok.cn/archives/10016b39-5056-4216-976c-57668927c05d)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/10016b39-5056-4216-976c-57668927c05d)\n\n### **安装cilium**\n\n下载地址：[Releases · cilium/cilium-cli (](https://github.com/cilium/cilium-cli/releases)[github.com](http://github.com)[)](https://github.com/cilium/cilium-cli/releases)\n\n```\ntar zxvf cilium-linux-amd64.tar.gz\nmv cilium /usr/local/bin\nrm -f cilium-linux-amd64.tar.gz\n```\n\n### **启用Hubble并开启UI**\n\n```\ncilium hubble enable --ui\n```\n\n> 如果已使用 `cilium hubble enable` 启用 Hubble，则必须首先使用 `cilium hubble disable` 暂时禁用 Hubble。这是因为 Hubble UI 无法在运行时添加。\n\n### **已知问题**\n\n`cilium daemonset处于Init:CrashLoopBackOff状态`\n\n#### **日志信息**\n\n```\n[root@master ~]# kubectl logs -f -n kube-system cilium-jdrzf mount-cgroup\nnsenter: cannot open /hostproc/1/ns/cgroup: No such file or directory\n```\n\n#### **原因**\n\n内核版本低于要求内核\n\n#### **解决方法**\n\n升级内核版本\n\n原文地址：[yum更新Linux内核（CentOS 7） - Elijah Blog (](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)\n","tags":["CNI","Kubernetes"],"categories":["云原生"]},{"title":"【云原生】Kubernetes 部署MinIO集群-四节点(manifest方式)","url":"/2024/02/16/【云原生】Kubernetes 部署MinIO集群-四节点(manifest方式)/","content":"\n### **前提条件**\n\n* 集群安装StorageClass作为数据存储\n\n### **安装MinIO StatefulSet**\n\n```\ncat > minio.yaml << EOF\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  serviceName: \"minio-headless\"\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      # nodeSelector:\n      #   minio: true\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ROOT_USER\n          value: \"admin\"\n        - name: MINIO_ROOT_PASSWORD\n          value: \"123456\"\n        image: minio/minio:RELEASE.2024-07-04T14-25-45Z\n        imagePullPolicy: IfNotPresent\n        command:\n          - /bin/sh\n          - -c\n          - minio server --console-address \":5000\" http://minio-{0...3}.minio-headless:9000/data\n        ports:\n        - name: data\n          containerPort: 9000\n          protocol: \"TCP\"\n        - name: console\n          containerPort: 5000\n          protocol: \"TCP\"\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: time-mount\n          mountPath: /etc/localtime\n      volumes:\n      - name: time-mount\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      storageClassName: \"rook-ceph-block\"\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 200Gi\nEOF\n```\n\n```\nkubectl apply -f minio.yaml\n```\n\n```\n# 可选,如果取消掉nodeSelector注释时,需要在node打标签\nkubectl label node k8s-worker01 minio=true\nkubectl label node k8s-worker02 minio=true\nkubectl label node k8s-worker03 minio=true\n```\n\n### **配置Headless和NodePort**\n\n```\ncat > minio-svc.yaml << EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-headless\n  labels:\n    app: minio\nspec:\n  clusterIP: None\n  ports:\n    - port: 9000\n      name: data\n  selector:\n    app: minio\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\nspec:\n  type: NodePort\n  ports:\n   - name: data\n     nodePort: 30009\n     port: 9000\n     targetPort: 9000\n   - name: console\n     nodePort: 31901\n     port: 5000\n     targetPort: 30005\n  selector:\n    app: minio\nEOF\n```\n\n```\nkubectl apply -f minio-svc.yaml\n```\n\n### **访问WebUI**\n\n![](/images/image-hwoh.png)\n\n![](/images/image-dode.png)\n","tags":["Kubernetes","MinIO"],"categories":["云原生"]},{"title":"【云原生】Kubernetes部署KeyCloak使用MySQL 8.0","url":"/2023/12/17/【云原生】Kubernetes部署KeyCloak使用MySQL 8.0/","content":"\n官方文档：[Kubernetes - Keycloak](https://www.keycloak.org/getting-started/getting-started-kube)\n\n#### 部署MySQL\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-keycloak-mysql\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"longhorn\"\n  resources:\n    requests:\n      storage: 50Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n        - name: mysql\n          image: mysql:8.0\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: keycloak\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/mysql\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: data-keycloak-mysql\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  selector:\n    app: mysql\n  type: NodePort\n  ports:\n  - port: 3306\n    targetPort: 3306\n    nodePort: 30036\n```\n\n##### 连接数据库\n\n`创建数据库keycloak，使用utf8mb4字符集`\n\n![](/images/image-kbzk.png)\n\n#### 部署KeyCloak\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: keycloak\n  labels:\n    app: keycloak\nspec:\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n      nodePort: 30080\n  selector:\n    app: keycloak\n  type: NodePort\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keycloak\n  labels:\n    app: keycloak\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keycloak\n  template:\n    metadata:\n      labels:\n        app: keycloak\n    spec:\n      containers:\n        - name: keycloak\n          image: quay.io/keycloak/keycloak:25.0.2\n          args: [\"start-dev\"]\n          env:\n            - name: KC_DB\n              value: \"mysql\"\n            - name: KC_DB_USERNAME\n              value: \"root\"\n            - name: KC_DB_PASSWORD\n              value: \"keycloak\"\n            - name: KC_DB_URL\n              value: \"jdbc:mysql://mysql:3306/keycloak?characterEncoding=UTF-8\"\n            - name: KC_HEALTH_ENABLED\n              value: \"true\"\n            - name: KEYCLOAK_ENABLE_STATISTICS\n              value: \"true\"\n            - name: KEYCLOAK_HOSTNAME\n              value: kc.sreok.cn\n            - name: KEYCLOAK_ADMIN\n              value: \"admin\"\n            - name: KEYCLOAK_ADMIN_PASSWORD\n              value: \"admin\"\n          ports:\n            - name: http\n              containerPort: 8080\n          readinessProbe:\n            httpGet:\n              path: /realms/master\n              port: 8080\n```\n\n参考文档：\n\n[Running Keycloak in a container - Keycloak](https://www.keycloak.org/server/containers)\n\n[Configuring Keycloak for production - Keycloak](https://www.keycloak.org/server/configuration-production)\n","tags":["Kubernetes"],"categories":["云原生"]},{"title":"【云原生】高可用负载方案：使用kube-vip作为控制平面负载入口","url":"/2023/11/23/【云原生】高可用负载方案：使用kube-vip作为控制平面负载入口/","content":"\n官方文档地址：[Static Pods | kube-vip](https://kube-vip.io/docs/installation/static/)\n\n### 生成静态pod清单\n\n在使用kubeadm部署集群前，在每个master节点生成静态pod清单\n\n```\n# 设置vip，前提先ping一下，确保IP没有被占用\nexport VIP=10.20.13.100\n# 指定网卡\nexport INTERFACE=ens192\n# 获取最新版本\n# KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r \".[0].name\")\n# 或者指定版本（这个版本目前比较稳定，不会出现报错）\nexport KVVERSION=v0.6.4\n# \nalias kube-vip=\"ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION; ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip\"\n```\n\n```\n# 生成清单文件\nmkdir -p /etc/kubernetes/manifests\n​\nkube-vip manifest pod \\\n    --interface $INTERFACE \\\n    --address $VIP \\\n    --controlplane \\\n    --services \\\n    --arp \\\n    --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n或者直接创建清单文件\n\n```\nmkdir -p /etc/kubernetes/manifests\n​\ncat > /etc/kubernetes/manifests/kube-vip.yaml << EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-vip\n  namespace: kube-system\nspec:\n  containers:\n  - args:\n    - manager\n    env:\n      # 使用ARP协议\n    - name: vip_arp\n      value: \"true\"\n    - name: port\n      value: \"6443\"\n    - name: vip_nodename\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: vip_interface\n      # 网卡名称\n      value: ens192\n    - name: vip_cidr\n      value: \"32\"\n    - name: dns_mode\n      value: first\n    - name: cp_enable\n      value: \"true\"\n    - name: cp_namespace\n      value: kube-system\n    - name: svc_enable\n      value: \"true\"\n    - name: svc_leasename\n      value: plndr-svcs-lock\n    - name: vip_leaderelection\n      value: \"true\"\n    - name: vip_leasename\n      value: plndr-cp-lock\n    - name: vip_leaseduration\n      value: \"5\"\n    - name: vip_renewdeadline\n      value: \"3\"\n    - name: vip_retryperiod\n      value: \"1\"\n    - name: address\n      # vip地址\n      value: 10.20.13.100\n    - name: prometheus_server\n      value: :2112\n    image: ghcr.io/kube-vip/kube-vip:v0.6.4\n    imagePullPolicy: IfNotPresent\n    name: kube-vip\n    resources: {}\n    securityContext:\n      capabilities:\n        add:\n        - NET_ADMIN\n        - NET_RAW\n    volumeMounts:\n    - mountPath: /etc/kubernetes/admin.conf\n      name: kubeconfig\n  hostAliases:\n  - hostnames:\n    - kubernetes\n    ip: 127.0.0.1\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/admin.conf\n    name: kubeconfig\nstatus: {}\nEOF\n```\n\n### 故障排查\n\n报错：`error retrieving resource lock kube-system/plndr-cp-lock`\n\n> 问题地址：\n>\n> [*kube-vip 需要 super-admin.conf 和 Kubernetes 1.29 ·问题 #684 ·kube-vip/kube-vip ·GitHub上*](https://github.com/kube-vip/kube-vip/issues/684)\n>\n> [*从 Kubernetes v1.29 开始对 kube-vip 使用超级管理员 Kubeconfig 由 abhay-krishna ·拉取请求 #7368 ·aws/eks-anywhere ·GitHub上*](https://github.com/aws/eks-anywhere/pull/7368)\n\n解决方法：\n\n```\n# 1.29版本以后需要对kube-vip修改kubernetes客户端路径\nsed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \\\n          /etc/kubernetes/manifests/kube-vip.yaml\nsystemctl restart kubelet\n```\n\n```\n# 部署后就可以使用以下命令恢复回来了\nsed -i 's#path: /etc/kubernetes/super-admin.conf#path: /etc/kubernetes/admin.conf#' \\\n          /etc/kubernetes/manifests/kube-vip.yaml\nsystemctl restart kubelet\n```\n\n### **注意**\n\n静态pod部署，主要适用于kubeadm集群，因为kubeadm创建集群的过程中需要使用虚拟ip，kube-vip还有daemonset方式，这里没用过不做讨论，以下是daemonset描述的原文直译：\n\n`一些Kubernetes发行版可以在不依赖预先存在的VIP（虚拟IP）的情况下创建Kubernetes集群（但它们也可以配置为支持VIP）。K3s就是一个典型的例子，它可以配置为启动并签署证书，以允许流量进入虚拟IP。鉴于我们不需要在集群创建之前存在VIP，我们可以启动K3s节点，然后将kube-vip作为所有控制平面节点的DaemonSet添加进去。`\n","tags":["Kubernetes","kube-vip"],"categories":["云原生"]},{"title":"【云原生】高可用负载方案：使用Keepalived + HAProxy作为控制平面负载入口","url":"/2023/11/12/【云原生】高可用负载方案：使用Keepalived + HAProxy作为控制平面负载入口/","content":"\n### 安装keepalived\n\n```\nyum -y install keepalived\n```\n\n```\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\n​\nglobal_defs {\n   router_id LVS_DEVEL\n}\n​\nvrrp_script chk_haproxy {\n  script \"killall -0 haproxy\"\n  interval 2\n  weight -3\n  fall 1\n  rise 2\n  timeout 2\n}\n​\nvrrp_instance VI_1 {\n    state MASTER        # 其他节点为BACKUP\n    interface ens5f0        # 网卡\n    virtual_router_id 60    # id与其他keepalived节点一致\n    priority 100            # 初始化权重，一致即可，不影响\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111      # 密码与其他keepalived节点一致\n    }\n    unicast_src_ip 172.25.2.4   # 本机ip\n    unicast_peer {\n        172.25.2.5      # 其他keepalived节点ip\n        172.25.2.6\n    }\n    virtual_ipaddress {\n        172.25.2.50     # vip\n    }\n    track_script {\n        chk_haproxy\n    }\n}\nEOF\n```\n\n> 注释的地方，按需求修改\n>\n> 我这里的配置文件只需要修改如下\n>\n> `state`MASTER改为BACKUP\n>\n> `unicast_src_ip`修改为本机IP\n>\n> `unicast_peer`修改为其他keepalived节点（其他master节点）\n>\n> 其他位置不用动。\n\n### **安装HAProxy**\n\n```\nyum -y install haproxy\n```\n\n```\ncat > /etc/haproxy/haproxy.cfg <<EOF\n#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   https://www.haproxy.org/download/1.8/doc/configuration.txt\n#\n#---------------------------------------------------------------------\n​\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n​\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n​\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n​\n    # utilize system-wide crypto-policies\n    ssl-default-bind-ciphers PROFILE=SYSTEM\n    ssl-default-server-ciphers PROFILE=SYSTEM\n​\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n​\n#---------------------------------------------------------------------\n# main frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend apisever\n    # 监听8443端口\n    bind *:8443\n    mode tcp\n    option tcplog\n    default_backend apiserver\n​\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\nbackend apiserver\n    option httpchk GET /healthz\n    http-check expect status 200\n    mode tcp\n    option ssl-hello-chk\n    balance     roundrobin\n        # apiserver节点\n        server k8s-master01 172.25.2.4:6443 check\n        server k8s-master02 172.25.2.5:6443 check\n        server k8s-master03 172.25.2.6:6443 check\nEOF\n```\n\n> 以上配置文件在master节点一致即可，不用额外修改\n\n### **开启并开机自启**\n\n```\nsystemctl enable keepalived --now\nsystemctl enable haproxy --now\n```\n\n### **验证vip漂移**\n\n```\nsystemctl stop haproxy\n# 5秒后漂移到其他节点\n```\n","tags":["Keepalived","HAProxy"],"categories":["云原生"]},{"title":"【云原生】etcd 入门 - 二进制部署etcd集群","url":"/2023/10/04/【云原生】etcd 入门 - 二进制部署etcd集群/","content":"\n# etcd 入门 - 二进制部署etcd集群\n\n### 环境信息\n\n最少三个及以上奇数个节点\n\n| 名称   | IP          |\n| ------ | ----------- |\n| etcd-1 | 10.20.13.10 |\n| etcd-2 | 10.20.13.11 |\n| etcd-3 | 10.20.13.12 |\n\n### etcd集群节点数量的相关说明：\n\netcd 是基于 raft算法的分布式键值数据库，生来就为集群化而设计的，由于Raft算法在做决策时需要超半数节点的投票，所以etcd集群一般推荐奇数节点，如3、5或者7个节点构成一个集群。\n\netcd官方推荐3、5、7个节点，虽然raft算法也是半数以上投票才能有 leader，但奇数只是推荐，其实偶数也是可以的。如 2、4、8个节点。下面分情况说明：\n\n- 1 个节点：就是单实例，没有集群概念，不做讨论\n- 2 个节点：是集群，但没人会这么配，这里说点废话：双节点的etcd能启动，启动时也能有主，可以正常提供服务，但是一台挂掉之后，就选不出主了，因为他只能拿到1票，剩下的那台也无法提供服务，也就是双节点无容错能力，不要使用。\n- 3 节点：标准的3 节点etcd 集群只能容忍1台机器宕机，挂掉 1 台此时等于2个节点的情况，如果再挂 1 台，就和 2节点的情形一致了，一直选，一直增加任期，但就是选不出来，服务也就不可用了\n- 4 节点：最大容忍1 台 服务器宕机\n- 5 节点：最大容忍 2 台 服务器宕机\n- 6 节点：最大容忍 2 台 服务器宕机\n- 7和8个节点，最大容忍3台 服务器宕机\n\n以此类推，9和10个节点，最大容忍4台 服务器宕机，总结以上可以得出结论：偶数节点虽然多了一台机器，但是容错能力是一样的，也就是说，你可以设置偶数节点，但没增加什么能力，还浪费了一台机器。同时etcd 是通过复制数据给所有节点来达到一致性，因此偶数的多一台机器增加不了性能，反而会拉低写入速度。\n\n## 部署etcd集群（三台节点）\n\n### 部署etcd\n\n下载地址：[Releases · etcd-io/etcd (github.com)](https://github.com/etcd-io/etcd/releases/)\n\n##### 解压\n\n```\ntar zxvf etcd-v3.5.13-linux-amd64.tar.gz\n```\n\n##### 添加到环境变量目录\n\n```\ncd etcd-v3.5.13-linux-amd64\nmv etcd etcdctl etcdutl /usr/local/bin/\n```\n\n##### 创建配置文件目录与数据目录\n\n```\nmkdir -p /etc/etcd\nmkdir -p /data/etcd\nmkdir -p /etc/etcd/pki\n```\n\n##### 注册系统服务\n\n```\nvim /etc/systemd/system/etcd.service\n```\n\n```\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=notify\nEnvironmentFile=/etc/etcd/etcd.conf\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.conf\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n##### etcd-1节点\n\n```\n# 节点名称\nname: \"etcd-1\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.10:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.10:2379,http://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.10:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.10:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n##### etcd-2节点\n\n```\n# 节点名称\nname: \"etcd-2\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.11:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.11:2379,https://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.11:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.11:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n##### etcd-3节点\n\n```\n# 节点名称\nname: \"etcd-3\"\n# 数据存储目录\ndata-dir: \"/data/etcd\"\n# 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点\nadvertise-client-urls: \"https://10.20.13.12:2379\"\n# 监听客户端请求的地址列表\nlisten-client-urls: \"https://10.20.13.12:2379,https://127.0.0.1:2379\"\n# 监听URL，用于节点之间通信监听地址\nlisten-peer-urls: \"https://10.20.13.12:2380\"\n# 服务端之间通讯使用的地址列表,该节点同伴监听地址，这个值会告诉集群中其他节点\ninitial-advertise-peer-urls: \"https://10.20.13.12:2380\"\n# etcd启动时，etcd集群的节点地址列表\ninitial-cluster: \"etcd-1=https://10.20.13.10:2380,etcd-2=https://10.20.13.11:2380,etcd-3=https://10.20.13.12:2380\"\n# etcd集群的初始集群令牌\ninitial-cluster-token: 'etcd-cluster'\n# etcd集群初始化的状态，new代表新建集群，existing表示加入现有集群\ninitial-cluster-state: 'new'\n# 日志配置\nlogger: zap\n\n# 客户端加密\nclient-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n\n# 节点加密\npeer-transport-security:\n  cert-file: \"/etc/etcd/pki/etcd.pem\"\n  key-file: \"/etc/etcd/pki/etcd-key.pem\"\n  client-cert-auth: True\n  trusted-ca-file: \"/etc/etcd/pki/ca.pem\"\n```\n\n先不启动，准备证书文件，如果不需要证书认证，在配置文件删除上方加密证书的配置启动。跳转到最下方测试即可。\n\n### 部署TLS加密集群\n\n#### 安装cfssl\n\n下载地址：[Releases · cloudflare/cfssl (github.com)](https://github.com/cloudflare/cfssl/releases)\n\n```\nmv cfssl_1.6.5_linux_amd64 /usr/bin/cfssl\nmv cfssljson_1.6.5_linux_amd64 /usr/bin/cfssljson\nchmod +x /usr/bin/{cfssl,cfssljson}\n```\n\n#### 创建默认配置文件（可选，下一步会替换）\n\n```\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n#### 创建CA证书\n\n##### 修改ca-config\n\n各个组件都需要配置证书，并且依赖 CA 证书来签发证书，所以首先要生成好 CA 证书以及后续的签发配置文件\n\n```\ncat > ca-config.json <<EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"Etcd\": {\n        \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"87600h\"\n      }\n    }\n  }\n}\nEOF\n```\n\n>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；\n>signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；\n>server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；\n>client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；\n\n##### 配置证书请求\n\n```\ncat > ca-csr.json <<EOF\n{\n    \"CN\": \"Etcd\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"BeiJing\",\n            \"ST\": \"BeiJing\",\n            \"O\": \"Etcd\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n```\n\n#### 生成ca证书\n\n```\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n```\n\n>CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法；\n>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；\n>kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识；\n\n#### 创建客户端与对等证书请求文件\n\n`注意hosts字段需要加上etcd全部节点的IP/主机名信息及127.0.0.1`\n\n```\ncat > etcd-csr.json <<EOF\n{\n    \"CN\": \"etcd\",\n    \"hosts\": [\n      \"127.0.0.1\",\n      \"10.20.13.10\",\n      \"10.20.13.11\",\n      \"10.20.13.12\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"beijing\",\n            \"ST\": \"BeiJing\",\n            \"O\": \"ETCD\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n```\n\n#####  生成证书和私钥\n\n```\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=Etcd etcd-csr.json | cfssljson -bare etcd\n```\n\n#### 拷贝密钥到etcd节点\n\n```\ncp *.pem /etc/etcd/pki/\nscp /etc/etcd/pki/* 10.20.13.11:/etc/etcd/pki/\nscp /etc/etcd/pki/* 10.20.13.12:/etc/etcd/pki/\n```\n\n\n\n#### 设置开机自启并启动\n\n```\nsystemctl daemon-reload\nsystemctl enable etcd --now\n```\n\n### 测试\n\n```\netcdctl --endpoints=https://10.20.13.10:2379 \\\n--cacert=/etc/etcd/pki/ca.pem \\\n--cert=/etc/etcd/pki/etcd.pem \\\n--key=/etc/etcd/pki/etcd-key.pem \\\nendpoint status --cluster -w table\n```\n\n![image-20240423142609585](/images/image-vvcc.png)\n\n如果没有使用证书，用下方命令测试\n\n```\netcdctl endpoint status --cluster -w table\n```\n\n![image-20240422164147285](/images/image-tlzm.png)\n\n测试读写\n\n在第一台节点上写入\n\n![image-20240423142708565](/images/image-ontn.png)\n\n第二台节点上读取\n\n![image-20240423142805685](/images/image-ugmd.png)\n\n第三台节点上读取\n\n![image-20240423142821737](/images/image-onxo.png)","tags":["etcd"],"categories":["云原生"]},{"title":"【云原生】etcd 入门 - 二进制部署单节点etcd","url":"/2023/10/03/【云原生】etcd 入门 - 二进制部署单节点etcd/","content":"\n# etcd 入门 - 二进制部署单节点etcd\n\n下载地址：[Releases · etcd-io/etcd (github.com)](https://github.com/etcd-io/etcd/releases/)\n\n#### 解压\n\n```\ntar zxvf etcd-v3.5.13-linux-amd64.tar.gz\n```\n\n#### 添加到环境变量目录\n\n```\ncd etcd-v3.5.13-linux-amd64\nmv etcd etcdctl etcdutl /usr/local/bin/\n```\n\n#### 创建配置文件目录与数据目录\n\n```\nmkdir -p /etc/etcd\nmkdir -p /data/etcd\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/etcd.service\n```\n\n```\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=notify\nEnvironmentFile=/etc/etcd/etcd.conf\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.conf\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n```\n# 节点名称\nname: 'etcd-1'\n# 指定节点的数据存储目录\ndata-dir: '/data/etcd'\n# 对外提供服务的地址，客户端会连接到这里和 etcd 交互\nlisten-client-urls: 'http://10.20.13.10:2379,http://127.0.0.1:2379'\n```\n\n#### 设置开机自启并启动\n\n```\nsystemctl daemon-reload\nsystemctl enable etcd --now\n```\n\n#### 测试\n\n```\netcdctl endpoint status --cluster -w table\n```\n\n![image-20240422160908837](/images/image-ozlm.png)\n\n![image-20240422160429784](images//image-dgcp.png)","tags":["etcd"],"categories":["云原生"]},{"title":"【DevOps】Kubernetes部署Jenkins持续集成v2.452.3-lts-jdk21（manifest方式）","url":"/2023/09/22/【DevOps】Kubernetes部署Jenkins持续集成v2.452.3-lts-jdk21（manifest方式）/","content":"\n### RBAC\n\n```\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: devops\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: jenkins\nrules:\n- apiGroups:\n  - '*'\n  resources:\n  - statefulsets\n  - services\n  - replicationcontrollers\n  - replicasets\n  - podtemplates\n  - podsecuritypolicies\n  - pods\n  - pods/log\n  - pods/exec\n  - podpreset\n  - poddisruptionbudget\n  - persistentvolumes\n  - persistentvolumeclaims\n  - jobs\n  - endpoints\n  - deployments\n  - deployments/scale\n  - daemonsets\n  - cronjobs\n  - configmaps\n  - namespaces\n  - events\n  - secrets\n  verbs:\n  - create\n  - get\n  - watch\n  - delete\n  - list\n  - patch\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n  - update\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: jenkins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: jenkins\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  # 授权给devops命名空间\n  name: system:serviceaccounts:devops\n```\n\n### 部署Jenkins\n\n```\n---\napiVersion: v1\nkind:  PersistentVolumeClaim\nmetadata:\n  name: jenkins-home\n  namespace: devops\nspec:\n  storageClassName: \"longhorn\"\n  accessModes: [ReadWriteMany]\n  resources:\n    requests:\n      storage: 200Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins\n  namespace: devops\n  labels:\n    app: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins\n  template:\n    metadata:\n      labels:\n        app: jenkins\n    spec:\n      tolerations:\n      - effect: NoSchedule\n        key: no-pod\n        operator: Exists\n      nodeSelector:\n        jenkins: \"true\"\n      containers:\n      - name: jenkins\n        image: jenkins/jenkins:2.452.3-lts-jdk21\n        ports:\n        - containerPort: 8080\n          name: web\n          protocol: TCP\n        - containerPort: 50000\n          name: agent\n          protocol: TCP\n        env:\n        - name: LIMITS_MEMORY\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: 1Mi\n        - name: JAVA_OPTS\n          value: -Dhudson.security.csrf.GlobalCrumbIssuerConfiguration.DISABLE_CSRF_PROTECTION=true\n        volumeMounts:\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n        - mountPath: /etc/localtime\n          name: localtime\n      volumes:\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: jenkins-home\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: jenkins\n  namespace: devops\n  labels:\n    app: jenkins\nspec:\n  selector:\n    app: jenkins\n  type: LoadBalancer\n  ports:\n  - name: web\n    nodePort: 30456\n    port: 8080\n    targetPort: web\n  - name: agent\n    nodePort: 30789\n    port: 50000\n    targetPort: agent\n```\n\n### Ingress\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    app: jenkins\n  name: jenkins-ingress\n  namespace: devops\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: jenkins.sreok.cn\n    http:\n      paths:\n      - backend:\n          service:\n            name: jenkins\n            port:\n              number: 8080\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - jenkins.sreok.cn\n    secretName: devops-tls\n```\n","tags":["DevOps","Jenkins"],"categories":["云原生"]},{"title":"【DevOps】Kubernetes部署Jenkins持续集成（Helm方式）","url":"/2023/09/21/【DevOps】Kubernetes部署Jenkins持续集成（Helm方式）/","content":"\n### 导入Helm源\n\n```\nhelm repo add jenkins https://charts.jenkins.io\nhelm pull jenkins/jenkins --untar\n```\n\n### 修改values.yaml\n\n```\ncontroller:\n  # -- Used for label app.kubernetes.io/component\n  componentName: \"jenkins-controller\"\n  image:\n    # -- Controller image registry\n    registry: \"docker.io\"\n    # -- Controller image repository\n    repository: \"jenkins/jenkins\"\n    # -- Controller image tag override; i.e., tag: \"2.440.1-jdk17\"\n    tag:\n\n    tagLabel: jdk17\n    pullPolicy: \"IfNotPresent\"\n  imagePullSecretName: \n  lifecycle: {}\n  disableRememberMe: false\n\n  numExecutors: 0\n  executorMode: \"NORMAL\"\n  customJenkinsLabels: []\n\n  hostNetworking: false\n  admin:\n    username: \"admin\"\n    # jenkins密码\n    password: \"admin\"\n...\n...\n...\n  installPlugins:\n    - kubernetes:4253.v7700d91739e5\n    - workflow-aggregator:600.vb_57cdd26fdd7\n    - git:5.2.2\n    - configuration-as-code:1836.vccda_4a_122a_a_e\n    # 安装中文插件\n    - localization-zh-cn:371.v23851f835d6b_\n...\n...\n...\n  ingress\n    # 开启Ingress\n    enabled: true\n    paths: []\n    apiVersion: \"extensions/v1beta1\"\n    # -- Ingress labels\n    labels: {}\n    # -- Ingress annotations\n    annotations: {}\n    # 设置ingress\n    ingressClassName: nginx\n\n    # Set this path to jenkinsUriPrefix above or use annotations to rewrite path\n    # -- Ingress path\n    path:\n\n    # configures the hostname e.g. jenkins.example.com\n    # -- Ingress hostname\n    hostName:\n    # -- Hostname to serve assets from\n    resourceRootUrl:\n    # -- Ingress TLS configuration\n    # 设置存在的自签名证书的secret\n    tls:\n    - secretName: devops-tls\n      hosts:\n        - jenkins.sreok.cn\n...\n...\n...\npersistence:\n  # -- Enable the use of a Jenkins PVC\n  enabled: true\n  # -- Provide the name of a PVC\n  existingClaim:\n  # -- Storage class for the PVC\n  storageClass:\n  # -- Annotations for the PVC\n  annotations: {}\n  # -- Labels for the PVC\n  labels: {}\n  # -- The PVC access mode\n  accessMode: \"ReadWriteOnce\"\n  # -- The size of the PVC\n  # 适当修改PVC大小\n  size: \"200Gi\"\n```\n\n> 修改中文注释部分。\n\n```\ncd jenkins/\nhelm upgrade --install -n devops jenkins --create-namespace \\\n--set controller.jenkinsUrl=\"jenkins.sreok.cn\" \\\n--set controller.admin.password=\"admin\" \\\n--set controller.ingress.enabled=true \\\n--set controller.ingress.ingressClassName=\"nginx\" \\\n--set controller.ingress.tls[0].secretName=\"devops-tls\" \\\n--set controller.ingress.tls[0].hosts[0]=\"jenkins.sreok.cn\" \\\n--set persistence.size=\"20Gi\" .\n```\n\njenkins plugin代理：<https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json>\n","tags":["DevOps","Jenkins"],"categories":["云原生"]},{"title":"【DevOps】Kubernetes部署私有代码仓库GitLab v17.2.0（Helm方式安装）","url":"/2023/09/10/【DevOps】Kubernetes部署私有代码仓库GitLab v17.2.0（Helm方式安装）/","content":"\n官方文档：[Deploy the GitLab Helm chart | GitLab](https://docs.gitlab.com/charts/installation/deployment.html)\n\n官方文档（推荐，国内镜像）：[部署极狐GitLab Helm chart | 极狐GitLab](https://docs.gitlab.cn/charts/installation/deployment.html)\n\n### 前提条件\n\n* cert-manager\n\n* nginx-ingress\n\n### 添加代码仓库\n\n```\nhelm repo add gitlab http://charts.gitlab.io/\n# 国内版\nhelm repo add gitlab-jh https://charts.gitlab.cn\n```\n\n```\nhelm pull gitlab/gitlab --untar\n# 国内版\nhelm pull gitlab-jh/gitlab --untar\n```\n\n### 修改values.yaml\n\n```\nglobal:\n  common:\n    labels: {}\n\n  image: {}\n\n  pod:\n    labels: {}\n\n  # 改为ce，社区版\n  # 国内版不需要修改，默认：jh\n  edition: ce\n\n  gitlabVersion: \"17.2.0\"\n\n  ## https://docs.gitlab.com/charts/charts/globals#application-resource\n  application:\n    create: false\n    links: []\n    allowClusterRoles: true\n  hosts:\n    # 根据自身域名修改，写到二级域名即可\n    domain: sreok.cn\n    hostSuffix:\n    https: true\n    externalIP:\n    ssh:\n    gitlab: {}\n    minio: {}\n    registry: {}\n    tls: {}\n    smartcard: {}\n    kas: {}\n    pages: {}\n  ingress:\n    apiVersion: \"\"\n    # 改为false，已经安装过不需要安装\n    configureCertmanager: false\n    useNewIngressForCerts: false\n    provider: nginx\n    # 改为ingressClass名称\n    class: nginx\n    annotations: {}\n    enabled: true\n    tls:\n      # 已存在的自签名证书，secret\n      enabled: true\n      secretName: devops-tls\n    path: /\n    pathType: Prefix\n\n...\n...\n...\n\ncertmanager:\n  # 关闭certmanager下载，因为我已经安装过了\n  installCRDs: false\n  nameOverride: certmanager\n  # 关闭certmanager下载，因为我已经安装过了\n  install: false\n  rbac:\n    create: true\n\nnginx-ingress: &nginx-ingress\n  # 关闭nginx-ingress下载\n  enabled: false\n  tcpExternalConfig: \"true\"\n  controller: &nginx-ingress-controller\n    addHeaders:\n      Referrer-Policy: strict-origin-when-cross-origin\n    config: &nginx-ingress-controller-config\n      annotation-value-word-blocklist: \"load_module,lua_package,_by_lua,location,root,proxy_pass,serviceaccount,{,},',\\\"\"\n      hsts: \"true\"\n      hsts-include-subdomains: \"false\"\n      hsts-max-age: \"63072000\"\n      server-name-hash-bucket-size: \"256\"\n      use-http2: \"true\"\n      ssl-ciphers: \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4\"\n      ssl-protocols: \"TLSv1.3 TLSv1.2\"\n      server-tokens: \"false\"\n      # Configure smaller defaults for upstream-keepalive-*, see https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration\n      upstream-keepalive-connections: 100 # Limit of 100 held-open connections\n      upstream-keepalive-time:        30s # 30 second limit for connection reuse\n      upstream-keepalive-timeout:       5 # 5 second timeout to hold open idle connections\n      upstream-keepalive-requests:   1000 # 1000 requests per connection, before recycling\n\n...\n...\n...\nprometheus:\n  # 关闭prometheus下载 \n  install: false\n  rbac:\n    create: true\n  alertmanager:\n    enabled: false\n  alertmanagerFiles:\n    alertmanager.yml: {}\n  kubeStateMetrics:\n    enabled: false\n  nodeExporter:\n    enabled: false\n  pushgateway:\n    enabled: false\n  server:\n    retention: 15d\n    strategy:\n      type: Recreate\n    image:\n      tag: v2.38.0\n\n...\n...\n...\ngitlab-runner:\n  # 使用自签名证书时， gitlab-runner 图表无法正常运行。参考链接：https://docs.gitlab.com/charts/installation/tls.html#option-4-use-auto-generated-self-signed-wildcard-certificate\n  # 关闭gitlab-runner下载\n  install: false\n  rbac:\n    create: true\n  runners:\n    locked: false\n    secret: \"nonempty\"\n    config: |\n      [[runners]]\n        [runners.kubernetes]\n        image = \"ubuntu:22.04\"\n        {{- if .Values.global.minio.enabled }}\n        [runners.cache]\n          Type = \"s3\"\n          Path = \"gitlab-runner\"\n          Shared = true\n          [runners.cache.s3]\n            ServerAddress = {{ include \"gitlab-runner.cache-tpl.s3ServerAddress\" . }}\n            BucketName = \"runner-cache\"\n            BucketLocation = \"us-east-1\"\n            Insecure = false\n        {{ end }}\n  podAnnotations:\n    gitlab.com/prometheus_scrape: \"true\"\n    gitlab.com/prometheus_port: 9252\n...\n...\n...\n```\n\n> 中文注释处修改。\n\n#### 安装\n\n```\ncd gitlab/\nhelm upgrade --install -n devops gitlab .\n```\n\n#### 获取密码\n\n```\nkubectl get secret  -n devops gitlab-gitlab-initial-root-password -ojsonpath='{.data.password}' | base64 --decode ; echo\n\n# 或者\nkubectl get secrets -n devops gitlab-gitlab-initial-root-password  -o yaml\n# base64 -d 解密password\n```\n\n#### 客户端导入自签证书\n\n```\ngit config --global http.sslCAInfo /path/to/ca.crt\n```\n","tags":["DevOps","GitLab"],"categories":["云原生"]},{"title":"【DevOps】Jenkins基于Kubernetes创建动态构建池","url":"/2023/09/07/【DevOps】Jenkins基于Kubernetes创建动态构建池/","content":"\n### 安装插件\n\n* Kubernetes\n\n* GitLab\n\n### 添加GitLab密钥\n\n1\\. GitLab ==> 创建jenkins用户\n\n2\\. 进入jenkins个人中心 ==> 创建token\n\n![](/images/image-gujn.png)\n\n3\\. jenkins ==> 凭证管理 ==> 系统 ==> 全局凭证 ==> 添加凭证 ==> 类型（GitLab API token）\n\n![](/images/image-sgvg.png)\n\n4. 创建kubernetes凭证（部署用）\n\n![](/images/image-tpuc.png)\n\n### 自定义构建镜像\n\n```\nFROM rockylinux:9.3\n\n# ca.crt是https的ca证书，用于获取gitlab代码和harbor镜像\nCOPY ca.crt /etc/pki/ca-trust/source/anchors/\n# 部署到kubernetes时使用\nCOPY kubectl /usr/local/bin\n# 部署helm使用\nCOPY helm /usr/local/bin\n# 启动时使用\nCOPY jenkins-agent /usr/local/bin\n# go环境\nCOPY build/go1.22.5.linux-amd64.tar.gz /tmp\n# java环境\nCOPY build/openjdk-22.0.2_linux-x64_bin.tar.gz /tmp\n# workdir时会创建这个目录\nWORKDIR /usr/share/jenkins\nCOPY agent.jar /usr/share/jenkins\n\n# 安装基础包、解压环境、信任证书\nRUN yum -y install yum-utils git wget vim && \\\n    tar -C /usr/local -xzf /tmp/go1.22.5.linux-amd64.tar.gz && \\\n    tar -C /usr/local -xzf /tmp/openjdk-22.0.2_linux-x64_bin.tar.gz && \\\n    update-ca-trust\n\n# 设置环境变量\nENV JAVA_HOME=/usr/local/jdk-22.0.2\nENV PATH=$PATH:/usr/local/go/bin:/usr/local/jdk-22.0.2/bin\nENV GOPATH=/usr/share/go\nENV GO111MODULE=on\nENV GOPRIVATE=gitee.cn\nENV GOPROXY=https://goproxy.cn,direct\n\n# 进入镜像时的目录，这里与一会agent要设置的workspaces一致，是为了排查时方便，这个无所谓。\nWORKDIR /home/jenkins/agent\nENTRYPOINT [\"/usr/local/bin/jenkins-agent\"]\n```\n\n> jenkins-agent最后一行改为：\n>\n> ```\n> exec $JAVA_BIN $JAVA_OPTIONS -jar /usr/share/jenkins/agent.jar $SECRET $AGENT_NAME $TUNNEL $URL $WORKDIR $WEB_SOCKET $DIRECT $PROTOCOLS $INSTANCE_IDENTITY \"$@\"\n> ```\n>\n> 去掉$REMOTING\\_OPTS，因为启动时jenkins传的参数无效，且用不上。\n>\n> 完整的jenkins-agent脚本\n>\n> ```\n> #!/usr/bin/env sh\n>\n> # The MIT License\n> #\n> #  Copyright (c) 2015-2020, CloudBees, Inc.\n> #\n> #  Permission is hereby granted, free of charge, to any person obtaining a copy\n> #  of this software and associated documentation files (the \"Software\"), to deal\n> #  in the Software without restriction, including without limitation the rights\n> #  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n> #  copies of the Software, and to permit persons to whom the Software is\n> #  furnished to do so, subject to the following conditions:\n> #\n> #  The above copyright notice and this permission notice shall be included in\n> #  all copies or substantial portions of the Software.\n> #\n> #  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n> #  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n> #  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n> #  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n> #  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n> #  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n> #  THE SOFTWARE.\n>\n> # Usage jenkins-agent.sh [options] -url http://jenkins -secret [SECRET] -name [AGENT_NAME]\n> # Optional environment variables :\n> # * JENKINS_JAVA_BIN : Java executable to use instead of the default in PATH or obtained from JAVA_HOME\n> # * JENKINS_JAVA_OPTS : Java Options to use for the remoting process, otherwise obtained from JAVA_OPTS\n> # * REMOTING_OPTS : Generic way to pass additional CLI options to agent.jar (see -help)\n> #\n> # Deprecated environment variables (prefer setting REMOTING_OPTS)\n> # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network\n> # * JENKINS_URL : alternate jenkins URL\n> # * JENKINS_SECRET : agent secret, if not set as an argument\n> # * JENKINS_AGENT_NAME : agent name, if not set as an argument\n> # * JENKINS_AGENT_WORKDIR : agent work directory, if not set by optional parameter -workDir\n> # * JENKINS_WEB_SOCKET: true if the connection should be made via WebSocket rather than TCP\n> # * JENKINS_DIRECT_CONNECTION: Connect directly to this TCP agent port, skipping the HTTP(S) connection parameter download.\n> #                              Value: \"<HOST>:<PORT>\"\n> # * JENKINS_INSTANCE_IDENTITY: The base64 encoded InstanceIdentity byte array of the Jenkins controller. When this is set,\n> #                              the agent skips connecting to an HTTP(S) port for connection info.\n> # * JENKINS_PROTOCOLS:         Specify the remoting protocols to attempt when instanceIdentity is provided.\n>\n> if [ $# -eq 1 ] && [ \"${1#-}\" = \"$1\" ] ; then\n>\n> \t# if `docker run` only has one arguments and it is not an option as `-help`, we assume user is running alternate command like `bash` to inspect the image\n> \texec \"$@\"\n>\n> else\n>\n> \t# if -tunnel is not provided, try env vars\n> \tcase \"$@\" in\n> \t\t*\"-tunnel \"*) ;;\n> \t\t*)\n> \t\tif [ ! -z \"$JENKINS_TUNNEL\" ]; then\n> \t\t\tTUNNEL=\"-tunnel $JENKINS_TUNNEL\"\n> \t\tfi ;;\n> \tesac\n>\n> \t# if -workDir is not provided, try env vars\n> \tif [ ! -z \"$JENKINS_AGENT_WORKDIR\" ]; then\n> \t\tcase \"$@\" in\n> \t\t\t*\"-workDir\"*) echo \"Warning: Work directory is defined twice in command-line arguments and the environment variable\" ;;\n> \t\t\t*)\n> \t\t\tWORKDIR=\"-workDir $JENKINS_AGENT_WORKDIR\" ;;\n> \t\tesac\n> \tfi\n>\n> \tif [ -n \"$JENKINS_URL\" ]; then\n> \t\tURL=\"-url $JENKINS_URL\"\n> \tfi\n>\n> \tif [ -n \"$JENKINS_NAME\" ]; then\n> \t\tJENKINS_AGENT_NAME=\"$JENKINS_NAME\"\n> \tfi\n>\n> \tif [ \"$JENKINS_WEB_SOCKET\" = true ]; then\n> \t\tWEB_SOCKET=-webSocket\n> \tfi\n>\n> \tif [ -n \"$JENKINS_PROTOCOLS\" ]; then\n> \t\tPROTOCOLS=\"-protocols $JENKINS_PROTOCOLS\"\n> \tfi\n>\n> \tif [ -n \"$JENKINS_DIRECT_CONNECTION\" ]; then\n> \t\tDIRECT=\"-direct $JENKINS_DIRECT_CONNECTION\"\n> \tfi\n>\n> \tif [ -n \"$JENKINS_INSTANCE_IDENTITY\" ]; then\n> \t\tINSTANCE_IDENTITY=\"-instanceIdentity $JENKINS_INSTANCE_IDENTITY\"\n> \tfi\n>\n> \tif [ \"$JENKINS_JAVA_BIN\" ]; then\n> \t\tJAVA_BIN=\"$JENKINS_JAVA_BIN\"\n> \telse\n> \t\t# if java home is defined, use it\n> \t\tJAVA_BIN=\"java\"\n> \t\tif [ \"$JAVA_HOME\" ]; then\n> \t\t\tJAVA_BIN=\"$JAVA_HOME/bin/java\"\n> \t\tfi\n> \tfi\n>\n> \tif [ \"$JENKINS_JAVA_OPTS\" ]; then\n> \t\tJAVA_OPTIONS=\"$JENKINS_JAVA_OPTS\"\n> \telse\n> \t\t# if JAVA_OPTS is defined, use it\n> \t\tif [ \"$JAVA_OPTS\" ]; then\n> \t\t\tJAVA_OPTIONS=\"$JAVA_OPTS\"\n> \t\tfi\n> \tfi\n>\n> \t# if both required options are defined, do not pass the parameters\n> \tif [ -n \"$JENKINS_SECRET\" ]; then\n> \t\tcase \"$@\" in\n> \t\t\t*\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;;\n> \t\t\t*)\n> \t\t\tSECRET=\"-secret ${JENKINS_SECRET}\" ;;\n> \t\tesac\n> \tfi\n>\n> \tif [ -n \"$JENKINS_AGENT_NAME\" ]; then\n> \t\tcase \"$@\" in\n> \t\t\t*\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;;\n> \t\t\t*)\n> \t\t\tAGENT_NAME=\"-name ${JENKINS_AGENT_NAME}\" ;;\n> \t\tesac\n> \tfi\n>\n> \t#TODO: Handle the case when the command-line and Environment variable contain different values.\n> \t#It is fine it blows up for now since it should lead to an error anyway.\n>\n>         #exec $JAVA_BIN $JAVA_OPTIONS -jar /usr/share/jenkins/agent.jar $SECRET $AGENT_NAME $TUNNEL $URL $WORKDIR $WEB_SOCKET $DIRECT $PROTOCOLS $INSTANCE_IDENTITY $REMOTING_OPTS \"$@\"\n>         exec $JAVA_BIN $JAVA_OPTIONS -jar /usr/share/jenkins/agent.jar $SECRET $AGENT_NAME $TUNNEL $URL $WORKDIR $WEB_SOCKET $DIRECT $PROTOCOLS $INSTANCE_IDENTITY \"$@\"\n>\n> fi\n> ```\n\n```\n# 构建镜像\ndocker build -t harbor.basepoint.net/library/rocky_build:jdk22-go1.22.5 .\n# 上传私有仓库\ndocker push harbor.basepoint.net/library/rocky_build:jdk22-go1.22.5\n```\n\n#### （可选）docker dind镜像上传到私有仓库，加速构建时下载镜像\n\n```\ndocker pull docker:dind\ndocker tag docker:dind harbor.basepoint.net/library/docker:dind\ndocker push harbor.basepoint.net/library/docker:dind\n```\n\n### 创建云节点\n\nJenkins ==> 系统管理 ==> CLouds ==> New cloud\n\n![](/images/image-qthn.png)\n\n![](/images/image-qucu.png)\n\n### 创建Pod模版\n\n![](/images/image-capf.png)\n\n> 名称不设置默认也是jenkins-agent，\n>\n> 命名空间写构建时运行的命名空间，我这里是jenkins在同一个命名空间下（前提RBAC权限要赋予到位）\n>\n> 标签列表，pipline选择节点标签一致时才会触发这个pod模版\n\n![](/images/image-ccsk.png)\n\n> 这里使用构建的镜像，jnlp名称是固定的，否则覆盖不了默认的jnlp容器，\n>\n> 运行的命令和参数不要写，否则会替换镜像中的CMD或者ENPTYPOINT参数\n\n![](/images/image-kekn.png)\n\n> （可选）使用Docker in Docker 构建docker镜像，必须使用最高权限运行，\n>\n> 运行的命令和参数不要写，否则会替换镜像中的CMD或者ENPTYPOINT参数\n\n![](/images/image-qfoy.png)\n\n> docker容器登录harbor时使用，harbor-ca是使用trust分发的ca.crt\n>\n> ```\n> apiVersion: trust.cert-manager.io/v1alpha1\n> kind: Bundle\n> metadata:\n>   name: harbor-ca\n> spec:\n>   sources:\n>   - secret:\n>       # devops-tls是源secret（使用cert-manager创建的自签名证书），包含ca.crt\n>       name: \"devops-tls\"\n>       key: \"ca.crt\"\n>   target:\n>     secret:\n>       key: \"ca.crt\"\n> ```\n>\n> cert-manager和trust参考请转移以下文章\n>\n> 原文地址：[使用cert-manager 生成自签名证书 - Elijah Blog (](https://sreok.cn/archives/c03ad0e3-f52e-4e3e-805a-95bcf2a9b37c)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/c03ad0e3-f52e-4e3e-805a-95bcf2a9b37c)\n>\n> 原文地址：[使用trust-manager将TLS证书同步到其他命名空间 - Elijah Blog (](https://sreok.cn/archives/ed87d9b2-89b3-46df-aa33-7a647bbd6395)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/ed87d9b2-89b3-46df-aa33-7a647bbd6395)\n\n![](/images/image-ztow.png)\n\n> （可选）pod模版中获取镜像如果是私有镜像需要用到，创建如下：\n>\n> ```\n> kubectl create secret docker-registry regcred \\\n> --docker-server=harbor.basepoint.net \\\n> --docker-username=admin \\\n> --docker-password=Harbor12345 \\\n> --docker-email=admin@basepoint.net\n> -n devops \n> ```\n\n### 创建pipeline项目（golang后端）\n\n```\npipeline {\n    agent {\n        // 与pod模版设置的标签一致\n        label 'slave'\n    }\n\n    stages {\n        stage('Git Clone') {\n            steps {\n                container(\"jnlp\"){\n                    //git  branch: \"main\", credentialsId: \"gitlab-root\", url: \"http://gitlab-webservice-default.devops.svc:8181/kube-go/kube-go.git\"\n                    git  branch: \"main\", credentialsId: \"gitlab-root\", url: \"https://gitlab.basepoint.net/kube-go/kube-go.git\"\n                }\n            }\n        }\n        stage('GO Build') {\n            steps {\n                container(\"jnlp\"){\n                    echo 'go编译'\n                    sh 'go version'\n                    sh 'go mod tidy'\n                    sh 'go build -o ./build/ ./main/main.go'\n                }\n            }\n        }\n        stage('Docker Build') {\n            steps {\n                container(\"docker\"){\n                    echo '构建docker镜像'\n                    sh 'docker -v'\n                    // 登录harbor私有仓库\n                    sh 'docker login harbor.basepoint.net -u admin -p Harbor12345'\n                    // FROM镜像不使用/etc/docker/certs/下的证书，提前拉取Dockerfile中的FROM镜像\n                    sh 'docker pull harbor.basepoint.net/library/golang:1.22.5'\n                    sh 'docker build -t harbor.basepoint.net/kube-go/kube-go:v1 .'\n                }\n            }\n        }\n        stage('Docker Push') {\n            steps {\n                container(\"docker\"){\n                    echo '推送docker镜像'\n                    sh 'docker push harbor.basepoint.net/kube-go/kube-go:v1'\n                }\n            }\n        }\n    }\n    post {\n        always{\n            script{\n                println(\"流水线结束后，经常做的事情\")\n            }\n        }\n            \n        success{\n            script{\n                println(\"流水线成功后，要做的事情\")\n            }\n            \n        }\n        failure{\n            script{\n                println(\"流水线失败后，要做的事情\")\n            }\n        }\n            \n        aborted{\n            script{\n                println(\"流水线取消后，要做的事情\")\n            }\n            \n        }\n    }\n}\n```\n\n### 测试构建\n\n![](/images/image-rbsq.png)\n\n#### 查看构建之后的镜像\n\n![](/images/image-ynye.png)\n","tags":["DevOps","Jenkins"],"categories":["云原生"]},{"title":"【DevOps】Jenkins基于Kubernetes创建静态构建池","url":"/2023/09/06/【DevOps】Jenkins基于Kubernetes创建静态构建池/","content":"\n### 制作镜像\n\n```\nFROM harbor.basepoint.net/library/rockylinux:9.3\n\n\nCOPY ca.crt /etc/pki/ca-trust/source/anchors/\nCOPY kubectl /usr/local/bin\nCOPY helm /usr/local/bin\nCOPY jenkins-agent /usr/local/bin\nCOPY build/go1.22.5.linux-amd64.tar.gz /tmp\nCOPY build/openjdk-22.0.2_linux-x64_bin.tar.gz /tmp\nWORKDIR /usr/share/jenkins\nCOPY agent.jar /usr/share/jenkins\n\nRUN yum -y install yum-utils git wget vim && \\\n    #yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo && \\\n    #sed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo &&\\\n    #yum -y install docker-ce && \\\n    tar -C /usr/local -xzf /tmp/go1.22.5.linux-amd64.tar.gz && \\\n    tar -C /usr/local -xzf /tmp/openjdk-22.0.2_linux-x64_bin.tar.gz && \\\n    update-ca-trust\n\nENV JAVA_HOME=/usr/local/jdk-22.0.2\nENV PATH=$PATH:/usr/local/go/bin:/usr/local/jdk-22.0.2/bin\nENV GOPATH=/usr/share/go\nENV GO111MODULE=on\nENV GOPRIVATE=gitee.cn\nENV GOPROXY=https://goproxy.cn,direct\n\nWORKDIR /home/jenkins/agent\nENTRYPOINT [\"/usr/local/bin/jenkins-agent\"]\n```\n\n### 创建节点\n\n![](/images/image-saol.png)\n\n![](/images/image-ctey.png)\n\n![](/images/image-ckzr.png)\n\n> 记录secret和name\n\n### 创建Deployment\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-slave01\n  namespace: devops\n  labels:\n    app: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins-slave01\n  template:\n    metadata:\n      labels:\n        app: jenkins-slave01\n    spec:\n      # 污点\n      #tolerations:\n      #- effect: NoSchedule\n      #  key: no-pod\n      #  operator: Exists\n      # 运行在指定label的k8s节点\n      #nodeSelector:\n      #  jenkins: \"true\"\n      imagePullSecrets:\n        - name: regcred\n      containers:\n      - name: agent\n        image: harbor.basepoint.net/library/rocky_build:jdk22-go1.22.5\n        #imagePullPolicy: IfNotPresent\n        imagePullPolicy: Always\n        env:\n        - name: JENKINS_URL\n          value: \"http://jenkins.devops.svc.cluster.local\"\n        - name: JENKINS_SECRET\n          value: \"df59d1d6f6052defd5d102c242301a93a0698821d2557ed4bca1d7160e14ba3c\"\n        - name: JENKINS_AGENT_WORKDIR\n          value: \"/home/jenkins/agent\"\n        - name: JENKINS_AGENT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.labels['app']\n      # 以下不可用，静态节点必须写在一个容器内。\n      #- name: docker\n      #  image: harbor.basepoint.net/library/docker:dind\n      #  imagePullPolicy: Always\n      #  securityContext:\n      #    privileged: true\n      #  volumeMounts:\n      #    - name: ca\n      #      mountPath: \"/etc/docker/certs.d/\"\n      #      readOnly: true\n      #volumes:\n      #  - name: ca\n      #    secret:\n      #      secretName: harbor-ca\n      #      #secretName: devops-tls\n      #      items:\n      #        - key: ca.crt\n      #          path: harbor.basepoint.net/ca.crt\n```\n\n#### 查看日志\n\n![](/images/image-ksqm.png)\n\n> 出现Connected即可\n\n#### 查看节点状态\n\n![](/images/image-gooa.png)\n\n### 测试Pipeline\n\n```\npipeline {\n    agent {\n        node {\n            // 与节点标签对应\n            label 'jenkins-slave01'\n        }\n    }\n\n    stages {\n        stage('Hello') {\n            steps {\n                echo 'Hello World'\n                sh 'java --version'\n            }\n        }\n    }\n}\n```\n\n![](/images/image-vrgr.png)\n","tags":["DevOps","Jenkins"],"categories":["云原生"]},{"title":"【DevOps】Harbor私有镜像仓库并利用cert-manager自签名证书开启HTTPS（helm方式安装）","url":"/2023/09/05/【DevOps】Harbor私有镜像仓库并利用cert-manager自签名证书开启HTTPS（helm方式安装）/","content":"\n### **前提条件**\n\n* cert-manager\n\n* ingress-nginx\n\n* Helm 2.8.0+\n\n* Kubernetes cluster 1.10+\n\n```\n# 命名空间\nkubectl create ns devops\n```\n\n### **自签名证书**\n\n```\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: selfsigned\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: devops-ca\n  namespace: cert-manager\nspec:\n  isCA: true\n  commonName: \"*.basepoint.net\"\n  secretName: devops-selfsigned-secret\n  duration: 876000h\n  renewBefore: 8760h\n  subject:\n    countries:\n    - China\n    localities:\n    - NanJing\n    organizations:\n    - basepoint\n    organizationalUnits:\n    - devops\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    name: selfsigned\n    kind: ClusterIssuer\n    group: cert-manager.io\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: devops-issuer\nspec:\n  ca:\n    secretName: devops-selfsigned-secret\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: harbor-ca\n  namespace: devops\n  #namespace: cert-manager\nspec:\n  isCA: false\n  usages:\n    - server auth\n    - client auth\n  dnsNames:\n  - basepoint.net\n  - harbor.basepoint.net\n  commonName: \"harbor.basepoint.net\"\n  secretName: harbor-selfsigned-secret\n  duration: 876000h\n  renewBefore: 8760h\n  subject:\n    countries:\n    - China\n    localities:\n    - NanJing\n    organizations:\n    - basepoint\n    organizationalUnits:\n    - harbor\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    name: devops-issuer\n    kind: ClusterIssuer\n    group: cert-manager.io\n```\n\n> 详解，原文链接：[使用cert-manager 生成自签名证书 - Elijah Blog (](https://sreok.cn/archives/c03ad0e3-f52e-4e3e-805a-95bcf2a9b37c)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/c03ad0e3-f52e-4e3e-805a-95bcf2a9b37c)\n\n#### **查看tls文件**\n\n```\n[root@k8s-master01 cert-manager]# kubectl get secrets -n devops harbor-selfsigned-secret\nNAME                       TYPE                DATA   AGE\nharbor-selfsigned-secret   kubernetes.io/tls   3      18s\n```\n\n### **安装Harbor**\n\n```\nhelm repo add harbor https://helm.goharbor.io\nhelm pull harbor/harbor --untar\n```\n\n#### **修改values.yaml**\n\n```\ncd ./harbor\nvim values.yaml\n```\n\n```\nexpose:\n  type: ingress\n  tls:\n    enabled: true\n    # 改成secret，使用刚才生成的自签名证书\n    certSource: secret\n    auto:\n      commonName: \"\"\n    secret:\n      # 改为自签名证书secret名\n      secretName: \"harbor-selfsigned-secret\"\n  ingress:\n    hosts:\n      # harbor域名\n      core: harbor.basepoint.net\n    controller: default\n    kubeVersionOverride: \"\"\n    className: \"nginx\"\n    annotations:nginx lines below\n      ingress.kubernetes.io/ssl-redirect: \"true\"\n      ingress.kubernetes.io/proxy-body-size: \"0\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    labels: {}\n  clusterIP:\n    name: harbor\n    staticClusterIP: \"\"\n    ports:\n      # The service port Harbor listens on when serving HTTP\n      httpPort: 80\n      # The service port Harbor listens on when serving HTTPS\n      httpsPort: 443\n    # Annotations on the ClusterIP service\n    annotations: {}\n    # ClusterIP-specific labels\n    labels: {}\n  nodePort:\n    # The name of NodePort service\n    name: harbor\n    ports:\n      http:\n        # The service port Harbor listens on when serving HTTP\n        port: 80\n        # The node port Harbor listens on when serving HTTP\n        nodePort: 30002\n      https:\n        # The service port Harbor listens on when serving HTTPS\n        port: 443\n        # The node port Harbor listens on when serving HTTPS\n        nodePort: 30003\n    # Annotations on the nodePort service\n    annotations: {}\n    # nodePort-specific labels\n    labels: {}\n  loadBalancer:\n    # The name of LoadBalancer service\n    name: harbor\n    # Set the IP if the LoadBalancer supports assigning IP\n    IP: \"\"\n    ports:\n      # The service port Harbor listens on when serving HTTP\n      httpPort: 80\n      # The service port Harbor listens on when serving HTTPS\n      httpsPort: 443\n    # Annotations on the loadBalancer service\n    annotations: {}\n    # loadBalancer-specific labels\n    labels: {}\n    sourceRanges: []\n​\n# The external URL for Harbor core service. It is used to\n# 1) populate the docker/helm commands showed on portal\n# 2) populate the token service URL returned to docker client\n#\n# Format: protocol://domain[:port]. Usually:\n# 1) if \"expose.type\" is \"ingress\", the \"domain\" should be\n# the value of \"expose.ingress.hosts.core\"\n# 2) if \"expose.type\" is \"clusterIP\", the \"domain\" should be\n# the value of \"expose.clusterIP.name\"\n# 3) if \"expose.type\" is \"nodePort\", the \"domain\" should be\n# the IP address of k8s node\n#\n# If Harbor is deployed behind the proxy, set it as the URL of proxy\n# 改为\nexternalURL: https://harbor.basepoint.net\n​\n# The persistence is enabled by default and a default StorageClass\n# is needed in the k8s cluster to provision volumes dynamically.\n# Specify another StorageClass in the \"storageClass\" or set \"existingClaim\"\n# if you already have existing persistent volumes to use\n#\n# For storing images and charts, you can also use \"azure\", \"gcs\", \"s3\",\n# \"swift\" or \"oss\". Set it in the \"imageChartStorage\" section\npersistence:\n  enabled: true\n  # Setting it to \"keep\" to avoid removing PVCs during a helm delete\n  # operation. Leaving it empty will delete PVCs after the chart deleted\n  # (this does not apply for PVCs that are created for internal database\n  # and redis components, i.e. they are never deleted automatically)\n  resourcePolicy: \"keep\"\n  persistentVolumeClaim:\n    registry:\n      existingClaim: \"\"\n      # storageClass根据实际更改，不设置代表使用默认的storageClass\n      storageClass: \"\"\n      subPath: \"\"\n      accessMode: ReadWriteOnce\n      # 大小根据需求更改\n      size: 5Gi\n      annotations: {}\n    jobservice:\n      jobLog:\n        existingClaim: \"\"\n        storageClass: \"\"\n        subPath: \"\"\n        accessMode: ReadWriteOnce\n        size: 1Gi\n        annotations: {}\n    # If external database is used, the following settings for database will\n    # be ignored\n    database:\n      existingClaim: \"\"\n      storageClass: \"\"\n      subPath: \"\"\n      accessMode: ReadWriteOnce\n      size: 1Gi\n      annotations: {}\n    # If external Redis is used, the following settings for Redis will\n    # be ignored\n    redis:\n      existingClaim: \"\"\n      storageClass: \"\"\n      subPath: \"\"\n      accessMode: ReadWriteOnce\n      size: 1Gi\n      annotations: {}\n    trivy:\n      existingClaim: \"\"\n      storageClass: \"\"\n      subPath: \"\"\n      accessMode: ReadWriteOnce\n      size: 5Gi\n      annotations: {}\n  imageChartStorage:\n    type: filesystem\n    filesystem:\n      rootdirectory: /storage\n      #maxthreads: 100\n    azure:\n      accountname: accountname\n      accountkey: base64encodedaccountkey\n      container: containername\n      #realm: core.windows.net\n      # To use existing secret, the key must be AZURE_STORAGE_ACCESS_KEY\n      existingSecret: \"\"\n    gcs:\n      bucket: bucketname\n      # The base64 encoded json file which contains the key\n      encodedkey: base64-encoded-json-key-file\n      #rootdirectory: /gcs/object/name/prefix\n      #chunksize: \"5242880\"\n      # To use existing secret, the key must be GCS_KEY_DATA\n      existingSecret: \"\"\n      useWorkloadIdentity: false\n    s3:\n      # Set an existing secret for S3 accesskey and secretkey\n      # keys in the secret should be REGISTRY_STORAGE_S3_ACCESSKEY and REGISTRY_STORAGE_S3_SECRETKEY for registry\n      #existingSecret: \"\"\n      region: us-west-1\n      bucket: bucketname\n      #accesskey: awsaccesskey\n      #secretkey: awssecretkey\n      #regionendpoint: http://myobjects.local\n      #encrypt: false\n      #keyid: mykeyid\n      #secure: true\n      #skipverify: false\n      #v4auth: true\n      #chunksize: \"5242880\"\n      #rootdirectory: /s3/object/name/prefix\n      #storageclass: STANDARD\n      #multipartcopychunksize: \"33554432\"\n      #multipartcopymaxconcurrency: 100\n      #multipartcopythresholdsize: \"33554432\"\n    swift:\n      authurl: https://storage.myprovider.com/v3/auth\n      username: username\n      password: password\n      container: containername\n      # keys in existing secret must be REGISTRY_STORAGE_SWIFT_PASSWORD, REGISTRY_STORAGE_SWIFT_SECRETKEY, REGISTRY_STORAGE_SWIFT_ACCESSKEY\n      existingSecret: \"\"\n      #region: fr\n      #tenant: tenantname\n      #tenantid: tenantid\n      #domain: domainname\n      #domainid: domainid\n      #trustid: trustid\n      #insecureskipverify: false\n      #chunksize: 5M\n      #prefix:\n      #secretkey: secretkey\n      #accesskey: accesskey\n      #authversion: 3\n      #endpointtype: public\n      #tempurlcontainerkey: false\n      #tempurlmethods:\n    oss:\n      accesskeyid: accesskeyid\n      accesskeysecret: accesskeysecret\n      region: regionname\n      bucket: bucketname\n      # key in existingSecret must be REGISTRY_STORAGE_OSS_ACCESSKEYSECRET\n      existingSecret: \"\"\n      #endpoint: endpoint\n      #internal: false\n      #encrypt: false\n      #secure: true\n      #chunksize: 10M\n      #rootdirectory: rootdirectory\n​\n​\nexistingSecretAdminPasswordKey: HARBOR_ADMIN_PASSWORD\n# 默认密码，按需修改\nharborAdminPassword: \"Harbor12345\"\n​\n...\n# 下面的配置按需求修改，我这里没改动就不贴了，太长了。\n```\n\n> 中文注释都是改动的位置\n\n#### **安装**\n\n```\nhelm upgrade --install harbor -n devops .\n```\n\n```\n[root@k8s-h3c-master01 ~]# kubectl get pod -n devops\nNAME                                READY   STATUS    RESTARTS        AGE\nharbor-core-659b9b95fc-7l69q        1/1     Running   0               21h\nharbor-database-0                   1/1     Running   0               3d17h\nharbor-jobservice-8b5f47bd5-lcdhw   1/1     Running   0               21h\nharbor-portal-67749b88d8-tcjqd      1/1     Running   0               3d17h\nharbor-redis-0                      1/1     Running   0               3d17h\nharbor-registry-687cc8d459-5w9l8    2/2     Running   0               21h\nharbor-trivy-0                      1/1     Running   0               3d17h\n```\n\n#### **查看ingress**\n\n```\n[root@k8s-h3c-master01 ~]# kubectl get ingress -n devops\nNAME             CLASS   HOSTS                  ADDRESS        PORTS     AGE\nharbor-ingress   nginx   harbor.basepoint.net   172.25.2.204   80, 443\n```\n\n#### **添加hosts**\n\n`系统添加，略过。`\n\n#### **查看证书**\n\n![](/images/image-zmhl.png)\n\n### **系统颁发证书**\n\n![img](https://sreok.cn/images/image-iydt.png)\n\n```\n# basse64解码并保存到电脑\necho \"connect in ca.crt\" | base64 -d\n```\n\n![img](https://sreok.cn/images/image-fhla.png)\n\n#### **MacOS 导入证书**\n\n![img](https://sreok.cn/images/image-mwml.png)\n\n![img](https://sreok.cn/images/image-aqcb.png)\n\n![img](https://sreok.cn/images/image-ybtx.png)\n\n#### **Linux 导入证书（全部k8s节点操作）**\n\n安装ca-certificates，一般系统是自带的\n\n```\nyum install ca-certificates\n```\n\n将证书放置以下目录\n\n```\n/etc/pki/ca-trust/source/anchors/\n```\n\n执行信任此证书\n\n```\nupdate-ca-trust\n```\n\n> 此操作会更新以下文件，而不是只是tls-ca-bundle.pem\n>\n> ```\n> /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\n> /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\n> /etc/pki/ca-trust/extracted/pem/email-ca-bundle.pem\n> /etc/pki/ca-trust/extracted/pem/objsign-ca-bundle.pem  \n> ```\n>\n> 删除时在`/etc/pki/ca-trust/source/anchors/`文件中删掉对应的ca证书，重新执行`update-ca-trust`即可。\n\n### **创建imagePullSecrets**\n\n#### **方式一：在命令行上提供凭据来创建 Secret**\n\n官方文档：[从私有仓库拉取镜像 | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line)\n\n```\nkubectl create secret docker-registry regcred \\\n--docker-server=harbor.basepoint.net \\\n--docker-username=admin \\\n--docker-password=Harbor12345 \\\n--docker-email=admin@basepoint.net\n# -n 命名空间 \n```\n\n#### **方式二：创建一个基于现有凭据的 Secret**\n\n官方文档：[从私有仓库拉取镜像 | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials)\n\n选择其中一个节点安装docker\n\n```\nyum -y install docker-ce\n```\n\n登录docker\n\n```\ndocker login harbor.basepoint.net -u admin -p Harbor12345\n```\n\n登录成功后会创建一个文件：`~/.docker/config.json`\n\n```\nkubectl create secret generic harbor-admin \\\n--from-file=.dockerconfigjson=~/.docker/config.json \\\n--type=kubernetes.io/dockerconfigjson\n# -n 命名空间 \n```\n\n### **测试拉取镜像**\n\n将library设置为私有仓库\n\n![](/images/image-oiwb.png)\n\n#### **拉取私有仓库镜像**\n\n```\ncat > nginx.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: harbor.basepoint.net/library/nginx:1.25.0\n        # 每次启动都拉取\n        imagePullPolicy: Always\n      # 注意命名空间，如果要安装在其他命名空间，就不存在这个seccret，则需要重新创建\n      imagePullSecrets:\n      - name: harbor-admin\nEOF\n```\n\n```\nkubectl apply -f nginx.yaml\n```\n\n#### **成功即可**\n\n```\n[root@k8s-h3c-master01 ~]# kubectl get pod\nNAME                         READY   STATUS    RESTARTS   AGE\nnginx-67d8d49fb6-4vwk9       1/1     Running   0          59m\n```\n\n![](/images/image-uuoq.png)\n","tags":["Harbor","DevOps"],"categories":["云原生"]},{"title":"【故障排查】kube-vip 与 MetalLB LoadBalancer Layer2 ARP冲突","url":"/2023/07/10/【故障排查】kube-vip 与 MetalLB LoadBalancer Layer2 ARP冲突/","content":"\n### 问题描述\n\n在启动了kube-vip Layer2模式的kubernetes HA集群，安装了MetalLB并使用Layer2模式转发service LoadBalancer到集群外部时，LoadBalancer IP不能被访问。\n\n#### 原因（猜测，日志无报错）\n\nARP表冲突，kube-vip默认启用了Service LoadBalancer模式（后面文章再研究），导致MetalLB申请的IP不能被ARP解析。\\\nkube-vip官网找到以下内容：\n\n![](/images/image-hbrc.png)\n\n### 解决方案\n\n关闭kube-vip的LoadBalancer，只使用MetalLB提供的LB能力。\n\n```\nvim /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n![](/images/image-xlnm.png)重启所有master的kubelet\n\n```\nsystemctl restart kubelet\n```\n\n### 删除vip重新生成\n\n`不确定有没有作用，先写上`\n\n```\n# 所有master都执行\nip addr del dev ens192 10.20.13.111/32\n```\n\n> ens192：网卡\n>\n> 10.20.13.111/32：vip\n\n### 测试\n\n经过以上步骤，此时再次访问vip，还是不通，原因我猜测是ip已经被写入ARP表还未过期，可以新增一个LoadBalancer模式的Service\n\n#### 方法一：\n\n`将不通的LoadBalancer IP作为源地址发送请求到k8s节，以此来刷新ARP表`\n\n```\narping -I ens192 -s 10.20.13.231 10.20.13.90\n```\n\n> 10.20.13.90：master ip其中之一（任意节点应该也可以）\n>\n> 10.20.13.231：不通的LoadBalancer IP\n\n#### 方法二：\n\n`` 新增service，使用一个之前未使用过的LoadBalancer` ``\n\n示例：\n\n```\nkubectl expose deployment nginx-deployment --type=LoadBalancer --port=80 --name=nginx-lb\n```\n\n> nginx-deployment：deployment名称\n","tags":["kube-vip","故障排除","MetalLB"],"categories":["云原生"]},{"title":"【监控】prometheus通过alertmanager集成钉钉告警","url":"/2023/07/02/【监控】prometheus通过alertmanager集成钉钉告警/","content":"\n# prometheus通过alertmanager集成钉钉告警\n\n下载二进制包：[Download | Prometheus](https://prometheus.io/download/)\n\n## 前提条件\n\n- Prometheus\n- alertmanager\n- 设置告警规则\n- 已有监控节点/服务\n\n### 创建告警机器人\n\n#### 创建群聊\n\n![image-20240418114025103](/images/image-hpxf.png)\n\n#### 添加机器人\n\n![image-20240418114122619](/images/image-rddd.png)\n\n#### 配置安全设置为加签，并记录Webhook和加签密钥\n\n![image-20240419143554146](/images/image-jmzx.png)\n\n\n\n### 安装dingtalk-webhook\n\n下载地址：[Releases · timonwong/prometheus-webhook-dingtalk (github.com)](https://github.com/timonwong/prometheus-webhook-dingtalk/releases)\n\n#### 安装\n\n```\ntar zxvf prometheus-webhook-dingtalk-2.1.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/prometheus-webhook-dingtalk-2.1.0.linux-amd64 /usr/local/prometheus/dingtalk\n```\n\n#### 修改配置文件\n\n##### 配置告警消息\n\n```\nvim /usr/local/prometheus/dingtalk/default.tmpl\n```\n\n```\n{{ define \"__subject\" }}\n[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}]\n{{ end }}\n\n\n{{ define \"__alert_list\" }}{{ range . }}\n---\n\n{{ if .Labels.owner }}@{{ .Labels.owner }}{{ end }}\n\n**告警主题**: {{ .Annotations.summary }}\n\n**告警类型**: {{ .Labels.alertname }}\n\n**告警级别**: {{ .Labels.severity }} \n\n**告警主机**: {{ .Labels.instance }} \n\n**告警信息**: {{ index .Annotations \"description\" }}\n\n**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}\n{{ end }}{{ end }}\n\n{{ define \"__resolved_list\" }}{{ range . }}\n---\n\n{{ if .Labels.owner }}@{{ .Labels.owner }}{{ end }}\n\n**告警主题**: {{ .Annotations.summary }}\n\n**告警类型**: {{ .Labels.alertname }} \n\n**告警级别**: {{ .Labels.severity }}\n\n**告警主机**: {{ .Labels.instance }}\n\n**告警信息**: {{ index .Annotations \"description\" }}\n\n**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}\n\n**恢复时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.EndsAt) \"Asia/Shanghai\" }}\n{{ end }}{{ end }}\n\n\n{{ define \"default.title\" }}\n{{ template \"__subject\" . }}\n{{ end }}\n\n{{ define \"default.content\" }}\n{{ if gt (len .Alerts.Firing) 0 }}\n**====侦测到{{ .Alerts.Firing | len  }}个故障====**\n\n{{ template \"__alert_list\" .Alerts.Firing }}\n---\n\n{{ end }}\n\n{{ if gt (len .Alerts.Resolved) 0 }}\n**====恢复{{ .Alerts.Resolved | len  }}个故障====**\n{{ template \"__resolved_list\" .Alerts.Resolved }}\n{{ end }}\n{{ end }}\n\n\n{{ define \"ding.link.title\" }}{{ template \"default.title\" . }}{{ end }}\n{{ define \"ding.link.content\" }}{{ template \"default.content\" . }}{{ end }}\n{{ template \"default.title\" . }}\n{{ template \"default.content\" . }}\n```\n\n##### 钉钉机器人集成\n\n```\nvim /usr/local/prometheus/dingtalk/config.yml\n```\n\n```\n## Request timeout\n# timeout: 5s\n\n## Uncomment following line in order to write template from scratch (be careful!)\n#no_builtin_template: true\n\n## Customizable templates path\ntemplates:\n  - /usr/local/prometheus/dingtalk/default.tmpl\n\n## You can also override default template using `default_message`\n## The following example to use the 'legacy' template from v0.3.0\n#default_message:\n#  title: '{{ template \"legacy.title\" . }}'\n#  text: '{{ template \"legacy.content\" . }}'\n\n## Targets, previously was known as \"profiles\"\ntargets:\n  webhook1:\n    # token\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxxxxxxxxxxx\n    # 加签密钥\n    secret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n\n\n\n\n### 修改alertmanager配置文件\n\n```\nvim /usr/local/prometheus/alertmanager/alertmanager.yml\n```\n\n```\nroute:\n  group_by: ['dingtalk']\n  group_wait: 1s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'dingtalk.webhook1'\n  routes:\n  - receiver: \"dingtalk.webhook1\"\n    match_re:\n      altername: \".*\"\nreceivers:\n  - name: 'dingtalk.webhook1'\n    webhook_configs:\n      - url: 'http://localhost:8060/dingtalk/webhook1/send'\n        send_resolved: true\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n```\n\n#### 重启alertmanager\n\n```\nsystemctl restart alertmanager\n```\n\n#### 验证\n\n访问alertmanager地址：`http://ip:9093/#/status`，验证配置生效。\n\n![image-20240419143915353](/images/image-vzdl.png)\n\n\n\n#### 测试\n\n找一个节点或服务，我这里停掉当前节点的node_exporter服务\n\n```\nsystemctl stop node_exporter\n```\n\n触发以下告警规则\n\n```\n  - alert: 服务器宕机\n    expr: up == 0\n    for: 1s\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 服务器宕机, 请尽快处理!\"\n      description: \"{{$labels.instance}} 服务器node_exporter服务被关闭,当前状态{{ $value }}. \"\n```\n\n稍等一下，收到\n\n![image-20240419140237250](/images/image-tkco.png)\n","tags":["prometheus"],"categories":["监控"]},{"title":"【笔记】使用kube-vip实现LoadBalancer（Layer2模式）","url":"/2023/06/04/【笔记】使用kube-vip实现LoadBalancer（Layer2模式）/","content":"\n### 前提条件\n\n* kube-vip\n\n### 安装云控制管理器\n\n官方文档：[On-Premises (kube-vip-cloud-controller) | kube-vip](https://kube-vip.io/docs/usage/cloud-provider/#using-the-kube-vip-cloud-provider)\n\n```\nwget https://github.com/kube-vip/kube-vip-cloud-provider/blob/v0.0.10/manifest/kube-vip-cloud-controller.yaml\nkubectl apply -f kube-vip-cloud-controller.yaml\n```\n\n### 创建CIDR\n\n#### 快速配置，创建全局LB网段\n\n```\nkubectl create configmap -n kube-system kubevip --from-literal range-global=172.25.2.51-172.25.2.100\n```\n\n#### 高级配置\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kubevip\n  namespace: kube-system\ndata:\n  cidr-default: 192.168.0.200/29                      # default命名空间CIDR\n  range-development: 192.168.0.210-192.168.0.219      # development命名空间IP池\n  cidr-finance: 192.168.0.220/29,192.168.0.230/29     # finance命名空间CIDR\n  cidr-global: 192.168.0.240/29                       # 全局CIDR\n```\n\n> 如果service不使用kube-vip提供的LB，在`servicemetadata.annotations`处配置`kube-vip.io/ignore=true` 告诉kube-vip 忽略掉这个服务\n\n### 设置service为LoadBalancer\n\n```\nkubectl edit svc -n namespace svc\n```\n\n![](/images/image-oezt.png)\n\n![](/images/image-sfal.png)\n\n### 配合MetalLB使用kube-vip\n\n原文地址：[【故障排查】kube-vip 与 MetalLB LoadBalancer Layer2 ARP冲突 - Elijah Blog (](https://sreok.cn/archives/81eb2a31-1b61-4331-bf40-098cbd20624c)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/81eb2a31-1b61-4331-bf40-098cbd20624c)\n\n原文地址：[使用MetalLB为k8s集群提供负载均衡服务（Layer 2模式） - Elijah Blog (](https://sreok.cn/archives/f0b5ce93-4404-4ed3-a3b8-a0745ff8cc23)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/f0b5ce93-4404-4ed3-a3b8-a0745ff8cc23)\n","tags":["kube-vip","LoadBalancer"],"categories":["Linux"]},{"title":"【笔记】使用Slim镜像瘦身工具实现精简Docker镜像","url":"/2023/06/02/【笔记】使用Slim镜像瘦身工具实现精简Docker镜像/","content":"\n### **前提条件**\n\n* 安装Docker\n\n* 上传需要缩小的镜像到docker\n\n### **安装slim**\n\n下载地址：[Releases · slimtoolkit/slim · GitHub](https://github.com/slimtoolkit/slim/releases)\n\n```\ntar zxvf dist_linux.tar.gz\ncd dist_linux/\nmv * /usr/local/bin/\n```\n\n### **使用**\n\n#### **构建镜像**\n\n```\nslim build nginx:latest\n```\n\n> slim build <镜像名/镜像id>\n\n![](/images/image-iggq.png)\n\n#### **查看镜像**\n\n```\ndocker images\n```\n\n![](/images/image-lgek.png)\n\n> 镜像缩小至13.3MB\n\n#### **测试运行**\n\n```\ndocker run -d -p 8082:80 --name nginx nginx.slim\n```\n\n![](/images/image-znjm.png)\n\n> 正常运行\n","tags":["Docker"],"categories":["Linux"]},{"title":"【笔记】使用MetalLB为k8s集群提供负载均衡服务（Layer 2模式）","url":"/2023/06/01/【笔记】使用MetalLB为k8s集群提供负载均衡服务（Layer 2模式）/","content":"\n官方文档：[*MetalLB, bare metal load-balancer for Kubernetes (*](https://metallb.universe.tf/installation/)[*universe.tf*](http://universe.tf)[*)*](https://metallb.universe.tf/installation/)\n\n### **1、kube-proxy ipvs模式开启ARP**\n\n```\nkubectl edit configmap -n kube-system kube-proxy\n```\n\n```\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: \"ipvs\"\nipvs:\n  strictARP: true\n```\n\n或者手动设置\n\n```\n# see what changes would be made, returns nonzero returncode if different\nkubectl get configmap kube-proxy -n kube-system -o yaml | \\\nsed -e \"s/strictARP: false/strictARP: true/\" | \\\nkubectl diff -f - -n kube-system\n​\n# actually apply the changes, returns nonzero returncode on errors only\nkubectl get configmap kube-proxy -n kube-system -o yaml | \\\nsed -e \"s/strictARP: false/strictARP: true/\" | \\\nkubectl apply -f - -n kube-system\n```\n\n### **2、安装MetalLB**\n\n```\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml\n```\n\n### **3、创建ip地址池**\n\n```\ncat > metallb-ip-pool.yaml << EOF\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: default\n  namespace: metallb-system\nspec:\n  addresses:\n  # 网段跟节点保持一致\n  - 10.20.12.190-10.20.12.200\n  # 自动分配可用地址，默认false\n  autoAssign: true\n  # 高级使用，指定命名空间或服务使用ip地址池\n  # serviceAllocation:\n  #   priority: 50\n  #   # 指定default、portal命名空间下所有service可用\n  #   namespaces:\n  #   - default\n  #   - portal\n  #   # 指定在任意命名空间下有foo: bar标签的所有service可用\n  #   namespaceSelectors:\n  #   - matchLabels:\n  #     foo: bar\n  #   # 指定在任意命名空间下service中包含app: bar标签可用\n  #   serviceSelectors:\n  #   - matchExpressions:\n  #     - {key: app, operator: In, values: [bar]}\nEOF\n​\nkubectl apply -f metallb-ip-pool.yaml\n```\n\n此时以及ip地址已经可用，但为了标准化和未来更多的功能，需要创建L2Advertisement\n\n### **4、通告服务IPs（Layer 2 模式）**\n\n```\n\ncat > layer2.yaml << EOF\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: default\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  # 指定ip地址池\n  - default\n  # 高级使用：指定网络接口，默认情况下，metallb 会从节点的所有网络接口播报 LoadBalancer IP\n  interfaces:\n  - ens192\n  # 高级使用：将地址池在指定节点中通告，注释掉默认在所有节点中通告，注意L2模式最终都是在可用节点中选择一个节点通告IP地址池\n  # nodeSelectors:\n  # - matchLabels:\n  #     kubernetes.io/hostname: k8s-node01\n  # - matchLabels:\n  #     kubernetes.io/hostname: k8s-node02\nEOF\n​\nkubectl apply -f layer2.yaml\n```\n\n### **验证**\n\n![](/images/image-ersg.png)\n\n外部访问 EXTERNAL-IP 即可。\n","tags":["MetalLB","LoadBalancer"],"categories":["Linux"]},{"title":"【笔记】使用Docker作为Kubernetes Runtime","url":"/2023/05/12/【笔记】使用Docker作为Kubernetes Runtime/","content":"\n### **解压docker程序**\n\n```\n# 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/\n# wget https://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-25.0.3.tgz\n​\n#解压\ntar xf docker-*.tgz \n#拷贝二进制文件\ncp docker/* /usr/bin/\n```\n\n### **创建containerd的service文件**\n\n```\n#创建containerd的service文件,并且启动\ncat >/etc/systemd/system/containerd.service <<EOF\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target local-fs.target\n​\n[Service]\nExecStartPre=-/sbin/modprobe overlay\nExecStart=/usr/bin/containerd\nType=notify\nDelegate=yes\nKillMode=process\nRestart=always\nRestartSec=5\nLimitNPROC=infinity\nLimitCORE=infinity\nLimitNOFILE=1048576\nTasksMax=infinity\nOOMScoreAdjust=-999\n​\n[Install]\nWantedBy=multi-user.target\nEOF\n​\n# 参数解释：\n# \n# [Unit]\n# - Description=containerd container runtime：指定服务的描述信息。\n# - Documentation=https://containerd.io：指定服务的文档链接。\n# - After=network.target local-fs.target：指定服务的启动顺序，在网络和本地文件系统启动之后再启动该服务。\n# \n# [Service]\n# - ExecStartPre=-/sbin/modprobe overlay：在启动服务之前执行的命令，使用`-`表示忽略错误。\n# - ExecStart=/usr/bin/containerd：指定服务的启动命令。\n# - Type=notify：指定服务的类型，`notify`表示服务会在启动完成后向systemd发送通知。\n# - Delegate=yes：允许服务代理其他服务的应答，例如收到关机命令后终止其他服务。\n# - KillMode=process：指定服务终止时的行为，`process`表示终止服务进程。\n# - Restart=always：指定服务终止后是否自动重启，`always`表示总是自动重启。\n# - RestartSec=5：指定服务重启的时间间隔，单位为秒。\n# - LimitNPROC=infinity：限制服务的最大进程数，`infinity`表示没有限制。\n# - LimitCORE=infinity：限制服务的最大核心数，`infinity`表示没有限制。\n# - LimitNOFILE=1048576：限制服务的最大文件数，指定为1048576。\n# - TasksMax=infinity：限制服务的最大任务数，`infinity`表示没有限制。\n# - OOMScoreAdjust=-999：指定服务的OOM（Out of Memory）得分，负数表示降低被终止的概率。\n# \n# [Install]\n# - WantedBy=multi-user.target：指定服务的安装方式，`multi-user.target`表示该服务在多用户模式下安装。\n​\n​\n# 设置开机自启\nsystemctl enable --now containerd.service\n```\n\n### **准备docker的service文件**\n\n```\n#准备docker的service文件\ncat > /etc/systemd/system/docker.service <<EOF\n[Unit]\n# 描述服务的作用，这里是Docker Application Container Engine，即Docker应用容器引擎\nDescription=Docker Application Container Engine\n# 提供关于此服务的文档链接，这里是Docker官方文档链接\nDocumentation=https://docs.docker.com\n# 说明该服务在哪些其他服务之后启动，这里是在网络在线、firewalld服务和containerd服务后启动\nAfter=network-online.target firewalld.service cri-docker.service docker.socket containerd.service\n# 说明该服务想要的其他服务，这里是网络在线服务\nWants=network-online.target\n# 说明该服务需要的其他服务，这里是docker.socket和containerd.service\nRequires=docker.socket containerd.service\n​\n[Service]\n# 服务类型，这里是notify，表示服务在启动完成时发送通知\nType=notify\n# 启动dockerd并指定一些参数，其中-H指定dockerd的监听地址为fd://，--containerd指定containerd的sock文件位置\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n# 重载命令，当接收到HUP信号时执行的命令，这里是/bin/kill -s HUP $MAINPID，即发送HUP信号给主进程ID\nExecReload=/bin/kill -s HUP $MAINPID\n# 服务超时时间，这里是0，表示没有超时限制\nTimeoutSec=0\n# 重启间隔时间，这里是2秒，表示重启失败后等待2秒再重启\nRestartSec=2\n# 重启策略，这里是always，表示总是重启\nRestart=always\n# 启动限制次数，这里是3，表示在启动失败后最多重试3次\nStartLimitBurst=3\n# 启动限制时间间隔，这里是60秒，表示两次启动之间最少间隔60秒\nStartLimitInterval=60s\n# 文件描述符限制，这里是infinity，表示没有限制\nLimitNOFILE=infinity\n# 进程数限制，这里是infinity，表示没有限制\nLimitNPROC=infinity\n# 核心转储限制，这里是infinity，表示没有限制\nLimitCORE=infinity\n# 最大任务数，这里是infinity，表示没有限制\nTasksMax=infinity\n# 修改权限，这里是yes，表示启用权限修改\nDelegate=yes\n# 杀死模式，这里是process，表示杀死整个进程组\nKillMode=process\n# 用于调整进程在系统内存紧张时的优先级调整，这里是-500，表示将OOM分数降低500\nOOMScoreAdjust=-500\n​\n[Install]\n# 指定该服务应该在多用户模式下启动\nWantedBy=multi-user.target\nEOF\n```\n\n### **准备docker的socket文件**\n\n```\n#准备docker的socket文件\ncat > /etc/systemd/system/docker.socket <<EOF\n[Unit]\n# 描述了该socket的作用，即为Docker API的socket\nDescription=Docker Socket for the API\n​\n[Socket]\n# 指定了socket的监听地址，该socket会监听在/var/run/docker.sock上，即Docker守护程序使用的默认sock文件\nListenStream=/var/run/docker.sock\n# 指定了socket文件的权限模式，此处为0660，即用户和用户组有读写权限，其他用户无权限\nSocketMode=0660\n# 指定了socket文件的所有者，此处为root用户\nSocketUser=root\n# 指定了socket文件的所属用户组，此处为docker用户组\nSocketGroup=docker\n​\n[Install]\n# 指定了该socket被启用时的目标，此处为sockets.target，表示当sockets.target启动时启用该socket\nWantedBy=sockets.target\nEOF\n​\n# 该配置文件的作用是为Docker提供API访问的通道，它监听在/var/run/docker.sock上，具有root用户权限，但只接受docker用户组的成员的连接，并且其他用户无法访问。这样，只有docker用户组的成员可以通过该socket与Docker守护进程进行通信。\n```\n\n### **配置加速器**\n\n```\n# 配置加速器\nmkdir /etc/docker/ -pv\ncat >/etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"registry-mirrors\": [\n    \"https://docker.mirrors.ustc.edu.cn\"\n  ],\n  \"max-concurrent-downloads\": 10,\n  \"log-driver\": \"json-file\",\n  \"log-level\": \"warn\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n    },\n  \"data-root\": \"/var/lib/docker\"\n}\nEOF\n```\n\n### **启动docker**\n\n```\ngroupadd docker\n#创建docker组\n​\nsystemctl daemon-reload\n​\nsystemctl enable --now docker.socket\n# 启用并立即启动docker.socket单元。docker.socket是一个systemd的socket单元，用于接收来自网络的Docker API请求。\n​\nsystemctl enable --now docker.service\n# 启用并立即启动docker.service单元。docker.service是Docker守护进程的systemd服务单元。\n​\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n​\ndocker info\n#验证\n```\n\n### **解压cri-docker**\n\n```\n# 由于1.24以及更高版本不支持docker所以安装cri-docker\n# 下载cri-docker \n# https://github.com/Mirantis/cri-dockerd/releases/\n# wget  https://mirrors.chenby.cn/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd-0.3.10.amd64.tgz\n​\n# 解压cri-docker\ntar xvf cri-dockerd-*.amd64.tgz \ncp -r cri-dockerd/  /usr/bin/\nchmod +x /usr/bin/cri-dockerd/cri-dockerd\n```\n\n### **写入启动cri-docker配置文件**\n\n```\n# 写入启动配置文件\ncat >  /usr/lib/systemd/system/cri-docker.service <<EOF\n[Unit]\nDescription=CRI Interface for Docker Application Container Engine\nDocumentation=https://docs.mirantis.com\nAfter=network-online.target firewalld.service docker.service\nWants=network-online.target\nRequires=cri-docker.socket\n​\n[Service]\nType=notify\nExecStart=/usr/bin/cri-dockerd/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\n​\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### **写入cri-docker的socket配置文件**\n\n```\n# 写入socket配置文件\ncat > /usr/lib/systemd/system/cri-docker.socket <<EOF\n[Unit]\nDescription=CRI Docker Socket for the API\nPartOf=cri-docker.service\n​\n[Socket]\nListenStream=%t/cri-dockerd.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n​\n[Install]\nWantedBy=sockets.target\nEOF\n```\n\n### **启动cri-docker**\n\n```\nsystemctl daemon-reload\n# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。\n​\nsystemctl enable --now cri-docker.service\n# 启用并立即启动cri-docker.service单元。cri-docker.service是cri-docker守护进程的systemd服务单元。\n​\nsystemctl restart cri-docker.service\n# 重启cri-docker.service单元，即重新启动cri-docker守护进程。\n​\nsystemctl status docker.service\n# 显示docker.service单元的当前状态，包括运行状态、是否启用等信息。\n```\n","tags":["Docker"],"categories":["Linux"]},{"title":"【笔记】使用Containerd作为Kubernetes Runtime","url":"/2023/05/12/【笔记】使用Containerd作为Kubernetes Runtime/","content":"\n### **安装Containerd**\n\n#### 方式一：yum安装\n\n```\n# 安装系统工具\nyum install -y yum-utils device-mapper-persistent-data lvm2\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 修改阿里源地址\nsed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n```\n\n```\nyum -y install containerd\nsystemctl enable containerd --now\ncontainerd -v\n```\n\n#### 方式二：离线安装\n\n[*containerd – 行业标准的容器运行时，强调简单性、健壮性和可移植性*](https://containerd.io/downloads/)\n\n```\nwget https://github.com/containerd/containerd/releases/download/v1.7.14/containerd-1.7.14-linux-amd64.tar.gz\ntar xvf containerd-1.7.14-linux-amd64.tar.gz\ncp -rf bin/* /usr/local/bin\n```\n\n```\ncat > /etc/systemd/system/containerd.service <<EOF\n[Unit]\n# 定义了服务的安装位置。这里指定为multi-user.target，表示将服务安装为多用户模式下的启动项\nDescription=containerd container runtime\n# 指向容器运行时的文档的URL\nDocumentation=https://containerd.io\n# 定义了在哪些依赖项之后该unit应该被启动。在网络和本地文件系统加载完成后启动，确保了容器运行时在这些依赖项可用时才会启动\nAfter=network.target local-fs.target\n​\n[Service]\n# 在启动containerd之前执行的命令。这里的命令是尝试加载内核的overlay模块，如果失败则忽略错误继续执行下面的命令\nExecStartPre=-/sbin/modprobe overlay\n# 实际执行的命令，用于启动containerd容器运行时\nExecStart=/usr/local/bin/containerd\n# 指定服务的通知类型。这里使用notify类型，表示当服务就绪时会通过通知的方式告知systemd\nType=notify\n# 允许systemd对此服务进行重启和停止操作\nDelegate=yes\n# 在终止容器运行时时使用的kill模式。这里使用process模式，表示通过终止进程来停止容器运行时\nKillMode=process\n# 定义了当容器运行时终止后的重启策略。这里设置为always，表示无论何时终止容器运行时，都会自动重新启动\nRestart=always\n# 在容器运行时终止后重新启动之前等待的秒数\nRestartSec=5\n# 指定容器运行时可以使用的最大进程数量。这里设置为无限制\nLimitNPROC=infinity\n# 指定容器运行时可以使用的最大CPU核心数量。这里设置为无限制\nLimitCORE=infinity\n# 指定容器运行时可以打开的最大文件数。这里设置为无限制\nLimitNOFILE=infinity\n# 指定容器运行时可以创建的最大任务数。这里设置为无限制\nTasksMax=infinity\n# 指定容器运行时的OOM（Out-Of-Memory）分数调整值。负数值表示容器运行时的优先级较高\nOOMScoreAdjust=-999\n​\n[Install]\n# 指定容器运行时的OOM（Out-Of-Memory）分数调整值。负数值表示容器运行时的优先级较高\nWantedBy=multi-user.target\nEOF\n```\n\n#### 加载模块\n\n```\ncat > /etc/modules-load.d/containerd.conf << EOF\noverlay\nbr_netfilter\nEOF\n```\n\n```\nsystemctl restart systemd-modules-load\n```\n\n#### 修改内核参数\n\n```\ncat > /etc/sysctl.d/99-kubernetes-cri.conf << EOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\n```\n\n#### 修改配置文件\n\n```\nmkdir -p /etc/containerd\n# 生成默认配置文件\ncontainerd config default > /etc/containerd/config.toml\n# SystemdCgroup参数的作用是为了确保containerd能够正确地管理容器的资源使用，以实现资源的限制、隔离和公平分配\nsed -i \"s#SystemdCgroup\\ \\=\\ false#SystemdCgroup\\ \\=\\ true#g\" /etc/containerd/config.toml\n# 修改镜像拉取地址为国内地址，这里是pause镜像地址\nsed -i \"s#registry.k8s.io#registry.aliyuncs.com/google_containers#g\" /etc/containerd/config.toml\n# 指定配置文件目录\nsed -i \"s#config_path\\ \\=\\ \\\"\\\"#config_path\\ \\=\\ \\\"/etc/containerd/registry\\\"#g\" /etc/containerd/config.toml\nsed -i 's/pause:3.6/pause:3.9/g'  /etc/containerd/config.toml\n```\n\n```\n# 设置镜像加速\nmkdir /etc/containerd/registry/docker.io -pv\ncat > /etc/containerd/registry/docker.io/hosts.toml << EOF\nserver = \"https://docker.io\"\n[host.\"https://docker.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\n[host.\"https://xk9ak4u9.mirror.aliyuncs.com\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://dockerproxy.com\"]\n  capabilities = [\"pull\", \"resolve\"]\n[host.\"https://docker.mirrors.sjtug.sjtu.edu.cn\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://docker.mirrors.ustc.edu.cn\"]\n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://docker.nju.edu.cn\"] \n  capabilities = [\"pull\",\"resolve\"]\n[host.\"https://registry-1.docker.io\"]\n  capabilities = [\"pull\",\"resolve\",\"push\"]\nEOF\n​\nmkdir /etc/containerd/registry/gcr.io -pv\ncat > /etc/containerd/registry/gcr.io/hosts.toml << EOF\nserver = \"https://gcr.io\"\n[host.\"https://gcr.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n​\nmkdir /etc/containerd/registry/registry.k8s.io -pv\ncat > /etc/containerd/registry/registry.k8s.io/hosts.toml << EOF\nserver = \"https://registry.k8s.io\"\n[host.\"https://k8s.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n\nmkdir /etc/containerd/registry/k8s.gcr.io -pv\ncat > /etc/containerd/registry/k8s.gcr.io/hosts.toml << EOF\nserver = \"https://k8s.gcr.io\"\n[host.\"https://k8s.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n\nmkdir /etc/containerd/registry/quay.io -pv\ncat > /etc/containerd/registry/quay.io/hosts.toml << EOF\nserver = \"https://quay.io\"\n[host.\"https://quay.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\"]\nEOF\n```\n\n#### 镜像加速脚本\n\n```\n#!/usr/bin/env bash\n \nexport PS4='\\[\\e[35m\\]+ $(basename $0):${FUNCNAME}:$LINENO: \\[\\e[0m\\]'\n[ \"$debug\" == \"true\" -o \"$debug\" == \"yes\" ] && set -x\n \nconfig_file=\"/etc/containerd/config.toml\"\nconfig_path='/etc/containerd/registry'\n \nif [ ! -f \"${config_file}\" ];then\n    [ ! -d \"${config_file%/*}\" ] && mkdir -p ${config_file%/*}\n    lineno=\"$(containerd config default | grep -n -A 1 -P '(?<=\\[plugins.\")io.containerd.grpc.v1.cri(?=\".registry])'|tail -1)\"\n    lineno=${lineno/-*}\n    containerd config default | sed -e \"${lineno}s@config.*@config_path = \\\"${config_path}\\\"@\" |sed '/SystemdCgroup/s/false/true/' > $config_file\nfi\n \n[ ! -d \"${config_path}\" ] && mkdir -p ${config_path}\nparams=\"${@:-registry.k8s.io:k8s.m.daocloud.io docker.io:docker.m.daocloud.io gcr.io:gcr.m.daocloud.io k8s.gcr.io:k8s.m.daocloud.io quay.io:quay.m.daocloud.io}\"\n \nfunction content(){\n    printf 'server = \"https://%s\"\\n'  \"${registry}\"\n    printf '[host.\"https://%s\"]\\n' \"${proxy_server}\"\n    printf '  capabilities = [\"pull\", \"resolve\"]'\n}\n \nfor param in ${params}\ndo\n    registry=\"${param/:*/}\"\n    proxy_server=\"${param/*:/}\"\n    hosts_path=\"$config_path/$registry\"\n    [ ! -d \"$hosts_path\" ] && mkdir -p ${hosts_path}\n    content > $hosts_path/hosts.toml\ndone\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl daemon-reload\nsystemctl enable containerd --now\nsystemctl restart containerd\nsystemctl status containerd\n```\n\n### **安装crictl**\n\n下载地址：[*Releases · kubernetes-sigs/cri-tools (*](https://github.com/kubernetes-sigs/cri-tools/releases)[*github.com*](http://github.com)[*)*](https://github.com/kubernetes-sigs/cri-tools/releases)\n\n```\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.30.0/crictl-v1.30.0-linux-amd64.tar.gz\ntar -zxf crictl-v1.30.0-linux-amd64.tar.gz \nmv crictl /usr/local/bin/\n```\n\n```\ncat > /etc/crictl.yaml <<EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\ndebug: false\npull-image-on-create: false\nEOF\n```\n","tags":["Containerd"],"categories":["Linux"]},{"title":"【笔记】yum更新Linux内核","url":"/2023/04/23/【笔记】yum更新Linux内核/","content":"\n## yum更新Linux内核\n\n```\n#载入ELRepo仓库的公共密钥\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\n#安装ELRepo仓库的yum源\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm\n#查看可用的系统内核包\nyum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available\n#安装最新版本内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y\n```\n\n### 设置默认内核\n\n#### 获取内核索引值\n\n```\nawk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg\n```\n\n![image-20240423150008735](/images/image-zjyd.png)\n\n更换的内核为6.8.7，索引值为0\n\n#### 修改grub配置文件\n\n```\nvim /etc/default/grub\n```\n\n![image-20240423145939049](/images/image-wihy.png)\n\n#### 生成grub配置文件\n\n```\ngrub2-mkconfig -o /boot/grub2/grub.cfg\n```\n\n![image-20240423150440339](/images/image-ssrx.png)\n\n#### 重启验证\n\n```\nreboot\n```\n\n![image-20240423150506336](/images/image-cqpp.png)","tags":["linux"],"categories":["系统"]},{"title":"【笔记】GeForce RTX 2080 Ti安装NVIDIA 550.107.02驱动+CUDA Toolkit","url":"/2023/02/11/【笔记】GeForce RTX 2080 Ti安装NVIDIA 550.107.02驱动+CUDA Toolkit/","content":"\n### 环境配置\n\n* **kernel-3.10.0-1160.119.1.el7.x86\\_64**\n\n* **kernel-devel-3.10.0-1160.119.1.el7.x86\\_64**\n\n* **CentOS 7.9.2009**\n\n* **Linux x64 (AMD64/EM64T) Display Driver 550.107.02**\n\n* **GeForce RTX 2080 Ti**\n\n> 内核版本测试过5.4.227，6.9.7都不能安装驱动，降级回3.10.0就可以了\n\n查看显卡\n\n```\nlspci | grep -i nvidia\n```\n\n> d8:00.0 VGA compatible controller: NVIDIA Corporation TU102 \\[GeForce RTX 2080 Ti Rev. A] (rev a1)\n>\n> d8:00.1 Audio device: NVIDIA Corporation TU102 High Definition Audio Controller (rev a1)\n>\n> d8:00.2 USB controller: NVIDIA Corporation TU102 USB 3.1 Host Controller (rev a1)\n>\n> d8:00.3 Serial bus controller \\[0c80]: NVIDIA Corporation TU102 USB Type-C UCSI Controller (rev a\n\n### 禁用显卡驱动\n\n```\n# 查看是否存在\nlsmod | grep nouveau\n# 修改下方配置文件\nvi /lib/modprobe.d/dist-blacklist.conf\n```\n\n![](/images/image-ohux.png)\n\n```\n# 应用更改\nupdate-initramfs -u\n# 再次查看是否存在\nlsmod | grep nouveau\n# 如果还存在，重启服务器\nreboot\n```\n\n```\n# 删除\nmv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak\n# 重建\ndracut /boot/initramfs-$(uname -r).img $(uname -r)\n```\n\n### 下载驱动\n\n地址：<https://www.nvidia.cn/Download/index.aspx?lang=cn>\n\n![](/images/image-rqxb.png)\n\n### 安装驱动\n\n```\n# 执行权限\nchmod +x NVIDIA-Linux-x86_64-550.107.02.run\n# 安装\n./NVIDIA-Linux-x86_64-550.107.02.run -no-x-check --no-opengl-files --kernel-source-path=/usr/src/kernels/5.4.227-1.el7.elrepo.x86_64\n```\n\n> -no-x-check: 安装驱动时不检查X服务，非必需，已经禁用图形界面\n>\n> -no-opengl-files: 只安装驱动文件，不安装OpenGL文件\n>\n> -no-nouveau-check：安装驱动时禁用nouveau，非必需，已经禁用nouveau\n>\n> \\--kernel-source-path：内核源码包位置，安装时会修改源码包，不带该参数，默认是当前内核\n\n![](/images/image-anmh.png)\n\n![](/images/image-rmwg.png)\n\n![](/images/image-nnwq.png)\n\n![](/images/image-jhri.png)\n\n![](/images/image-revk.png)\n\n![](/images/image-waew.png)\n\n### 验证驱动\n\n![](/images/image-pwyg.png)\n\n### 安装CUDA Toolkit\n\n版本信息：[1. CUDA 12.6 Release Notes — Release Notes 12.6 documentation (](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id5)[nvidia.com](http://nvidia.com)[)](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id5)\n\n> 对应驱动版本\n\n本次安装CentOS 7 v12.4 GA：[CUDA Toolkit 12.4 Downloads | NVIDIA Developer](https://developer.nvidia.com/cuda-12-4-0-download-archive)\n\n![](/images/image-wbdx.png)\n\n```\nwget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run\nsh cuda_12.4.0_550.54.14_linux.run\n```\n\n![](/images/image-jzqk.png)\n\n![](/images/image-pwzz.png)\n\n#### 已知问题\n\n> Installation failed. See log at /var/log/cuda-installer.log for details.\n\n##### 排查思路\n\n解决方案：\n\n```\n# 查看日志\ncat /var/log/cuda-installer.log\n```\n\n```\n[INFO]: Initializing menu\n[INFO]: nvidia-fs.setKOVersion(2.19.6)\n[INFO]: Setup complete\n[INFO]: Installing: Driver\n[INFO]: Installing: 550.54.14\n[INFO]: Executing NVIDIA-Linux-x86_64-550.54.14.run --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd  2>&1\n[INFO]: Finished with code: 256\n[ERROR]: Install of driver component failed. Consult the driver log at /var/log/nvidia-installer.log for more details.\n[ERROR]: Install of 550.54.14 failed, quitting\n```\n\n> 安装nvidia驱动失败，继续根据提示查看日志\n\n```\ncat /var/log/nvidia-installer.log \n```\n\n```\nUsing built-in stream user interface\n-> Detected 64 CPUs online; setting concurrency level to 32.\n-> Scanning the initramfs with lsinitrd...\n-> /usr/bin/lsinitrd requires a file path argument, but none was given.\n-> Executing: /usr/bin/lsinitrd    /boot/initramfs-3.10.0-1160.119.1.el7.x86_64.img\n-> Tagging shared libraries with chcon -t textrel_shlib_t.\n-> The file '/tmp/.X0-lock' exists and appears to contain the process ID '2843' of a running X server.\n-> You appear to be running an X server.  Installing the NVIDIA driver while X is running is not recommended, as doing so may prevent the installer from detecting some potential installation problems, and it may not be possible to start new graphics applications after a new driver is installed.  If you choose to continue installation, it is highly recommended that you reboot your computer after installation to use the newly installed driver. (Answer: Abort installation)\nERROR: Installation has failed.  Please see the file '/var/log/nvidia-installer.log' for details.  You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.\n```\n\n> 2843进程正在使用驱动，kill掉这个进程重新安装CUDA\n\n问题二\n\n```\nERROR: The installation was canceled due to the availability or presence of an alternate driver installation. Please see /var/log/nvidia-installer.log for more details.\n```\n\n> CUDA版本低于NVIDIA驱动版本\n\n### 验证CUDA驱动\n\n```\n /usr/local/cuda/bin/nvcc --version\n```\n\n> nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Tue\\_Feb\\_27\\_16:19:38\\_PST\\_2024 Cuda compilation tools, release 12.4, V12.4.99 Build cuda\\_12.4.r12.4/compiler.33961263\\_0\n","tags":["GPU"],"categories":["Linux"]},{"title":"【云原生】Kubernetes Dashboard v7.5.0 部署（Helm方式）","url":"/2023/01/29/【云原生】Kubernetes Dashboard v7.5.0 部署（Helm方式）/","content":"\n#### **安装dashboard**\n\n```\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\nhelm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\n```\n\n#### **修改NodePort**\n\n```\nkubectl edit svc  -n kube-system kubernetes-dashboard-kong-proxy\n```\n\n> ClusterIP 改为NodePort\n\n#### **创建token**\n\n```\ncat > dashboard-user-token.yaml << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-user\n  namespace: kubernetes-dashboard\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dashboard-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"dashboard-user\"\ntype: kubernetes.io/service-account-token\nEOF\n```\n\n```\nkubectl apply -f dashboard-user-token.yaml\n```\n\n#### **获取token**\n\n```\nkubectl get secret -n kubernetes-dashboard dashboard-user -o jsonpath={\".data.token\"} | base64 -d\n```\n","tags":["Kubernetes"],"categories":["云原生"]},{"title":"【云原生】Kubernetes Dashboard v2.7.0部署（token、kubeconfig登录）","url":"/2022/12/09/【云原生】Kubernetes Dashboard v2.7.0部署（token、kubeconfig登录）/","content":"\n### **部署dashboard**\n\n```\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n```\n\n### **创建sa、clusterrolebing、secret**\n\n```\nkubectl create serviceaccount dashboard -n kubernetes-dashboard\nkubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard\n```\n\n### **token登录**\n\n```\nkubectl -n kubernetes-dashboard create token dashboard\n```\n\n### **kubeconfig登录**\n\n```\nkubectl apply -f-\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dashboard\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"dashboard\"\ntype: kubernetes.io/service-account-token\n```\n\n```\nkubectl describe secrets -n kubernetes-dashboard dashboard\n```\n\n```\ncp ~/.kube/config ~/kubeconfig\nvim kubeconfig\n```\n\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: xxxx\n    server: https://xx.xx.xx.xx:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: xxxx\n    client-key-data: xxxx\n    token: 新增token\n```\n","tags":["Kubernetes"],"categories":["云原生"]},{"title":"【笔记】浪潮服务器安装NVIDIA Tesla T4显卡","url":"/2022/11/30/【笔记】浪潮服务器安装NVIDIA Tesla T4显卡/","content":"### 服务器上架\n\n卸下机盖，拆下riser扩展版，安装到pcie卡槽，装回riser。\n\n![](/images/c2b91fd76e4e5894679e123d53b2ad7.jpg)\n\n### 开机查看系统\n\n![](/images/be82e0dcc1054c349230e5c43238e4a.png)\n\n### 下载驱动\n\n搜索驱动：[下载 NVIDIA 官方驱动 | NVIDIA](https://www.nvidia.cn/drivers/lookup/)\n\n![](/images/image-igzv.png)\n\n![](/images/image-jsuo.png)\n\n### **安装驱动**\n\n```perl\n# 执行权限\nchmod +x nvidia-driver-local-repo-kylin10-565.57.01-1.0-1.x86_64.rpm\n# 安装\nyum -y localinstall nvidia-driver-local-repo-kylin10-565.57.01-1.0-1.x86_64.rpm\n```\n\n参考地址：[GeForce RTX 2080 Ti安装NVIDIA 550.107.02驱动+CUDA Toolkit - Elijah's Blog](https://sreok.cn/archives/1723448671609)\n","tags":["GPU"],"categories":["Linux"]},{"title":"【云原生】Kubernetes弹性伸缩方案：VPA（Vertical Pod Autoscaler）纵向自动伸缩","url":"/2022/10/14/【云原生】Kubernetes弹性伸缩方案：VPA（Vertical Pod Autoscaler）纵向自动伸缩/","content":"\n### **前提条件**\n\n* Kubernetes >= 1.25\n\n* Openssl >= 1.1.1\n\n* 部署MetricServer\n\n官方地址：[自动扩缩工作负载 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically)\n\n官方Github：[autoscaler/vertical-pod-autoscaler at vertical-pod-autoscaler-1.1.2 · kubernetes/autoscaler · GitHub](https://github.com/kubernetes/autoscaler/tree/vertical-pod-autoscaler-1.1.2/vertical-pod-autoscaler)\n\n### **部署VPA（v1.1.2）**\n\n```\ngit clone https://github.com/kubernetes/autoscaler.git -b vertical-pod-autoscaler-1.1.2\ncd autoscaler/vertical-pod-autoscaler/hack\n./vpa-up.sh\n```\n\n#### **已知问题**\n\n`ERROR: Failed to create CA certificate for self-signing. If the error is \"unknown option -addext\", update your openssl version or deploy VPA from the vpa-release-0.8 branch.`\n\n原因：openssl 版本太低\n\n解决方法：执行`vpa-down.sh`卸载后，升级openssl，重新安装\n\n### **升级openssl（部署机升级即可）**\n\n查看当前版本\n\n![](/images/image-asik.png)\n\n下载地址（最新稳定版3.3，支持到2026.4.9）：[openssl-3.3.1.tar.gz](https://www.openssl.org/source/openssl-3.3.1.tar.gz)\n\n#### **备份**\n\n```\nmv /usr/bin/openssl /usr/bin/openssl.bak\nmv /usr/include/openssl /usr/include/openssl.bak\n```\n\n#### **安装依赖**\n\n```\nyum -y install perl-IPC-Cmd perl-ExtUtils-MakeMaker perl-Data-Dumper\n```\n\n#### **编译**\n\n```\ntar zxvf openssl-3.3.1.tar.gz\ncd openssl-3.3.1/\n./config shared --openssldir=/usr/local/openssl --prefix=/usr/local/openssl\nmake && make install\n```\n\n#### **软连接**\n\n```\nln -s /usr/local/openssl/lib/libssl.so.3 /usr/lib/libssl.so.3\nln -s /usr/local/openssl/lib64/libssl.so.3 /usr/lib64/libssl.so.3\nln -s /usr/local/openssl/lib/libcrypto.so.3 /usr/lib/libcrypto.so.3\nln -s /usr/local/openssl/lib64/libcrypto.so.3 /usr/lib64/libcrypto.so.3\n​\nln -s /usr/local/openssl/bin/openssl /usr/bin/openssl\nln -s /usr/local/openssl/include/openssl /usr/include/openssl\necho “/usr/local/openssl/lib” >> /etc/ld.so.conf\nldconfig -v\n```\n\n查看版本\n\n![](/images/image-qzgf.png)\n\n### **测试VPA**\n\n#### **创建Deployment**\n\n```\ncat > apache.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: apache\n  template:\n    metadata:\n      labels:\n        run: apache\n    spec:\n      containers:\n      - name: apache\n        image: registry.k8s.io/hpa-example\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: apache\n  labels:\n    run: apache\nspec:\n  ports:\n  - port: 80\n  selector:\n    run: apache\nEOF\n```\n\n```\nkubectl apply -f apache.yaml\n```\n\n查看当前pod资源\n\n```\nkubectl describe pod <podname>\n```\n\n![](/images/image-vjja.png)\n\n> 没有限制，所以没有Limits和Requests\n\n#### **创建VPA**\n\n```\ncat > vpa-apache.yaml << EOF \napiVersion: autoscaling.k8s.io/v1 \nkind: VerticalPodAutoscaler \nmetadata: \n  name: apache-deployment-basic-vpa   \nnamespace: default \nspec: \n  targetRef: \n    apiVersion: \"apps/v1\" \n    kind:       Deployment \n    name:       apache \n  updatePolicy: \n    updateMode: \"Auto\" \nEOF\n```\n\n![](/images/image-fpfd.png)\n\n```\nkubectl apply -f vpa-apache.yaml\n```\n\n```\n# 需要等待两分钟，才能返回结果\nkubectl get vpa\n```\n\n![](/images/image-shot.png)\n\n![](/images/image-clym.png)\n\n> pod会根据推荐资源配置，重新分配\n\n```\nkubectl describe vpa apache-deployment-basic-vpa\n```\n\n![](/images/image-tiyv.png)\n\n```\nkubectl describe pod <podname>\n```\n\n![](/images/image-fslq.png)\n\n> vpa与重新创建的pod资源对应\n\n### **增加负载**\n\n```\nyum -y install httpd-tools\n```\n\n```\nab -c 100 -n 500000 http://10.20.13.140:29704/\n```\n\n![](/images/image-noje.png)\n","tags":["Kubernetes","VPA"],"categories":["云原生"]},{"title":"【云原生】Kubernetes弹性伸缩方案：HPA（Horizontal Pod Autoscaler）横向自动伸缩","url":"/2022/10/11/【云原生】Kubernetes弹性伸缩方案：HPA（Horizontal Pod Autoscaler）横向自动伸缩/","content":"\n### **前提条件**\n\n* Kubernetes >= 1.23\n\n### **安装Metrics-server**\n\n原文地址：[*k8s 安装metrics-server v0.7.1 - (*](https://sreok.cn/archives/2e52ea09-e55f-4a0a-bdae-e6f4129c6516)[*sreok.cn*](http://sreok.cn)[*)*](https://sreok.cn/archives/2e52ea09-e55f-4a0a-bdae-e6f4129c6516)\n\n### **创建Deployment**\n\n```\ncat > apache.yaml << EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache\nspec:\n  selector:\n    matchLabels:\n      run: apache\n  template:\n    metadata:\n      labels:\n        run: apache\n    spec:\n      containers:\n      - name: apache\n        image: registry.k8s.io/hpa-example\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 200m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: apache\n  labels:\n    run: apache\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n  selector:\n    run: apache\nEOF\n```\n\n> resources: limits: cpu: 500m requests: cpu: 200m\n>\n> 此字段是必须的\n\n```\nkubectl apply -f apache.yaml\n```\n\n```\nkubectl get svc # 记录svc端口，测试时使用\n```\n\n### **创建HPA**\n\n```\nkubectl autoscale deployment apache  --cpu-percent=50 --min=1 --max=10\n```\n\n![](/images/image-upgh.png)\n\n### **负载测试**\n\n#### 安装测试工具\n\n```\nyum -y install httpd-tools\n```\n\n#### 并发测试\n\n```\nab -c 100 -n 5000 http://10.20.13.140:32010/\n```\n\n> -c 100 并发数\n>\n> -n 5000 请求数\n>\n> ctrl + c停止并发，\n\n![](/images/image-hbti.png)\n\n![](/images/image-evsp.png)\n\n### **停止负载**\n\n![](/images/image-skul.png)\n\n![](/images/image-ceup.png)\n\n### **总结**\n\nHPA控制器会根据以下公式来计算需要的Pod副本数：\n\n官方地址：[*Pod 水平自动扩缩 | Kubernetes*](https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details)\n\n```\n期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)]\n```\n\n> ceil是向上取整函数\n\n根据以上Deployment配置，每个Apache Pod请求了200m的CPU资源（`requests: cpu: 200m`），并且设置了CPU资源限制为500m（`limits: cpu: 500m`）。\n\n现在，我们设置HPA的目标CPU利用率为50%，这意味着我们希望每个Pod平均使用其请求CPU量的50%。\n\n注意：百分比是根据pod的`requests`参数，由于每个Pod请求了200m的CPU，50%的利用率即为100m（200m的一半）。HPA将会监控所有Pod的CPU使用情况，并计算平均CPU使用率。如果平均CPU使用率超过100m，HPA就会开始扩容。\n","tags":["Kubernetes","HPA"],"categories":["云原生"]},{"title":"【故障排查】Postgre启动失败：Bus error (core dumped)","url":"/2022/09/12/【故障排查】Postgre启动失败：Bus error (core dumped)/","content":"\n### **问题描述**\n\n```\nBus error (core dumped) \nchild process exited with exit code 135\n```\n\n![](/images/image-agos.png)\n\n### **修复**\n\n`在pod所在节点操作`\n\n```\n# 临时设置\nsysctl -w vm.nr_hugepages=0\ncat /proc/sys/vm/nr_hugepages\n```\n\n```\n# 永久设置\nsed -i 's/vm.nr_hugepages = 250/vm.nr_hugepages = 0/g' /etc/sysctl.conf\nsysctl --system\n```\n\n### **参考链接**\n\n[*Error 135 in Initdb deploying inside Kubernetes · Issue #451 · docker-library/postgres (*](https://github.com/docker-library/postgres/issues/451)[*github.com*](http://github.com)[*)*](https://github.com/docker-library/postgres/issues/451)\n\n[*postgresql - Kubernetes Postgres Bus error (core dumped) - Stack Overflow*](https://stackoverflow.com/questions/67941955/kubernetes-postgres-bus-error-core-dumped)\n","tags":["Kubernetes","故障排除","Postgre"],"categories":["云原生"]},{"title":"【笔记】docker-compose部署OpenLDAP + phpLdapAdmin + 自助修改密码平台","url":"/2022/08/06/【笔记】docker-compose部署OpenLDAP + phpLdapAdmin + 自助修改密码平台/","content":"\n\n### **安装docker、docker-compose**\n\n略\n\n### **部署OpenLDAP**\n\n```\nversion: '2'\nservices:\n  openldap:\n    image: osixia/openldap:1.5.0  ### 如果有私有仓库可以从自己的私有仓库拉取镜像\n    container_name: openldap\n    restart: always\n    environment:\n      LDAP_LOG_LEVEL: \"256\"\n      LDAP_ORGANISATION: \"vsoul\"   ### 您的组织名称\n      LDAP_DOMAIN: \"vsoul.cn\"    ### 公司域名\n      LDAP_BASE_DN: \"dc=vsoul,dc=cn\"   ### 根据域名组成\n      LDAP_ADMIN_PASSWORD: \"123456\"   ### 密码自己来设置\n      LDAP_CONFIG_PASSWORD: \"123456\"\n      LDAP_READONLY_USER: \"false\"\n      #LDAP_READONLY_USER_USERNAME: \"readonly\"\n      #LDAP_READONLY_USER_PASSWORD: \"readonly\"\n      LDAP_RFC2307BIS_SCHEMA: \"false\"\n      LDAP_BACKEND: \"mdb\"\n      #LDAP_TLS: \"true\"\n      #LDAP_TLS_CRT_FILENAME: \"zaq.test.pem\"\n      #LDAP_TLS_KEY_FILENAME: \"zaq.test.key\"\n      #LDAP_TLS_DH_PARAM_FILENAME: \"dhparam.pem\"\n      #LDAP_TLS_CA_CRT_FILENAME: \"ca.crt\"\n      #LDAP_TLS_ENFORCE: \"false\"\n      #LDAP_TLS_CIPHER_SUITE: \"SECURE256:-VERS-SSL3.0\"\n      # LDAP_TLS_VERIFY_CLIENT: \"demand\"\n      LDAP_REPLICATION: \"false\"\n      #LDAP_REPLICATION_CONFIG_SYNCPROV: 'binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=\"$$LDAP_CONFIG_PASSWORD\" searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical'\n      #LDAP_REPLICATION_DB_SYNCPROV: 'binddn=\"cn=admin,$$LDAP_BASE_DN\" bindmethod=simple credentials=\"$$LDAP_ADMIN_PASSWORD\" searchbase=\"$$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical'\n      #LDAP_REPLICATION_HOSTS: \"#PYTHON2BASH:['ldap://ldap.example.org','ldap://ldap2.example.org']\"\n      KEEP_EXISTING_CONFIG: \"false\"\n      LDAP_REMOVE_CONFIG_AFTER_SETUP: \"true\"\n      #LDAP_SSL_HELPER_PREFIX: \"ldap\"\n    tty: true\n    stdin_open: true\n    volumes:\n      - /opt/openldap/ldap:/var/lib/ldap\n      - /opt/openldap/slapd.d:/etc/ldap/slapd.d\n      - /opt/openldap/certs:/container/service/lapd/assets/certs\n    ports:\n      - \"389:389\"\n      - \"636:636\"\n    # For replication to work correctly, domainname and hostname must be\n    # set correctly so that \"hostname\".\"domainname\" equates to the\n    # fully-qualified domain name for the host.\n    domainname: \"vsoul.cn\"\n    hostname: \"ldap-server\"\n  phpldapadmin:\n    image: osixia/phpldapadmin:latest\n    container_name: phpldapadmin\n    restart: always\n    environment:\n      PHPLDAPADMIN_LDAP_HOSTS: \"openldap\"   ### 如果部署后登录不进去有可能是这里出了问题，直接换为部署openldap服务的公网IP试试\n      PHPLDAPADMIN_HTTPS: \"false\"\n    ports:\n      - \"50080:80\"\n    depends_on:\n      - openldap\n  self-service-password:\n    container_name: self-service-password\n    image: tiredofit/self-service-password:5.3.3\n    restart: always\n    ports:\n      - \"80:80\"\n    environment:\n      - LDAP_SERVER=ldap://openldap:389\n      - LDAP_BINDDN=cn=admin,dc=vsoul,dc=cn\n      - LDAP_BINDPASS=123456\n      - LDAP_BASE_SEARCH=dc=vsoul,dc=cn\n      - MAIL_FROM=info@vsoul.cn\n      - MAIL_FROM_NAME=LDAP账号自助服务平台\n      - SMTP_DEBUG=0\n      - SMTP_HOST=smtp.qiye.aliyun.cn\n      - SMTP_USER=info@vsoul.cn\n      - SMTP_PASS=<邮箱密码>\n      - SMTP_PORT=25\n      #- SMTP_SECURE_TYPE=ssl\n      - SMTP_AUTH_ON=true\n      - NOTIFY_ON_CHANGE=true\n    volumes:\n      - /etc/localtime:/etc/localtime\n      - /opt/openldap/self-service-password/htdocs:/www/ssp\n      - /opt/openldap/self-service-password/logs:/www/logs\n    deploy:\n      resources:\n        limits:\n           memory: 4G\n        reservations:\n           memory: 512M\n```\n\n```\ndocker-compose up -d\n```\n\n### **访问**\n\n```\nhttp://IP:50080\n账号：cn=admin,dc=vsoul,dc=cn\n密码：123456\n```\n\n创建账号\n\n![](/images/image-tdpd.png)\n\n![](/images/image-iycc.png)\n\n### **自助修改密码**\n\n`配置好发件邮箱账号`\n\n```\nhttp://IP\n```\n\n![](/images/image-qfmv.png)\n\n![](/images/image-rvth.png)\n\n![](/images/image-aiws.png)\n\n![](/images/image-wosf.png)\n","tags":["openladp"],"categories":["Linux"]},{"title":"【云原生】Kubernetes部署Bitnami MySQL v8.0.37主从复制（Helm方式）","url":"/2022/07/04/【云原生】Kubernetes部署Bitnami MySQL v8.0.37主从复制（Helm方式）/","content":"\n### 添加helm仓库\n\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n### 拉取仓库\n\n```\nhelm pull bitnami/mysql --version 10.3.0 --untar\n```\n\n### 修改values.yaml\n\n`vim value.yaml`\n\n```\n# architecture: standalone\narchitecture: replication\n\nauth:\n  rootPassword: \"123456\"\n  # createDatabase: true\n  createDatabase: false\n  # database: \"my_database\"\n  database: \"\"\n  username: \"\"\n  password: \"\"\n  ##\n  replicationUser: replicator\n  ##\n  replicationPassword: \"123456\"\n```\n\n### 已知问题\n\n#### 主从复制失败\n\n`解决方案：删除从库，删除pvc，重新拉取主库数据`\n\n```\nkubectl scale sts -n mysql mysql-slave --replicas=0\nkubectl delete pvc -n mysql data-mysql-slave-0\nkubectl scale sts -n mysql mysql-slave --replicas=1\n```\n","tags":["MySQL"],"categories":["云原生"]},{"title":"【监控】二进制部署prometheus+alertmanager+grafana监控平台","url":"/2022/06/30/【监控】二进制部署prometheus+alertmanager+grafana监控平台/","content":"\n# 二进制部署prometheus+alertmanager+grafana监控平台\n\n下载二进制包：[Download | Prometheus](https://prometheus.io/download/)\n\n### 安装prometheus\n\n本文档使用LTS长期支持版：2.45.4\n\n```\nmkdir /usr/local/prometheus  # 新建存放prometheus组件目录\n# 安装\ntar zxvf prometheus-2.45.4.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/prometheus-2.45.4.linux-amd64 /usr/local/prometheus/prometheus\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/prometheus.service\n```\n\n```\n[Unit]\nDescription=Prometheus Server\nDocumentation=https://prometheus.io/docs/introduction/overview/\nAfter=network-online.target\n\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nRestart=on-failure\nExecStart=/usr/local/prometheus/prometheus/prometheus \\\n  --config.file=/usr/local/prometheus/prometheus/prometheus.yml \\\n  --storage.tsdb.path=/usr/local/prometheus/prometheus/data \\\n  --storage.tsdb.retention.time=60d \\\n  --web.enable-lifecycle\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable prometheus --now\n```\n\n#### 检查状态\n\n```\nsystemctl status prometheus\n```\n\n#### 验证\n\n访问`http://ip:9090`，访问到即可。\n\n![image-20240419141231845](/images/image-lmaq.png)\n\n### 安装alertmanager\n\n```\ntar zxvf alertmanager-0.27.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/alertmanager-0.27.0.linux-amd64 /usr/local/prometheus/alertmanager\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/alertmanager.service\n```\n\n```\n[Unit]\nDescription=Alert Manager\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nExecStart=/usr/local/prometheus/alertmanager/alertmanager \\\n  --config.file=/usr/local/prometheus/alertmanager/alertmanager.yml \\\n  --storage.path=/usr/local/prometheus/alertmanager/data\n\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 修改配置文件\n\n```\nvim /usr/local/prometheus/alertmanager/alertmanager.yml\n```\n\n```\nroute:\n  group_by: ['dingtalk']\n  group_wait: 1s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'dingtalk.webhook1'\n  routes:\n  - receiver: \"dingtalk.webhook1\"\n    match_re:\n      altername: \".*\"\nreceivers:\n  - name: 'dingtalk.webhook1'\n    webhook_configs:\n      - url: 'http://localhost:8060/dingtalk/webhook1/send'\n        send_resolved: true\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable alertmanager --now\n```\n\n#### 检查状态\n\n```\nsystemctl status alertmanager\n```\n\n#### 验证\n\n![image-20240419140922190](/images/image-ltls.png)\n\n\n\n### 安装node_exporter\n\n```\ntar zxvf node_exporter-1.7.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/node_exporter-1.7.0.linux-amd64 /usr/local/prometheus/node_exporter\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/node_exporter.service\n```\n\n```\n[Unit]\nDescription=node_exporter\nDocumentation=https://prometheus.io/\nAfter=network.target\n[Service]\n#User=prometheus\n#Group=prometheus\nExecStart=/usr/local/prometheus/node_exporter/node_exporter\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable node_exporter --now\n```\n\n#### 检查状态\n\n```\nsystemctl status node_exporter\n```\n\n#### 验证\n\n访问`http://ip:9100/metrics`，监控数据存在即可。\n\n![image-20240419141314354](/images/image-oktg.png)\n\n### 安装grafana\n\n下载地址：[Grafana get started | Cloud, Self-managed, Enterprise](https://grafana.com/get/?tab=self-managed)\n\n```\ntar zxvf node_exporter-1.7.0.linux-amd64.tar.gz -C /usr/local/prometheus\nmv /usr/local/prometheus/node_exporter-1.7.0.linux-amd64 /usr/local/prometheus/node_exporter\n```\n\n#### 注册系统服务\n\n```\nvim /etc/systemd/system/grafana-server.service\n```\n\n```\n[Unit]\nDescription=Grafana server\nDocumentation=http://docs.grafana.org\n[Service]\nType=simple\n#User=prometheus\n#Group=prometheus\nRestart=on-failure\nExecStart=/usr/local/prometheus/grafana/bin/grafana-server \\\n  --config=/usr/local/prometheus/grafana/conf/defaults.ini \\\n  --homepath=/usr/local/prometheus/grafana\n[Install]\nWantedBy=multi-user.target\n```\n\n#### 启动并设置开机自启\n\n```\nsystemctl enable grafana-server --now\n```\n\n#### 检查状态\n\n```\nsystemctl status grafana-server\n```\n\n浏览器访问`http://ip:3000`,账号admin，密码admin，首次根据提示修改密码\n\n![image-20240419141413034](/images/image-hlhn.png)\n\n\n\n### 添加监控\n\n#### 创建告警规则\n\n```\nmkdir -p /usr/local/prometheus/prometheus/rules\n```\n\n```\nvim node.yaml\n```\n\n```yaml\ngroups:\n- name: 服务器资源监控\n  rules:\n  - alert: 内存使用率过高\n    expr: 100 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 80\n    for: 3m \n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{ $labels.instance }} 内存使用率过高, 请尽快处理！\"\n      description: \"{{ $labels.instance }}内存使用率超过80%,当前使用率{{ $value }}%.\"\n          \n  - alert: 服务器宕机\n    expr: up == 0\n    for: 1s\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 服务器宕机, 请尽快处理!\"\n      description: \"{{$labels.instance}} 服务器node_exporter服务被关闭,当前状态{{ $value }}. \"\n \n  - alert: CPU高负荷\n    expr: 100 - (avg by (instance,job)(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 90\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} CPU使用率过高,请尽快处理！\"\n      description: \"{{$labels.instance}} CPU使用大于90%,当前使用率{{ $value }}%. \"\n      \n  - alert: 磁盘IO性能\n    expr: avg(irate(node_disk_io_time_seconds_total[1m])) by(instance,job)* 100 > 90\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流入磁盘IO使用率过高,请尽快处理！\"\n      description: \"{{$labels.instance}} 流入磁盘IO大于90%,当前使用率{{ $value }}%.\"\n \n \n  - alert: 网络流入\n    expr: ((sum(rate (node_network_receive_bytes_total{device!~'tap.*|veth.*|br.*|docker.*|virbr*|lo*'}[5m])) by (instance,job)) / 100) > 102400\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流入网络带宽过高，请尽快处理！\"\n      description: \"{{$labels.instance}} 流入网络带宽持续5分钟高于100M. RX带宽使用量{{$value}}.\"\n \n  - alert: 网络流出\n    expr: ((sum(rate (node_network_transmit_bytes_total{device!~'tap.*|veth.*|br.*|docker.*|virbr*|lo*'}[5m])) by (instance,job)) / 100) > 102400\n    for: 5m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.instance}} 流出网络带宽过高,请尽快处理！\"\n      description: \"{{$labels.instance}} 流出网络带宽持续5分钟高于100M. RX带宽使用量{$value}}.\"\n  \n  - alert: TCP连接数\n    expr: node_netstat_Tcp_CurrEstab > 10000\n    for: 2m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \" TCP_ESTABLISHED过高！\"\n      description: \"{{$labels.instance}} TCP_ESTABLISHED大于100%,当前使用率{{ $value }}%.\"\n \n  - alert: 磁盘容量\n    expr: 100-(node_filesystem_free_bytes{fstype=~\"ext4|xfs\"}/node_filesystem_size_bytes {fstype=~\"ext4|xfs\"}*100) > 90\n    for: 1m\n    labels:\n      severity: 严重告警\n    annotations:\n      summary: \"{{$labels.mountpoint}} 磁盘分区使用率过高，请尽快处理！\"\n      description: \"{{$labels.instance}} 磁盘分区使用大于90%，当前使用率{{ $value }}%.\"\n```\n\n#### 修改配置文件\n\n```\nvim /usr/local/prometheus/prometheus/prometheus.yml\n```\n\n```yaml\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n           - localhost:9093 # alertmanager地址，先写上\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  - \"rules/*.yml\"  # 告警规则\n  # - \"first_rules.yml\"\n  # - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n\n    static_configs:\n      - targets: [\"localhost:9090\"]  # 监控本主机\n\n  - job_name: 'alertmanager'\n    scrape_interval: 10s\n    static_configs:\n    - targets: ['localhost:9093'] # 监控alertmanager 主机\n\n  - job_name: 'node-exporter'\n    scrape_interval: 10s\n    static_configs:\n    - targets: ['localhost:9100'] # 监控节点，节点安装node_exporter\n      labels:\n        instance: Prometheus服务器\n```\n\n#### 验证数据\n\n重启prometheus\n\n```\nsystemctl restart prometheus\n```\n\n![image-20240419141800186](/images/image-bjzo.png)\n\n### grafana集成prometheus\n\n#### 浏览器添加数据源\n\n![image-20240419141921453](/images/image-fnfh.png)\n\n添加Prometheus地址，我这里是同一台安装，使用localhost\n\n![image-20240419142016950](/images/image-kvgk.png)\n\n最后保存即可，不需要其他配置。\n\n#### 添加dashboard\n\n这里使用1860id，节点监控仪表盘，更多仪表盘访问官方获取：[Dashboards | Grafana Labs](https://grafana.com/grafana/dashboards/)\n\n![image-20240419142355115](/images/image-qoik.png)","tags":["prometheus"],"categories":["监控"]},{"title":"【云原生】calico开启双协议栈（IPv4/IPv6）","url":"/2022/05/17/【云原生】calico开启双协议栈（IPv4、IPv6）/","content":"\n### **前提条件**\n\n* 节点配置ipv4/ipv6地址\n\n* k8s集群>=1.23\n\n* 安装calico插件>=3.23\n\n#### 地址规划\n\n| **网络**       | **ipv4**      | **ipv6**   |\n| ------------ | ------------- | ---------- |\n| pod-cidr     | 10.244.0.0/16 | 2000::/112 |\n| service-cidr | 10.96.0.0/12  | 3000::/112 |\n\n`注意：ipv6掩码位需要>=108，否则kube-apiserver会报错`\n\n### **网卡同时配置ipv4/ipv6**\n\n```\nvim /etc/sysconfig/network-scripts/ifcfg-XXX\n```\n\n```\nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=none\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=yes\nIPV6_ADDR_GEN_MODE=stable-privacy\nIPV6ADDR=2031:0:130c::90         # 改为实际的ipv6地址\nIPV6_DEFAULTGW=2031:0:130c::1    # 改为实际的ipv6网关地址\nIPV6_DEFROUTE=yes\nDEVICE=ens192\nONBOOT=yes\nIPADDR=10.20.13.90\nPREFIX=24\nGATEWAY=10.20.13.1\nDNS1=218.2.2.2\nDNS2=2001:4860:4860::8888        # ipv6 dns 不用动\nIPV6_PRIVACY=no\n```\n\n```\nsystemctl restart network\n```\n\n#### 测试ipv4\n\n```\nping 10.20.13.90\n```\n\n#### 测试ipv6\n\n```\nping -6 2031:0:130c::90\n```\n\n### **节点开启ipv6（所有节点）**\n\n```\nvim /etc/sysctl.d/k8s.conf\n```\n\n```\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv6.conf.all.disable_ipv6 = 0\nnet.ipv6.conf.default.disable_ipv6 = 0\nnet.ipv6.conf.lo.disable_ipv6 = 0\nnet.ipv6.conf.all.forwarding = 1\n```\n\n```\nsysctl --system\n```\n\n### **k8s开启双栈（主节点）**\n\n#### 修改kube-apiserver\n\n```\n vim /etc/kubernetes/manifests/kube-apiserver.yaml\n```\n\n```\n--service-cluster-ip-range=10.96.0.0/12,3000::/112\n```\n\n![](/images/image-vbud.png)\n\n#### 修改kube-controller-manager\n\n```\nvim /etc/kubernetes/manifests/kube-controller-manager.yaml\n```\n\n![](/images/image-vthx.png)\n\n#### 修改kube-proxy\n\n```\nkubectl edit cm -n kube-system kube-proxy\n```\n\n![](/images/image-veqq.png)\n\n重启kube-proxy容器\n\n```\nkubectl get pod -n kube-system | grep kube-proxy | awk '{print $1}' | xargs kubectl delete pod -n kube-system\n```\n\n#### calico 配置双栈\n\n```\nkubectl edit cm -n kube-system calico-config\n```\n\n```\n\"ipam\": {\n    \"type\": \"calico-ipam\",\n    \"assign_ipv4\": \"true\",\n    \"assign_ipv6\": \"true\"\n},\n```\n\n![](/images/image-zwor.png)\n\n```\nkubectl edit ds -n kube-system calico-node\n```\n\n```\n# 设置自动获取ipv6地址\n- name: IP6\n  value: autodetect\n# felix启用ipv6支持\n- name: FELIX_IPV6SUPPORT\n  value: \"true\"\n# 设置ipv6地址池，与pod-cidr一致，官方说使用kubeadm安装的集群不需要配置，但是没试过。\n- name: CALICO_IPV6POOL_CIDR\n  value: 2000::/112\n# pod通信使用vxlan封装，如果节点跨子网，使容器之间可以访问\n- name: CALICO_IPV6POOL_VXLAN\n  value: CrossSubnet\n# 开启ipv6 NAT功能，使容器可以访问外部的ipv6地址\n- name: CALICO_IPV6POOL_NAT_OUTGOING\n  value: \"true\"\n```\n\n![](/images/image-zktz.png)\n\n等待calico重启\n\n#### 修改kubeadm-config\n\n上方修改的apiserver、controller-manager是静态文件，如果集群升级kubeadm-config会重置静态文件，导致ipv6的配置消失，所以将这个配置文件同时修改\n\n```\nkubectl edit cm -n kube-system kubeadm-config\n```\n\n![](/images/image-bitu.png)\n\n### **验证**\n\n#### 查看ipv6地址池\n\n```\nkubectl get ippool\n```\n\n![](/images/image-rkbg.png)\n\n#### pod获取ipv6地址\n\n重启pod，查看获取的ipv6地址\n\n```\nkubectl delete pod -n xxxx xxxxxx-xxx-xxx --force\n\nkubectl get pod - n xxxx xxxxxx-xxx-xxx -o jsonpath={.status.podIPs}\n```\n\n![](/images/image-tbnz.png)\n\n#### services配置双栈\n\n```\nkubectl edit svc -n xxxx xxxxx-svc\n```\n\n![](/images/image-yoqx.png)\n\n* `SingleStack`：单栈 Service。控制面使用第一个配置的服务集群 IP 范围为 Service 分配集群 IP\n\n* `PreferDualStack`：为 Service 分配 IPv4 和 IPv6 集群 IP 地址。\n\n* `RequireDualStack`：从 IPv4 和 IPv6 的地址范围分配 Service 的`ClusterIPs`。\n\n  从 `ipFamilies` 数组中第一个ip地址的 `ClusterIPs`作为`ClusterIP`\n\n`注意：ipFamilies 字段修改是有条件的：你可以添加或删除第二个 IP 地址族， 但你不能更改现有 Service 的主要 IP 地址。`\n\n```\nkubectl describe svc -n xxxx xxxxxx-svc\n```\n\n![](/images/image-unvc.png)\n\n### **service示例**\n\n#### 新增service单栈ipv6\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  ipFamilyPolicy: SingleStack\n  ports:\n  - nodePort: 30080\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n#### service双栈\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: default\nspec:\n  ipFamilies:\n  - IPv6\n  - IPv4\n  ipFamilyPolicy: PreferDualStack\n  ports:\n  - nodePort: 30081\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  sessionAffinity: None\n  type: NodePort\n```\n\n参考链接：[*IPv4/IPv6 双协议栈 | Kubernetes*](https://kubernetes.io/zh-cn/docs/concepts/services-networking/dual-stack/)、[*自定义 Calico 配置 |Calico 文档 (*](https://docs.tigera.io/calico/latest/networking/ipam/ipv6#enable-dual-stack)[*tigera.io*](http://tigera.io)[*)*](https://docs.tigera.io/calico/latest/networking/ipam/ipv6#enable-dual-stack)\n","tags":["CNI","Kubernetes","calico"],"categories":["云原生"]},{"title":"【云原生】Kubernetes 安装CNI插件calico并设置双协议栈","url":"/2022/05/16/【云原生】Kubernetes 安装CNI插件calico并设置双协议栈/","content":"\n\n官网文档：[为本地部署安装 Calico 网络和网络策略 |Calico 文档 --- Install Calico networking and network policy for on-premises deployments | Calico Documentation (](https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico)[tigera.io](http://tigera.io)[)](https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico)\n\n#### 下载calico清单文件\n\n```\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml -O\n```\n\n#### 查看cluster-cidr\n\n```\n# 查看pod subnet，设置网络组件时须保持一致\ncat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep \"cluster-cidr\"\n# 查看service subnet，这一步不需要，只记录\ncat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep \"service-cluster\"\n```\n\n#### 修改yaml文件\n\n支持IPv4/IPv6双协议栈\n\n```\nvim calico.yaml\n```\n\n![](/images/image-sfca.png)\n\n![](/images/image-njkt.png)\n\n#### 部署calico\n\n```\nkubectl apply -f calico.yaml\n```\n","tags":["CNI","Kubernetes","calico"],"categories":["云原生"]},{"title":"【云原生】k8s节点系统初始化+内核优化","url":"/2022/05/14/【云原生】k8s节点系统初始化+内核优化/","content":"\n### 初始化系统\n\n#### 网络配置\n\n```\nvim /etc/sysconfig/network-scripts/ifcfg-ens192\n​\n# 指定连接类型为以太网\nTYPE=Ethernet\n# 指定网卡名称\nNAME=ens192\n# 指定设备名称\nDEVICE=ens192\n# 指定开机自动启用这个连接\nONBOOT=yes\n# 指定不使用代理方法\nPROXY_METHOD=none\n# 指定不仅仅在浏览器中使用代理\nBROWSER_ONLY=no\n# 指定自动分配地址的方式为无（即手动配置IP地址）\nBOOTPROTO=none\n# 指定默认路由开启\nDEFROUTE=yes\n# 指定IPv4连接失败时不宣告严重错误\nIPV4_FAILURE_FATAL=no\n# 指定IPv4地址\nIPADDR=10.20.13.10\n# 指定IPv4地址的子网掩码为24\nPREFIX=24\n# 指定IPv4的网关地址\nGATEWAY=10.20.13.1\n​\n## 以下ipv6配置，若没有注释掉即可\n# 指定启用IPv6\nIPV6INIT=yes\n# 指定不自动配置IPv6地址\nIPV6_AUTOCONF=no\n# 指定默认IPv6路由开启\nIPV6_DEFROUTE=yes\n# 指定IPv6连接失败时不宣告严重错误\nIPV6_FAILURE_FATAL=no\n# 指定IPv6地址生成模式为稳定隐私模式\nIPV6_ADDR_GEN_MODE=stable-privacy\n# 指定IPv6地址\nIPV6ADDR=2031:0:130c::10/64\n# 指定IPv6的默认网关地址\nIPV6_DEFAULTGW=2031:0:130c::1\n​\n# 指定首选DNS服务器\nDNS1=114.114.114.114\n# 指定备用DNS服务器\nDNS2=2400:3200::1\n​\n# 重启网卡\nsystemctl restart network\n```\n\n![](/images/image-jtuv.png)\n\n#### 设置主机名\n\n```\nhostnamectl set-hostname k8s-master01\nhostnamectl set-hostname k8s-master02\n...\nhostnamectl set-hostname k8s-node01\nhostnamectl set-hostname k8s-node02\n...\n```\n\n#### host解析（非必要）\n\n```\ncat > /etc/hosts << EOF\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n10.20.13.10    k8s-master01\n10.20.13.11    k8s-master02\n10.20.13.12    k8s-master03\n10.20.13.13    k8s-node01\n10.20.13.14    k8s-node02\n10.20.13.15    k8s-node03\nEOF\n```\n\n#### 配置yum源\n\n```\nsed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' -i.bak /etc/yum.repos.d/CentOS-*.repo\n```\n\n#### 时间同步\n\n```\n# 设置时区（所有节点）\ntimedatectl set-timezone Asia/Shanghai\n​\n​\n# 服务端（其中一台节点，一般是master01）\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \n# 从公网同步\npool ntp.aliyun.com iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度\ndriftfile /var/lib/chrony/drift\n# 当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整\nmakestep 1.0 3\n# 启用硬件时钟同步功能，可以提高时钟的准确性\nrtcsync\n# 允许10.20.13.0/24网段范围内的主机与chrony进行时间同步\nallow 10.20.13.0/24\n# 将本地时钟设为stratum 10，stratum值表示时钟的准确度，值越小表示准确度越高\nlocal stratum 10\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证\nkeyfile /etc/chrony.keys\n# 指定时区为UTC\nleapsectz right/UTC\n# 指定日志文件存放目录\nlogdir /var/log/chrony\nEOF\nsystemctl restart chronyd ; systemctl enable chronyd\n​\n# 客户端\nyum install chrony -y\ncat > /etc/chrony.conf << EOF \n# 从服务端同步\npool 10.20.13.10 iburst\n# 指定使用ntp.aliyun.com作为时间服务器池，iburst选项表示在初始同步时会发送多个请求以加快同步速度\ndriftfile /var/lib/chrony/drift\n# 当系统时间与服务器时间偏差大于1秒时，会以1秒的步长进行调整。如果偏差超过3秒，则立即进行时间调整\nmakestep 1.0 3\n# 启用硬件时钟同步功能，可以提高时钟的准确性\nrtcsync\n# 指定使用的密钥文件路径，用于对时间同步进行身份验证\nkeyfile /etc/chrony.keys\n# 指定时区为UTC\nleapsectz right/UTC\n# 指定日志文件存放目录\nlogdir /var/log/chrony\nEOF\nsystemctl restart chronyd ; systemctl enable chronyd\n​\n#使用客户端进行验证\nchronyc sources -v\n```\n\n#### 关闭防火墙\n\n```\n# 关闭防火墙\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X && iptables -P FORWARD ACCEPT\nsystemctl disable firewalld --now\n```\n\n#### 禁用SELinux\n\n```\n# 禁用selinux\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=disabled/g' /etc/selinux/config\n```\n\n#### 关闭NetworkManager\n\n```\n# 关闭 NetworkManager\nsystemctl disable NetworkManager --now\n```\n\n#### 关闭swap\n\n```\n# 临时关闭swap\nswapoff -a\n# 永久关闭swap\nsed -i 's/.*swap.*/#&/g' /etc/fstab\n```\n\n#### 配置yum源\n\n参考地址：[*centos | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror*](https://mirrors.tuna.tsinghua.edu.cn/help/centos/)\n\n```\nsed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g'  -i.bak /etc/yum.repos.d/CentOS-*.repo\n```\n\n#### 安装系统工具\n\n```\nyum update -y && yum -y install wget psmisc jq vim net-tools nfs-utils telnet yum-utils device-mapper-persistent-data lvm2 git tar curl\n```\n\n#### 添加elrepo源\n\n```\nyum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y \nsed -i \"s@mirrorlist@#mirrorlist@g\" /etc/yum.repos.d/elrepo.repo \nsed -i \"s@elrepo.org/linux@mirrors.tuna.tsinghua.edu.cn/elrepo@g\" /etc/yum.repos.d/elrepo.repo \n```\n\n#### 升级内核\n\n原文地址：[yum更新Linux内核（CentOS 7） - Elijah Blog (](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)[sreok.cn](http://sreok.cn)[)](https://sreok.cn/archives/419a2d41-26b9-4c32-bc97-14c18c39e384)\n\n```\n# 查看可用内核\nyum -y --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available\n# 我这里选择的是稳定版kernel-ml   如需更新长期维护版本kernel-lt\nyum -y --enablerepo=elrepo-kernel  install  kernel-ml\n# 查看已安装那些内核\nrpm -qa | grep kernel\n# 查看默认内核\ngrubby --default-kernel\n# 若不是最新的使用命令设置\ngrubby --set-default $(ls /boot/vmlinuz-* | grep elrepo)\n# 重启生效\nreboot\n```\n\n#### 加载IPVS模块\n\n```\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\ncat > /etc/modules-load.d/k8s.conf <<EOF \n# IPVS 是 Linux 内核中的一个模块，用于实现负载均衡和高可用性。它通过在前端代理服务器上分发传入请求到后端实际服务器上，提供了高性能和可扩展的网络服务\nip_vs\n# IPVS 轮询调度算法\nip_vs_rr\n# IPVS 加权轮询调度算法\nip_vs_wrr\n# IPVS 哈希调度算法\nip_vs_sh\n# overlay是containerd默认使用的存储驱动，它提供了一种轻量级的、可堆叠的、逐层增量的文件系统,它通过在现有文件系统上叠加文件系统层来创建容器的文件系统视图。每个容器可以有自己的一组文件系统层，这些层可以共享基础镜像中的文件，并在容器内部进行修改。使用overlay可以有效地使用磁盘空间，并使容器更加轻量级\noverlay\n# nf_conntrack用于跟踪和管理网络连接，包括 TCP、UDP 和 ICMP 等协议。它是实现防火墙状态跟踪的基础\nnf_conntrack\n# ip_tables提供了对 Linux 系统 IP 数据包过滤和网络地址转换（NAT）功能的支持\nip_tables\n# 扩展了 iptables 的功能，支持更高效的 IP 地址集合操作\nip_set\n# 扩展了 iptables 的功能，支持更高效的数据包匹配和操作\nxt_set\n# 用户空间工具，用于配置和管理 xt_set 内核模块\nipt_set\n# 用于实现反向路径过滤，用于防止 IP 欺骗和 DDoS 攻击\nipt_rpfilter\n# 用于拒绝 IP 数据包，并向发送方发送响应，指示数据包被拒绝\nipt_REJECT\n# 用于实现 IP 封装在 IP（IP-over-IP）的隧道功能。它可以在不同网络之间创建虚拟隧道来传输 IP 数据包\nipip\nEOF\n​\nsystemctl restart systemd-modules-load.service\n```\n\n#### 配置ulimit\n\n```\nulimit -SHn 65535\ncat > /etc/security/limits.conf <<EOF\n# soft表示软限制，nofile表示一个进程可打开的最大文件数，默认值为1024。这里的软限制设置为655360，即一个进程可打开的最大文件数为655360\n* soft nofile 655360\n# hard表示硬限制，即系统设置的最大值。nofile表示一个进程可打开的最大文件数，默认值为4096。这里的硬限制设置为131072，即系统设置的最大文件数为131072\n* hard nofile 131072\n# nproc表示一个用户可创建的最大进程数，默认值为30720。即一个用户可创建的最大进程数为655350\n* soft nproc 655350\n# nproc表示一个用户可创建的最大进程数，默认值为4096。即系统设置的最大进程数为655350\n* hard nproc 655350\n# memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。这里的软限制设置为unlimited，即一个进程可锁定的最大内存为无限制\n* seft memlock unlimited\n# memlock表示一个进程可锁定在RAM中的最大内存，默认值为64 KB。即系统设置的最大内存锁定为无限制\n* hard memlock unlimitedd\nEOF\n```\n\n#### 修改内核参数\n\n```\ncat > /etc/sysctl.d/k8s.conf << EOF\n# 启用了IPv4的IP转发功能，允许服务器作为网络路由器转发数据包\nnet.ipv4.ip_forward = 1\n# 当使用网络桥接技术时，将数据包传递到iptables进行处理\nnet.bridge.bridge-nf-call-iptables = 1\n# 当该参数设置为1时，IPv6数据包将被传递到ip6tables进行处理；当该参数设置为0时，IPv6数据包将绕过ip6tables直接传递。默认情况下，这个参数的值是1\nnet.bridge.bridge-nf-call-ip6tables = 1\n# 允许在挂载文件系统时，允许被其他进程使用\nfs.may_detach_mounts = 1\n# 允许原始的内存过量分配策略，当系统的内存已经被完全使用时，系统仍然会分配额外的内存\nvm.overcommit_memory=1\n# 当系统内存不足（OOM）时，禁用系统崩溃和重启\nvm.panic_on_oom=0\n# 设置系统允许一个用户的inotify实例可以监控的文件数目的上限\nfs.inotify.max_user_watches=89100\n# 设置系统同时打开的文件数的上限\nfs.file-max=52706963\n# 设置系统同时打开的文件描述符数的上限\nfs.nr_open=52706963\n# 设置系统可以创建的网络连接跟踪表项的最大数量\nnet.netfilter.nf_conntrack_max=2310720\n# 设置TCP套接字的空闲超时时间（秒），超过该时间没有活动数据时，内核会发送心跳包\nnet.ipv4.tcp_keepalive_time = 600\n# 设置未收到响应的TCP心跳探测次数\nnet.ipv4.tcp_keepalive_probes = 3\n# 设置TCP心跳探测的时间间隔（秒）\nnet.ipv4.tcp_keepalive_intvl =15\n# 设置系统可以使用的TIME_WAIT套接字的最大数量\nnet.ipv4.tcp_max_tw_buckets = 36000\n# 启用TIME_WAIT套接字的重新利用，允许新的套接字使用旧的TIME_WAIT套接字\nnet.ipv4.tcp_tw_reuse = 1\n# 设置系统可以同时存在的TCP套接字垃圾回收包裹数的最大数量\nnet.ipv4.tcp_max_orphans = 327680\n# 设置系统对于孤立的TCP套接字的重试次数\nnet.ipv4.tcp_orphan_retries = 3\n# 启用TCP SYN cookies保护，用于防止SYN洪泛攻击\nnet.ipv4.tcp_syncookies = 1\n# 设置新的TCP连接的半连接数（半连接队列）的最大长度\nnet.ipv4.tcp_max_syn_backlog = 16384\n# 设置系统可以创建的网络连接跟踪表项的最大数量\nnet.ipv4.ip_conntrack_max = 65536\n# 关闭TCP时间戳功能，用于提供更好的安全性\nnet.ipv4.tcp_timestamps = 0\n# 设置系统核心层的连接队列的最大值\nnet.core.somaxconn = 16384\n​\n# 启用IPv6协议\nnet.ipv6.conf.all.disable_ipv6 = 0\n# 启用IPv6协议\nnet.ipv6.conf.default.disable_ipv6 = 0\n# 启用IPv6协议\nnet.ipv6.conf.lo.disable_ipv6 = 0\n# 允许IPv6数据包转发\nnet.ipv6.conf.all.forwarding = 1\nEOF\n​\nsysctl --system\n```\n\n### **安装k8s工具**\n\n版本查看：[*kubernetes-new-core-stable安装包下载\\_开源镜像站-阿里云 (*](https://mirrors.aliyun.com/kubernetes-new/core/stable/)[*aliyun.com*](http://aliyun.com)[*)*](https://mirrors.aliyun.com/kubernetes-new/core/stable/)\n\n```\n# 指定安装的k8s工具版本\nexport k8sVersion=v1.30\n```\n\n```\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/$k8sVersion/rpm/repodata/repomd.xml.key\nEOF\n```\n\n```\nyum install -y kubelet kubeadm kubectl\nsystemctl enable kubelet --now\n```\n","tags":["Kubernetes"]},{"title":"【虚拟化】v2v：proxmox虚拟机迁移ESXi（zst格式转换vmdk）","url":"/2022/04/21/【虚拟化】v2v：proxmox虚拟机迁移ESXi（zst格式转换vmdk）/","content":"\nk8sgpt官方文档：[*In-Cluster Operator - k8sgpt*](https://docs.k8sgpt.ai/getting-started/in-cluster-operator/)\n\nlocalai官方文档：[*Run with Kubernetes | LocalAI documentation*](https://localai.io/basics/kubernetes/)\n\n参考文献：[*K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! | by Tyler | ITNEXT*](https://itnext.io/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)\n\n### **安装LocalAI**\n\n#### 可选一：（kubectl方式）\n\n```\ncat > local-ai.yaml << EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: local-ai\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: models-pvc\n  namespace: local-ai\nspec:\n  storageClassName: nfs-storage # 实际的storageclass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-ai\n  namespace: local-ai\n  labels:\n    app: local-ai\nspec:\n  selector:\n    matchLabels:\n      app: local-ai\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: local-ai\n      name: local-ai\n    spec:\n      containers:\n        - env:\n          - name: DEBUG\n            value: \"true\"\n          name: local-ai\n          #image: quay.io/go-skynet/local-ai:latest\n          image: quay.io/go-skynet/local-ai:master-ffmpeg-core\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: models-volume\n              mountPath: /build/models\n      volumes:\n        - name: models-volume\n          persistentVolumeClaim:\n            claimName: models-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: local-ai\n  namespace: local-ai\nspec:\n  selector:\n    app: local-ai\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\nEOF\n```\n\n```\nkubectl apply -f local-ai.yaml\n```\n\n#### 可选二：（helm方式）\n\n```\nhelm repo add go-skynet https://go-skynet.github.io/helm-charts/\nhelm show values go-skynet/local-ai > values.yaml\n```\n\n```\nvim values.yaml\n```\n\n```\n\nreplicaCount: 1\n​\ndeployment:\n  image:\n    repository: quay.io/go-skynet/local-ai\n    tag: latest    # latest镜像23.5G\n    # tag: master-ffmpeg-core\n  env:\n    threads: 8  # 最好8cpu\n    context_size: 512\n  modelsPath: \"/models\"\n  download_model:\n    image: busybox\n  prompt_templates:\n    image: busybox\n  pullPolicy: IfNotPresent\n  imagePullSecrets: []\nresources: {}\npromptTemplates: {}\nmodels:\n  forceDownload: false\n  list:\n    - url: \"https://gpt4all.io/models/ggml-gpt4all-j.bin\" # 提前下载，放到models的pvc里\n      # basicAuth: base64EncodedCredentials\ninitContainers: []\nsidecarContainers: []\npersistence:\n  models: \n    enabled: true\n    annotations: {}\n    storageClass: nfs-storage # 实际的storageclassName\n    accessModes: ReadWriteMany\n    size: 10Gi\n    globalMount: /models\n  output:\n    enabled: true\n    annotations: {}\n    storageClass: nfs-storage # 实际的storageclassName\n    accessModes: ReadWriteMany\n    size: 5Gi\n    globalMount: /tmp/generated\nservice:\n  type: ClusterIP\n  port: 80\n  annotations: {}\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\nnodeSelector: {}\ntolerations: []\naffinity: {}\n```\n\n```\nhelm install local-ai go-skynet/local-ai -f values.yaml\n```\n\n### **下载模型**\n\nGPT4all-j下载地址（3.52G）：[*https://gpt4all.io/models/ggml-gpt4all-j.bin*](https://gpt4all.io/models/ggml-gpt4all-j.bin)\n\n下载后安装到models的pvc下\n\n#### 测试LocalAI GPT4All模型\n\n```\ncurl http://10.20.13.140:27410/v1/models\n```\n\n![](/images/image-vwti.png)\n\n```\ncurl http://10.20.13.140:27410/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"ggml-gpt4all-j.bin\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}],\n  \"temperature\": 0.7\n}'\n```\n\n![](/images/image-xjyf.png)\n\n### **安装k8sgpt客户端，诊断集群**\n\n下载地址：[*Installation - k8sgpt*](https://docs.k8sgpt.ai/getting-started/installation/)\n\n```\nrpm -ivh -i k8sgpt_amd64.rpm\n```\n\n```\nk8sgpt auth add --backend localai --model ggml-gpt4all-j.bin --baseurl http://10.20.13.140:27410/v1\n```\n\n```\nk8sgpt analyze --explain -b localai # 如果这一步卡住，很可能你的资源问题不能被确认\n```\n\n![](/images/image-riug.png)\n\n```\nk8sgpt analyze --explain -b localai --filter Pod # 只过滤Pod\n```\n\n![](/images/image-mmzs.png)\n\n#### 资源使用问题\n\n![](/images/image-wvvc.png)\n\n查看local-ai的pod在node01节点，如果正在运算可以看到top使用率\n\n![](/images/image-zuwt.png)\n\n### **集成k8sgpt-operator配置自动诊断**\n\n### **安装k8sgpt-operator**\n\n```\nhelm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n```\n\n### **创建K8sGPT**\n\n```\nvim k8sgpt.yaml\n```\n\n```\n\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-local\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    backend: localai\n    model: ggml-gpt4all-j.bin\n    baseUrl: http://local-ai.local-ai.svc.cluster.local:8080/v1\n  noCache: false\n  version: v0.3.8\n```\n\n```\nkubectl apply -f k8sgpt.yaml\n```\n\n#### 测试自动诊断\n\n创建一个错误的pod，镜像tag故意写一个不存在的\n\n```\n\ncat > broken-pod.yaml << EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pod\nspec:\n  containers:\n    - name: broken-pod\n      image: nginx:1.a.b.c\n      livenessProbe:\n        httpGet:\n          path: /\n          port: 90\n        initialDelaySeconds: 3\n        periodSeconds: 3\nEOF\n```\n\n![](/images/image-ycna.png)\n\n查看results\n\n![](/images/image-qjeq.png)\n\n查看详情\n\n```\nkubectl get results -n k8sgpt-operator-system xxxxxxxxx -o json\n```\n\n![](/images/image-rdoz.png)\n\n创建一个没有绑定的svc\n\n```\nkubectl create svc clusterip no-endpoint --tcp=80:80\n```\n\n查看result\n\n![](/images/image-nnat.png)\n\n![](/images/image-gbbr.png)\n\n### **注意：**\n\nai的回答不是统一的，有时候会有弱智回答，这个看主要大模型和机器的资源，如果回答比较模糊，可以删掉results，等待重建后再查看details\n","tags":["v2v"],"categories":["虚拟化"]},{"title":"【虚拟化】p2v：windows物理机转换vSphere虚拟机","url":"/2022/04/19/【虚拟化】p2v：windows物理机转换vSphere虚拟机/","content":"\n### 安装VMware vCenter Converter\n\n下载地址：<https://pan.baidu.com/s/1hFi69TUrF34WOHcH4Rl2uw?pwd=18y6>\n\n管理员身份运行VMware vCenter Converter\n\n### 选择本机\n\n![](/images/image-rjnj.png)\n\n### 目标vsphere地址和账号密码\n\n![](/images/image-padg.png)\n\n### 虚拟机名称和位置\n\n![](/images/image-bnfx.png)\n\n### 选择资源和存储\n\n![](/images/image-oxpl.png)\n\n### 按需修改虚拟机配置\n\n![](/images/image-pqsb.png)\n\n### 确认信息\n\n![](/images/image-kzbh.png)\n\n### 等待完成\n\n![](/images/image-fvyy.png)\n","tags":["p2v"],"categories":["虚拟化"]},{"title":"【虚拟化】p2p：Ghost备份恢复windows物理机","url":"/2022/04/18/【虚拟化】p2p：Ghost备份恢复windows物理机/","content":"\n备份windows\n\n1. 准备PE盘插入电脑，开机从U盘启动\n\n2. 通过ghost备份磁盘：依次点击Local -> Partition -> To Image\n\n3. 选择磁盘 ![](/images/image-hyem.png)\n\n4. 选择分区 ![](/images/image-uoms.png)\n\n5. 选择保存位置 ![](/images/image-ymju.png)\n\n6. 导出格式为GHO ![](/images/image-zkky.png)\n\n还原主机\n\n1. U盘启动进入pe系统\n\n2. 磁盘分区工具分区C盘MBR格式\n\n3. 打开Ghost工具依次点击：Local -> Partition -> From Image\n\n4. 选择备份的GHO文件\n\n5. 选择还原到C盘，开始还原\n\n已知问题\n\n1. not enough room on target partition**（目标分区容量不够）**\n\n2. An internal inconsistency has been detected**（****检测到内部不一致****）**\n\n   1. 排查思路：\n\n   2. 更换分区格式NTFS为FAT32，与恢复前的分区格式对应\n\n   3. 更换PE为OneKey Ghost，雨林木风出版的**（未验证）**\n\n   4. 使用GhostExplorer打开GHO文件，查看创建此GHO文件的GHOST的版本，使用相同版本\n\n   5. 使用GhostExplorer提取出文件，做成ISO镜像，安装到C盘\n","tags":["p2p"],"categories":["虚拟化"]},{"title":"【笔记】frp内网穿透工具","url":"/2022/02/18/【笔记】frp内网穿透工具/","content":"\n### 前提条件\n\n* 公网服务器\n\n* 安装frp\n\n  * github地址：[Releases · fatedier/frp (](https://github.com/fatedier/frp/releases)[github.com](http://github.com)[)](https://github.com/fatedier/frp/releases)\n\n### 服务端部署（公网服务器）\n\n`vim frps.toml`\n\n```\nbindPort = 7000\n \nwebServer.addr = \"0.0.0.0\"\nwebServer.port = 7500\nwebServer.user = \"admin\"\nwebServer.password = \"admin\"\n \n \n#transport.tls.force = true  # 服务端将只接受 TLS链接\n#auth.method = 'token'       # 客户端访问验证方式\n#auth.token = \"54321\" \t\t # 客户端访问验证密码，frpc要与frps一致\n \n \n# 自定义的监听的端口，所有对服务器该端口访问将被转发到本地内网，做了反向代理可不处理防火墙放行\n#vhostHTTPPort = 8000\n#vhostHTTPSPort = 45443\n```\n\n```\n./frps -c frps.toml\n```\n\n> 开放端口：\n>\n> * 7000（通信使用）\n>\n> * 7500（web页面）\n>\n> * 30001（即将被映射的端口）\n>\n> web页面：http\\://公网ip:7500\n\n### 客户端部署（内网服务器）\n\n`vim frpc.toml`\n\n```\nserverAddr = \"公网地址\"\nserverPort = 7000\n\n[[proxies]]\nname = \"test-tcp\"\ntype = \"tcp\"\nlocalIP = \"127.0.0.1\"\nlocalPort = 8080\nremotePort = 30001\n```\n\n```\n./frpc -c frpc.toml\n```\n","tags":["frp"],"categories":["Linux"]},{"title":"【网络】H3C-IRF堆叠+无线配置（实验环境）","url":"/2021/12/20/【网络】H3C-IRF堆叠+无线配置（实验环境）/","content":"\n1.为了保证网络的可靠性sw1与sw2做堆叠，逻辑上为一台交换机；\n\n2.划分vlan 10,20,30,40将宿舍，教学楼，办公楼，食堂区域划分到对应vlan中；\n\n3.vlan100为无线的管理vlan，vlan200为宿舍，办公，教学楼无线vlan；vlan300为食堂的无线vlan.\n\n4.所有用户通过DHCP服务器获取IP；\n\n5.要求食堂区域不可以和其他三个区域之间互访，只能访问INTERNET；\n\n6.在R1上配置NAT地址转换，实现内网用户访问INTERNET;\n\n```\n#首先修改sw2的编号为2，不可与sw1重复\nirf member 2 priority 1\n#在SW2上将用于堆叠的物理端口关闭\ninterface range Ten-GigabitEthernet2/0/49 t2/0/50\n shutdown\n#将端口加入堆叠组，在sw1和sw2分别配置\nirf-port 1/1\n port group interface Ten-GigabitEthernet1/0/49\n port group interface Ten-GigabitEthernet1/0/50\nirf-port 2/2\n port group interface Ten-GigabitEthernet2/0/49\n port group interface Ten-GigabitEthernet2/0/50\n#配置完成后在SW2上将物理端口启用，此时交换机会重启\ninterface range Ten-GigabitEthernet2/0/49 t2/0/50\n undo shutdown\n```\n\n### 核心交换机配置\n\n```\n#\n sysname SW1\n#\n irf mac-address persistent timer\n irf auto-update enable\n undo irf link-delay\n irf member 1 priority 1\n irf member 2 priority 1\n#\n dhcp enable\n#\nvlan 10\n#\nvlan 20\n#\nvlan 30\n#\nvlan 40\n#\nvlan 100\n#\nvlan 200\n#\nvlan 300\n#\nirf-port 1/1\n port group interface Ten-GigabitEthernet1/0/49\n port group interface Ten-GigabitEthernet1/0/50\n#\nirf-port 2/2\n port group interface Ten-GigabitEthernet2/0/49\n port group interface Ten-GigabitEthernet2/0/50\n#\ndhcp server ip-pool vlan10\n gateway-list 10.1.10.254\n network 10.1.10.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ndhcp server ip-pool vlan20\n gateway-list 10.1.20.254\n network 10.1.20.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ndhcp server ip-pool vlan30\n gateway-list 10.1.30.254\n network 10.1.30.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ndhcp server ip-pool vlan40\n gateway-list 10.1.40.254\n network 10.1.40.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ndhcp server ip-pool vlan200\n gateway-list 10.1.200.254\n network 10.1.200.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ndhcp server ip-pool vlan300\n gateway-list 10.2.210.254\n network 10.2.210.0 mask 255.255.255.0\n dns-list 8.8.8.8\n#\ninterface Bridge-Aggregation1\n port link-type trunk\n port trunk permit vlan all\n#\ninterface Route-Aggregation1\n ip address 100.1.1.2 255.255.255.252\n#\ninterface NULL0\n#\ninterface Vlan-interface10\n ip address 10.1.10.254 255.255.255.0\n dhcp server apply ip-pool vlan10\n#\ninterface Vlan-interface20\n ip address 10.1.20.254 255.255.255.0\n dhcp server apply ip-pool vlan20\n#\ninterface Vlan-interface30\n ip address 10.1.30.254 255.255.255.0\n dhcp server apply ip-pool vlan30\n#\ninterface Vlan-interface40\n ip address 10.1.40.254 255.255.255.0\n packet-filter 3000 inbound\n packet-filter 3000 outbound\n dhcp server apply ip-pool vlan40\n#\ninterface Vlan-interface200\n ip address 10.1.200.254 255.255.255.0\n dhcp server apply ip-pool vlan200\n#\ninterface Vlan-interface300\n ip address 10.2.210.254 255.255.255.0\n packet-filter 3000 inbound\n packet-filter 3000 outbound\n dhcp server apply ip-pool vlan300\n#\ninterface GigabitEthernet1/0/5\n port link-mode route\n combo enable fiber\n port link-aggregation group 1\n#\ninterface GigabitEthernet2/0/5\n port link-mode route\n combo enable fiber\n port link-aggregation group 1\n#\ninterface GigabitEthernet1/0/1\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet1/0/2\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet1/0/3\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet1/0/4\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet2/0/1\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet2/0/2\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet2/0/3\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface GigabitEthernet2/0/4\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n#\ninterface Ten-GigabitEthernet1/0/51\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n port link-aggregation group 1\n#\ninterface Ten-GigabitEthernet2/0/51\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n port link-aggregation group 1\n#\n ip route-static 0.0.0.0 0 100.1.1.1\n#\nacl advanced 3000\n rule 0 permit udp source-port range bootps bootpc\n rule 5 permit ip source 10.1.40.0 0.0.0.255\n rule 10 permit ip source 10.2.210.0 0.0.0.255\n rule 20 permit ip source 200.1.1.0 0.0.0.3\n rule 25 permit ip source 100.1.1.0 0.0.0.3\n rule 30 deny ip\n```\n\n### AC配置\n\n```\n#\n sysname AC\n#\nwlan global-configuration\n#\n dhcp enable\n#\nvlan 1\n#\nvlan 100\n#\nvlan 200\n#\nvlan 300\n#\ndhcp server ip-pool vlan100\n gateway-list 10.1.100.254\n network 10.1.100.0 mask 255.255.255.0\n#\nwlan service-template 1\n ssid sushe\n service-template enable\n#\nwlan service-template 2\n ssid jiaoxuelou\n service-template enable\n#\nwlan service-template 3\n ssid bangonglou\n service-template enable\n#\nwlan service-template 4\n ssid shitang\n service-template enable\n#\ninterface Bridge-Aggregation1\n port link-type trunk\n port trunk permit vlan all\n#\ninterface NULL0\n#\ninterface Vlan-interface100\n ip address 10.1.100.254 255.255.255.0\n dhcp server apply ip-pool vlan100\n#\ninterface Ten-GigabitEthernet1/0/24\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n port link-aggregation group 1\n#\ninterface Ten-GigabitEthernet1/0/25\n port link-mode bridge\n port link-type trunk\n port trunk permit vlan all\n combo enable fiber\n port link-aggregation group 1\n#\nuser-group system\n#\n wlan auto-ap enable\n#\nwlan ap-group bangonglou\n vlan 1\n vlan 100\n ap bangonglou\n ap-model WA6320-HCL\n  radio 1\n   radio enable\n   service-template 3 vlan 200\n  radio 2\n   radio enable\n   service-template 3 vlan 200\n  gigabitethernet 1\n#\nwlan ap-group default-group\n vlan 1\n vlan 100\n#\nwlan ap-group jiaoxuelou\n vlan 1\n vlan 100\n ap jiaoxuelou\n ap-model WA6320-HCL\n  radio 1\n   radio enable\n   service-template 2 vlan 200\n  radio 2\n   radio enable\n   service-template 2 vlan 200\n  gigabitethernet 1\n#\nwlan ap-group shitang\n vlan 1\n vlan 100\n ap shitang\n ap-model WA6320-HCL\n  radio 1\n   radio enable\n   service-template 4 vlan 300\n  radio 2\n   radio enable\n   service-template 4 vlan 300\n  gigabitethernet 1\n#\nwlan ap-group sushe\n vlan 1\n vlan 100\n ap sushe\n ap-model WA6320-HCL\n  radio 1\n   radio enable\n   service-template 1 vlan 200\n  radio 2\n   radio enable\n   service-template 1 vlan 200\n  gigabitethernet 1\n#\nwlan virtual-ap-group default-virtualapgroup\n#\nwlan ap bangonglou model WA6320-HCL \n serial-id H3C_2A-A2-D3-3C-0A-00\n vlan 1\n radio 1\n radio 2\n gigabitethernet 1\n#\nwlan ap jiaoxuelou model WA6320-HCL \n serial-id H3C_2A-A2-CF-87-09-00\n vlan 1\n radio 1\n radio 2\n gigabitethernet 1\n#\nwlan ap shitang model WA6320-HCL \n serial-id H3C_2A-A2-D7-65-0B-00\n vlan 1\n radio 1\n radio 2\n gigabitethernet 1\n#\nwlan ap sushe model WA6320-HCL \n serial-id H3C_2A-A2-CB-4A-08-00\n vlan 1\n radio 1\n radio 2\n gigabitethernet 1\n#\nreturn\n```\n\n### R1配置\n\n```\n#\n sysname R1\n#\nvlan 1\n#\ninterface Route-Aggregation1\n ip address 100.1.1.1 255.255.255.252\n#\ninterface GigabitEthernet0/0/0\n port link-mode route\n combo enable copper\n port link-aggregation group 1\n#\ninterface GigabitEthernet0/0/1\n port link-mode route\n combo enable copper\n port link-aggregation group 1\n#\ninterface GigabitEthernet0/0/2\n port link-mode route\n combo enable copper\n ip address 200.1.1.1 255.255.255.252\n nat outbound 2000\n#\n ip route-static 10.0.0.0 8 100.1.1.2\n#\nacl basic 2000\n rule 5 permit source 10.0.0.0 0.255.255.255\n#\n```\n","tags":["IRF"],"categories":["网络"]},{"title":"【云原生】k8s 安装metrics-server v0.7.1","url":"/2021/11/02/【云原生】k8s 安装metrics-server v0.7.1/","content":"\n官方github：[*Releases · kubernetes-sigs/metrics-server (*](https://github.com/kubernetes-sigs/metrics-server/releases/)[*github.com*](http://github.com)[*)*](https://github.com/kubernetes-sigs/metrics-server/releases/)\n\n#### v0.7.1安装\n\n```\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.1/components.yaml\n```\n\n##### **手动安装：**\n\n```\nkubectl apply -f - << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n  name: system:aggregated-metrics-reader\nrules:\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/metrics\n  verbs:\n  - get\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server-auth-reader\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server:system:auth-delegator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    k8s-app: metrics-server\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: metrics-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: metrics-server\n            namespaces:\n            - kube-system\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=10250\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        image: registry.k8s.io/metrics-server/metrics-server:v0.7.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          periodSeconds: 10\n        name: metrics-server\n        ports:\n        - containerPort: 10250\n          name: https\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-dir\n      nodeSelector:\n        kubernetes.io/os: linux\n      priorityClassName: system-cluster-critical\n      serviceAccountName: metrics-server\n      volumes:\n      - emptyDir: {}\n        name: tmp-dir\n---\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: v1beta1.metrics.k8s.io\nspec:\n  group: metrics.k8s.io\n  groupPriorityMinimum: 100\n  insecureSkipTLSVerify: true\n  service:\n    name: metrics-server\n    namespace: kube-system\n  version: v1beta1\n  versionPriority: 100\nEOF\n```\n\n#### 高可用安装：\n\n```\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml\n```\n\n> 此配置要求群集至少包含 2 个节点，可以在该节点上调度指标服务器\n\n##### **手动安装：**\n\n```\nkubectl apply -f - << EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n  name: system:aggregated-metrics-reader\nrules:\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/metrics\n  verbs:\n  - get\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server-auth-reader\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server:system:auth-delegator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    k8s-app: metrics-server\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: metrics-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: metrics-server\n            namespaces:\n            - kube-system\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=10250\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        image: registry.k8s.io/metrics-server/metrics-server:v0.7.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          periodSeconds: 10\n        name: metrics-server\n        ports:\n        - containerPort: 10250\n          name: https\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-dir\n      nodeSelector:\n        kubernetes.io/os: linux\n      priorityClassName: system-cluster-critical\n      serviceAccountName: metrics-server\n      volumes:\n      - emptyDir: {}\n        name: tmp-dir\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: v1beta1.metrics.k8s.io\nspec:\n  group: metrics.k8s.io\n  groupPriorityMinimum: 100\n  insecureSkipTLSVerify: true\n  service:\n    name: metrics-server\n    namespace: kube-system\n  version: v1beta1\n  versionPriority: 100\nEOF\n```\n\n### 问题排查：\n\n报错：\n\nFailed to scrape node\" err=\"Get \\\\\"[https://xx.xx.xx.xx:10250/metrics/resource\\\\](https://10.20.13.15:10250/metrics/resource\\\\)\": tls: failed to verify certificate: x509: cannot validate certificate for xx.xx.xx.xx because it doesn't contain any IP SANs\" node=\"xxx\"\n\n原因：\n\nkubelet 的10250端口使用的是https协议，连接需要验证tls证书\n\n```\nkubectl edit deployments.apps -n kube-system metrics-server\n```\n\n```\n- --kubelet-insecure-tls\n```\n\n![](/images/image-zrfd.png)\n","tags":["metrics-server"]},{"title":"【IDC】Lenovo服务器ESXi7.0.0安装ipmitool管理BMC","url":"/2021/10/31/【IDC】Lenovo服务器ESXi7.0.0安装ipmitool管理BMC/","content":"\n```\n# 获取bmc基本信息\nesxcli hardware ipmi bmc get\n```\n\n### 安装ipmitool\n\n下载地址：<https://vswitchzero.files.wordpress.com/2019/08/ipmitool-esxi-vib-1.8.11-2.zip>\n\n备用地址：<https://sreok.cn/upload/ipmitool-esxi-vib-1.8.11-2.zip>\n\n```\n# 安装\nesxcli software acceptance set --level=CommunitySupported\nesxcli software vib install -v /tmp/ipmitool-1.8.11-2.x86_64.vib\n```\n\n### 简单使用\n\n```\n# 修改密码\n[root@localhost:/tmp] /opt/ipmitool/ipmitool user list 1\nID  Name\t     Callin  Link Auth\tIPMI Msg   Channel Priv Limit\n2   USERID           false   false      true       ADMINISTRATOR\n[root@localhost:/tmp] /opt/ipmitool/ipmitool user set password 2 Passw0rd\n```\n\n```\n# 修改网络\n[root@localhost:/tmp] /opt/ipmitool/ipmitool lan set 1 ipsrc static\n[root@localhost:/tmp] /opt/ipmitool/ipmitool lan set 1 ipaddr 192.168.3.16\nSetting LAN IP Address to 192.168.3.16\n[root@localhost:/tmp] /ipmitool lan set 1 netmask 255.255.255.0\nSetting LAN Subnet Mask to 255.255.255.0\n[root@localhost:/tmp] /ipmitool lan set 1 defgw ipaddr 192.168.3.1\nSetting LAN Default Gateway IP to 192.168.3.1\n```\n\n### 更多操作\n\n```\n# 常用操作\n\n#查看ipmitool版本\n/opt/ipmitool/ipmitool -V \n\n#查看bmc版本\n/opt/ipmitool/ipmitool mc info\n\n#使BMC重新启动：\n/opt/ipmitool/ipmitool mc reset <warm|cold> #warm表示软重启；cold表示硬重启\n\n#----------------------------------------------------------------\n# ipmitool管理服务器\n\n#查看服务器状态\n/opt/ipmitool/ipmitool chassis power status\n#服务器开机\n/opt/ipmitool/ipmitool chassis power on\n#服务器关机\n/opt/ipmitool/ipmitool chassis power off\n#服务器重启\n/opt/ipmitool/ipmitool chassis power reset\n\n#----------------------------------------------------------------\n#ipmitool 配置ip\n\n#说明：[ChannelNo] 字段是可选的，ChannoNo为1(Share Nic网络)或者8（BMC独立管理网络）；设置网络参数，必须首先设置IP为静态，然后再进行其他设置；\n\n#查看ip配置：\n/opt/ipmitool/ipmitool lan print [ChannelNo]\n#修改IP为静态还是DHCP模式\n/opt/ipmitool/ipmitool lan set <ChannelNo> ipsrc<static/dhcp>\n#修改ip地址：\n/opt/ipmitool/ipmitool lan set <ChannelNo> ipaddr <IPAddress>\n#修改子网掩码：\n/opt/ipmitool/ipmitool lan set <ChannelNo> netmask <NetMask>\n#修改默认网关：\n/opt/ipmitool/ipmitool lan set <ChannelNo> defgw ipaddr <Gateway >\n\n#配置示例：\n/opt/ipmitool/ipmitool lan set 1 ipsrc static\n/opt/ipmitool/ipmitool lan set 1 ipaddr 192.168.2.60\n/opt/ipmitool/ipmitool lan set 1 netmask 255.255.0.0\n/opt/ipmitool/ipmitool lan set 1 defgw ipaddr 192.168.2.1\n\n#----------------------------------------------------------------\n#查看配置：\n/opt/ipmitool/ipmitool lan print 1\n#注：目前使用的服务器型号（浪潮、戴尔等），ChannoNo均为1，ipmi独立于操作系统而存在，控制服务器的开关机等\n#/opt/ipmitool/ipmitool lan print 1 报错解决方案\n 解决方案：\n 1、查看系统是否已经加载IPMI模块：lsmod | grep ipmi\n 2、加载模块modprobe ipmi_msghandler\nmodprobe ipmi_devintf\nmodprobe ipmi_si\n 3、再次执行查询命令：查看有无报错\n\n#----------------------------------------------------------------\n# ipmitool 管理用户权限\n说明：[ChannelNo] 字段是可选的，ChannoNo为1、2或8；BMC默认有2个用户：user id为1的匿名用户，user id为2的admin用户；<>字段为必选内容；<privilege level>：2为User权限，3为Operator权限，4为Administrator权限\n#查看用户信息：\n/opt/ipmitool/ipmitool user list [ChannelNo]\n#添加用户：\n/opt/ipmitool/ipmitool user set name <user id> <username>\n#查看用户权限：\n/opt/ipmitool/ipmitool channel getaccess [ChannelNo] <user id>\n#设置用户权限：\n/opt/ipmitool/ipmitool channel setaccess [ChannelNo] <user id>[callin=on|off] [ipmi=on|off] [link=on|off] [privilege=level]\n#privilege的值定义如下；\n 1 callback\n 2 user\n 3 operator\n 4 administrator\n 5 OEM\n#示例\n#ipmitool channel setaccess 1 3 callin=off ipmi=on link=notallow=4\n#设置用户密码：\n/opt/ipmitool/ipmitool user set password <user id> <password>\n#示例\n#/opt/ipmitool/ipmitool user set password 3 123456\n#启用/禁用用户\n/opt/ipmitool/ipmitool user enable/disable <user id>\n\n#----------------------------------------------------------------\n\n# 重置\n/opt/ipmitool/ipmitool bmc reset <warm|cold> \n/opt/ipmitool/ipmitool mc reset <warm|cold> \n```\n","tags":["BMC"]},{"title":"【IDC】DELL服务器ESXI 7.0.0安装RACADM与iSM管理iDRAC","url":"/2021/10/26/【IDC】DELL服务器ESXI 7.0.0安装RACADM与iSM管理iDRAC/","content":"\n## DELL服务器不关机重置iDRAC密码\n\n参考文章：[如何在VMware ESXi上安装RACADM 并配置iDRAC IP | Dell 中国](https://www.dell.com/support/kbdoc/zh-cn/000193731/%E5%A6%82%E4%BD%95%E5%9C%A8esxi-%E4%B8%8A%E5%AE%89%E8%A3%85racadm-%E5%B9%B6%E9%85%8D%E7%BD%AEidrac-ip?lang=zh#a)\n\n输入服务码或型号：[支持 | Dell 中国](https://www.dell.com/support/home/zh-cn/products)\n\n### 下载RACADM\n\n在驱动中搜索iDRAC Tools for ESXi关键字\n\n**注意：在所在服务器型号没找到驱动，可以尝试相邻型号服务器。**\n\n![](/images/image-letp.png)\n\n![](/images/image-kmlq.png)\n\n![](/images/image-kcwi.png)\n\n### 安装RACADM\n\n可选一：操作系统安装软件包\n\n```\nesxcli software vib install -d /opt/DellEMC-iDRACTools-Web-ESX70i.VIB-10.2.0.0-4631_A00.zip\n```\n\n可选二：通过web页面安装，注意：路径拼接是虚拟盘路径\n\n![](/images/image-zpvw.png)\n\n![](/images/image-zssg.png)\n\n### 修改iDRAC IP（可选）\n\n```\nracadm setniccfg -s 192.168.0.126 255.255.255.0 192.168.0.1\n```\n\n### 重置密码\n\n```\n#iDRAC9\n重置 - 这将重新启动 iDRAC 而不更改任何 iDRAC 配置设置。\n例子： racadm racreset\n\n重置（强制）— 添加 -f 到任何 iDRAC 命令会强制执行 命令。\n例子： racadm racreset -f\n\n将 iDRAC 重置为出厂默认值 — 这会将所有更改重置回出厂设置，但 会保留 用户和网络设置。\n例子： racadm racresetcfg\n\n将 iDRAC 配置重置为默认值，并将用户重置为发运值。\n例子： racadm racresetcfg -all\n\n将 iDRAC 配置重置为默认值，并将用户重置为 的rootcalvin用户名和密码。\n例子：racadm racresetcfg -rc\n\n将 iDRAC 配置重置为自定义默认值。\n例子： racadm racrsetcfg -custom\n```\n\n## 扩展\n\n### 安装iSM（Dell EMC iDRAC Service Module）对iDRAC硬重启\n\n安装方法同RACADM\n\n```\n# 硬重启idrac\n/opt/dell/srvadmin/iSM/bin/Invoke-iDRACHardReset\n```\n\n### 查看应用\n\n```\n[root@localhost:~] esxcli software vib list | grep racadm\nracadm                           10.2.0.0.4631-DEL.700.0.0.15843807     DEL      PartnerSupported  2022-03-03\n[root@localhost:~] esxcli software vib list | grep ism\ndcism                            3.5.1.1952-DEL.700.0.0.14828939        DEL      VMwareAccepted    2022-03-03\n```\n\n### 卸载应用\n\n```\n[root@localhost:~] esxcli software vib remove --vibname=dcism\nRemoval Result\n   Message: Operation finished successfully.\n   Reboot Required: false\n   VIBs Installed: \n   VIBs Removed: DEL_bootbank_dcism_3.5.1.1952-DEL.700.0.0.14828939\n   VIBs Skipped: \n[root@localhost:~] esxcli software vib remove --vibname=racadm\nRemoval Result\n   Message: Operation finished successfully.\n   Reboot Required: false\n   VIBs Installed: \n   VIBs Removed: DEL_bootbank_racadm_10.2.0.0.4631-DEL.700.0.0.15843807\n   VIBs Skipped: \n[root@localhost:~] \n```\n","tags":["iDrac"],"categories":["IDC"]},{"title":"【监控】zabbix 监控接入钉钉告警","url":"/2021/09/25/【监控】zabbix 监控接入钉钉告警/","content":"\n## zabbix 监控接入钉钉告警\n\n### 创建群聊\n\n![image-20240418114025103](/images/image-zlhb.png)\n\n### 添加机器人\n\n![image-20240418114122619](/images/image-vxfw.png)\n\n#### 配置安全设置为加签，并记录Webhook和加签密钥\n\n![image-20240418114101147](/images/image-qete.png)\n\n### 编写告警脚本\n\n在alertscripts目录下添加以下脚本，修改Webhook与加签密钥\n\n```\n#!/usr/bin/python3\n# coding:utf-8 \n\nimport time\nimport hmac\nimport hashlib\nimport base64\nimport urllib.parse\nimport requests\nimport json\nimport sys\n\n\napi_url = 'https://oapi.dingtalk.com/robot/send?access_token=' \\\n              'xxxxxxxxxxxxxxxxxxxxxxxxxxxxx'   # webhook token粘贴在这里\n\n\n# 计算签名\ndef get_timestamp_sign():\n    \"\"\"\n    official url https://open.dingtalk.com/document/robots/customize-robot-security-settings\n    :return: timestamp, sign\n    \"\"\"\n    timestamp = str(round(time.time() * 1000))\n    secret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'  # 加签密钥粘贴在这里\n    secret_enc = secret.encode('utf-8')\n    string_to_sign = '{}\\n{}'.format(timestamp, secret)\n    string_to_sign_enc = string_to_sign.encode('utf-8')\n    hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()\n    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))\n    return timestamp, sign\n\n\n# 获取签名计算后的链接\ndef get_signed_url():\n    \"\"\"\n    :拼接url format --> https://oapi.dingtalk.com/robot/send?access_token=XXXXXX&timestamp=XXX&sign=XXX\n    :return:  webhook\n    \"\"\"\n    timestamp, sign = get_timestamp_sign()\n    webhook = api_url + \"&timestamp=\" + timestamp + \"&sign=\" + sign\n    return webhook\n\n\n# 定义webhook消息模式\ndef get_webhook(mode):\n    if mode == 0:                 # 仅关键字\n        webhook = api_url\n    elif mode == 1 or mode == 2:  # 关键字+加签 或 关键字+加签+ip\n        webhook = get_signed_url()\n    else:\n        webhook = \"\"\n        print(\"error! mode:   \", mode, \"  webhook :  \", webhook)\n    print(webhook)\n    return webhook\n\n\ndef get_message(text, user_info):\n    message = {\n        \"msgtype\": \"text\",  # 有text, \"markdown\"、link、整体跳转ActionCard 、独立跳转ActionCard、FeedCard类型等\n        \"text\": {\n            \"content\": text  # 消息内容\n        },\n        \"at\": {\n            \"atMobiles\": [\n                user_info,\n            ],\n            \"isAtAll\": False  # 是否是发送群中全体成员\n        }\n    }\n    return message\n\n\ndef send_ding_message(text, user_info):\n    \"\"\"\n    model: 0 --> 关键字; 1 --> 关键字 +加签; 2 --> 关键字+加签+IP\n    :param text:\n    :param user_info:\n    :return:\n    \"\"\"\n    webhook = get_webhook(1)\n    # 构建请求头部\n    header = {\n        \"Content-Type\": \"application/json\",\n        \"Charset\": \"UTF-8\"\n    }\n    message = get_message(text, user_info)\n    message_json = json.dumps(message)\n    info = requests.post(url=webhook, data=message_json, headers=header).json()\n    code = info[\"errcode\"]\n    errmsg = info[\"errmsg\"]\n    now_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n    if code == 0:\n        print(now_time, ':Message sent successfully, return info:{} {}\\n'.format(code, errmsg))\n    else:\n        print(now_time + \":Message sending failed, return info:{} {}\\n\".format(code, errmsg))\n        exit(3)\n\n\nif __name__ == \"__main__\":\n    t = sys.argv[3]\n    u = sys.argv[1]\n    send_ding_message(t, u)\n```\n\n![image-20240418114455247](/images/image-vkbg.png)\n\n#### 测试发送信息\n\n```shell\n./dingding.py all subject \"test message\"\n```\n\n![image-20240418115015785](/images/image-lvop.png)\n\n### 添加告警媒介\n\n![image-20240418114558534](/images/image-bjvx.png)\n\n![image-20240418114614275](/images/image-cmfp.png)\n\n![image-20240418114625802](/images/image-yzri.png)\n\n### 添加动作\n\n触发器触发后告警给用户群组\n\n添加用户告警媒介，收件人为钉钉注册的手机号，如果发送的是具体人会@。\n\n![image-20240418140140808](/images/image-pihp.png)\n\n![image-20240418115407582](/images/image-mewc.png)\n\n![image-20240418115426344](/images/image-ecmr.png)\n\n#### 配置告警消息\n\n```\n当前状态: {TRIGGER.STATUS}\n告警主机: {HOST.NAME}\n告警 IP: {HOST.IP}\n告警时间: {EVENT.DATE}-{EVENT.TIME}\n告警等级: {TRIGGER.SEVERITY}\n告警信息: {TRIGGER.NAME}\n告警项目: {TRIGGER.KEY1}\n```\n\n![image-20240418115452935](/images/image-uunn.png)\n\n### 测试触发告警\n\n占用cpu单核心负载，根据核数开对应的终端，每个终端运行以下命令。\n\n```\nwhile : ; do  openssl speed; done\n```\n\n当前是4核主机，开四个线程\n\n![image-20240418135821593](/images/image-gttg.png)\n\n![image-20240418141834590](/images/image-bxsn.png)","tags":["zabbix"],"categories":["监控"]},{"title":"【云计算】使用kolla-ansible部署openstack-zed版（Rocky9.3）","url":"/2021/08/15/【云计算】使用kolla-ansible部署openstack-zed版（Rocky9.3）/","content":"\n\n### 安装Python\n\n```\nwget https://www.python.org/ftp/python/3.9.21/Python-3.9.21.tgz\ntar zxvf Python-3.9.21.tgz\ncd Python-3.9.21\n./configure --prefix=/usr/local/python3\nmake && make install\nrm -rf /usr/bin/python /usr/bin/python2 /usr/bin/pip /usr/bin/pip2\nln -s /usr/local/python3/bin/python3.9 /usr/local/bin/python\nln -s /usr/local/python3/bin/pip3.9 /usr/local/bin/pip\nln -s /usr/bin/python2.7 /usr/bin/python2\n# python3环境变量\nexport PATH=$PATH:/usr/local/python3/bin/\ncat >> /etc/profile < EOF\nexport PATH=$PATH:/usr/local/python3/bin/\nEOF\n\n#pip加速\nmkdir -pv ~/.pip\ncat > ~/.pip/pip.conf << EOF\n[global]\nindex-url=http://mirrors.aliyun.com/pypi/simple/\n[install]\ntrusted-host=mirrors.aliyun.com\nEOF\n\n\n#解决yum不能使用python3问题\nsed -i '1s/python/python2/' /usr/bin/yum\nsed -i '1s/python/python2/' /usr/libexec/urlgrabber-ext-down\npython -V\npip -V\n```\n\n### 安装ansible、kolla-ansible\n\n```\npip install setuptools_rust\npip install -U pip\npip install ansible==2.9.27\npip install kolla-ansible==9.3.0\npip list | grep ansible\n```\n\n已知问题：\n\nERROR! Unexpected Exception, this is probably a bug: cannot import name 'AnsibleCollectionLoader'\n\n```\npip uninstall ansible\nrm -rf /usr/local/python3/lib/python3.9/site-packages/ansible\npip install ansible==2.9.27\n```\n\n### 安装openstack\n\n```\ncp -rv /usr/local/python3/share/kolla-ansible/ansible/inventory/* .\n\nmkdir -pv /etc/kolla/\ncp -rv /usr/local/python3/share/kolla-ansible/etc_examples/kolla/* /etc/kolla/\n```\n\n```\n# 生成密码\nkolla-genpwd\n# 修改登录密码\nvim /etc/kolla/passwords.yml\nkeystone_admin_password: 123456\n```\n\n```\nvim /etc/kolla/globals.yml\n//第14行和第15行，选择下载的基础镜像，5选1\n# Valid options are ['centos', 'debian', 'oraclelinux', 'rhel', 'ubuntu']\nkolla_base_distro: \"centos\"\n \n//第17行和第18行，选择的安装方法，2选1。binary二进制安装，source源码安装\n# Valid options are [ binary, source ]\nkolla_install_type: \"source\"\n \n//第20行和第21行，选择OpenStack的版本标签，详细请看：https://releases.openstack.org/\n# Valid option is Docker repository tag\nopenstack_release: \"zed\"\n \n//第23行和第24行，存放配置文件的位置\n# Location of configuration overrides\n#node_custom_config: \"/etc/kolla/config\"　　//默认存放地址\n \n//第31行，OpenStack内部管理网络地址，通过该IP访问OpenStack Web页面进行管理。如果启用了高可用，需要设置为VIP（漂移IP）\nkolla_internal_vip_address: \"192.168.128.240\"\n \n//第87行，OpenStack内部管理网络地址的网卡接口\nnetwork_interface: \"ens32\"\n \n//第105行，OpenStack外部（或公共）网络的网卡接口，可以是vlan模式或flat模式。\n//此网卡应该在没有IP地址的情况下处于活动，如果不是，那么OpenStack云平台中的云主机实例将无法访问外部网络。（存在IP时br-ex桥接就不成功）\nneutron_external_interface: \"ens33\"\n \n//第190行，关闭高可用\nenable_haproxy: \"no\"\n \n//第213行，关闭cinder（块存储）\n#enable_cinder: \"no\"\n \n//第443行和第444行,指定nova-compute守护进程使用的虚拟化技术。（kvm好像有点问题，大家可以试试，看看你们能不能过nova下载）\n//nova-compute是一个非常重要的守护进程，负责创建和终止虚拟机实例，即管理虚拟机实例的生命周期\n# Valid options are [ qemu, kvm, vmware, xenapi ]\nnova_compute_virt_type: \"qemu\"        #我选择的是KVM\n```\n\n```\n#安装bootstrap-servers部署OpenStack所需的依赖包。该包由kolla-ansible提供，包含docker。\n \nkolla-ansible -i /etc/kolla/all-in-one bootstrap-servers\n \n#对当前主机进行预部署（检测），直接看最后的统计即可。如果检测报错，可以查看前面的TASK:[precheck ...]部分，可以快速定位到错误\nkolla-ansible -i /etc/kolla/all-in-one prechecks\n \n#拉取镜像\nkolla-ansible -i /etc/kolla/all-in-one pull \n\n#开始部署\nkolla-ansible -i /etc/kolla/all-in-one deploy\n\n#验证部署，并且生成/etc/kolla/admin-openrc.sh\nkolla-ansible -i /etc/kolla/all-in-one post-deploy\n\n#可以打印出详情；\n# kolla-ansible -i /etc/kolla/all-in-one post-deploy  -vvv \n```\n\n已知问题：\n\nERROR! the playbook: /usr/local/python3/ansible/site.yml could not be found\n\n```\ncp -r /usr/local/python3/share/kolla-ansible/ansible /usr/local/python3/\n```\n","tags":["kolla-ansible"],"categories":["openstack"]},{"title":"【故障排查】Linux重启后磁盘号错乱问题","url":"/2021/07/19/【故障排查】Linux重启后磁盘号错乱问题/","content":"\n### 问题描述：\n\n> linux拥有多块磁盘设备，重启后磁盘号更换，导致使用/dev/sda挂载的文件切换到其他磁盘号。\n\n重启前：\n\n![](/images/1721035729171.jpg)\n\n重启后：\n\n![](/images/1721035729179.png)\n\n### 解决方案\n\n#### 文件系统（须分区）\n\n##### 方式一：使用UUID（推荐）\n\n```shell\n# 查看分区UUID\nblkid\n# 或者\nlsblk -o name,maj:min,rm,size,ro,type,mountpoint,label,uuid\n```\n\n```shell\n# 修改前\n/dev/sdb1       /data0       xfs       defaults       1 2\n# 修改后\nUUID=79ba7c2e-23da-405e-95c0-6cdd215a984a       /data0       xfs       defaults       1 2\n```\n\n##### 方式二：使用LABEL（只能是ext2、ext3、ext4分区类型，不支持xfs分区）\n\n```shell\ne2label /dev/sdb1 labelname\n```\n\n```shell\n# 修改前\n/dev/sdb1       /data0       xfs       defaults       1 2\n# 修改后\nLABEL=labelname       /data0       ext4       defaults       1 2\n```\n\n#### 块设备\n\n块设备是没有分区的裸盘，并没有UUID，可以使用设备id\n\n```\nll /dev/disk/by-id/\n```\n\n```\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 ata-HGST_HUS726T4TALE6L4_V1J32VRG -> ../../sdf\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 ata-HGST_HUS726T4TALE6L4_V1J53H2G -> ../../sde\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 ata-HGST_HUS726T4TALE6L4_V1J5U38G -> ../../sda\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 ata-HGST_HUS726T4TALE6L4_V1J5U9RG -> ../../sdb\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 ata-HGST_HUS726T4TALE6L4_V1J5W55G -> ../../sdg\n...\n# more\n...\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-SATA_HGST_HUS726T4TAL_V1J32VRG -> ../../sdf\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-SATA_HGST_HUS726T4TAL_V1J53H2G -> ../../sde\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-SATA_HGST_HUS726T4TAL_V1J5U38G -> ../../sda\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-SATA_HGST_HUS726T4TAL_V1J5U9RG -> ../../sdb\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-SATA_HGST_HUS726T4TAL_V1J5W55G -> ../../sdg\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-STOSHIBA_AL15SEB060N_Y0T0A2UYFM9F -> ../../sdd\nlrwxrwxrwx 1 root root  9 Jul 15 21:36 scsi-STOSHIBA_AL15SEB060N_Y0T0A30YFM9F -> ../../sdc\n```\n\n> 重复的既有ata和scsi随便选一个就行，都是指向同一个设备文件，我这里选scsi\n\n示例（ceph实例 ）：\n\n![](/images/image-mozg.png)\n","tags":["Disk","故障排除"],"categories":["Linux"]},{"title":"【笔记】Linux系统代理socks5","url":"/2021/04/09/【笔记】Linux系统代理socks5/","content":"\n### 前提条件\n\n* windows开启代理，关闭防护墙\n\n* 本地代理使用Clash端口7890\n\n* linux互通\n\n#### 添加系统代理\n\n```\nexport http_proxy=socks5://192.168.208.107:7890\nexport https_proxy=socks5://192.168.208.107:7890\nexport ALL_PROXY=socks5://192.168.208.107:7890\nexport no_proxy=\"localhost,127.0.0.1\"\n```\n\n#### 获取外网IP\n\n```\ncurl https://ip.42.pl/raw\n# 或者\ncurl \"http://httpbin.org/get\" |jq .origin|sed 's/\\\"//g'\n```\n\n取消代理\n\n```\nexport http_proxy=\"\"\nexport https_proxy=\"\"\nexport ALL_PROXY=\"\"\n```\n","tags":["代理"],"categories":["Linux"]},{"title":"【网络】华为交换机-ACL策略隔离同vlan下的互访","url":"/2021/03/12/【网络】华为交换机-ACL策略隔离同vlan下的互访/","content":"\n需求：一个vlan下分配了两个ip：192.168.1.1和192.168.2.1 sub，分配到192.168.1.1段的ip禁止互相访问。192.168.2.1不受影响。\n\n1. **进入系统视图**：\n\n   ```\n   system-view\n   ```\n\n2. **配置VLAN接口的IP地址**：\n\n   * 假设您的VLAN16接口是Vlanif16，您需要配置两个IP地址：\n\n     ```\n     vlan 16\n     int vlan 16\n     ip address 192.168.1.1 255.255.255.0\n     ip address 192.168.2.1 255.255.255.0 sub\n     ```\n\n3. **创建并配置ACL**：\n\n   * 定义一个ACL规则，拒绝192.168.1.0/24网段之间的访问：\n\n     ```\n     acl number 3000\n     rule deny ip source 192.168.1.0 0.0.0.255 destination 192.168.1.0 0.0.0.255\n     ```\n\n   * 这个规则会拒绝源IP和目的IP都在10.16.51.0/24网段之间的所有IP包。\n\n4. **将ACL应用到接口上**：\n\n   * 进入接口GigabitEthernet0/0/1，将ACL应用接口的入方向：\n\n     ```\n     interface GigabitEthernet0/0/1\n     port link-type access\n     port default vlan 16\n     traffic-filter inbound acl 3000\n     ```\n\n   * 这条命令将ACL 3000应用到接口GigabitEthernet0/0/1的入方向，拒绝10.16.51.0/24网段之间的流量。\n","tags":["ACL"],"categories":["网络"]},{"title":"【转载】DELL服务器新增硬盘扩容Raid5（System Setup方式）","url":"/2020/11/26/【转载】DELL服务器新增硬盘扩容Raid5（System Setup方式）/","content":"\n转载：[原文链接](https://zhuanlan.zhihu.com/p/704970516)\n\n如果服务器的空间不足了，如何进行扩容？我基本上按照如何重新配置[虚拟磁盘](https://zhida.zhihu.com/search?content_id=244791150\\&content_type=Article\\&match_order=1\\&q=%E8%99%9A%E6%8B%9F%E7%A3%81%E7%9B%98\\&zhida_source=entity)或添加其他硬盘来进行操作。我的机器上已经有三块硬盘了，组了Raid5，现在再添加一块硬盘。\n\n先把要添加的硬盘插入服务器，无论是在IDRAC还是管理界面，能看到新添加的硬盘（00:01:06）是Ready状态。\n\n![](https://pic4.zhimg.com/v2-74d847d985a527c7ac4b934822840b87_1440w.jpg)\n\n开机后，首先F2进入System Setup。\n\n选择设备设置\n\n![](https://pica.zhimg.com/v2-dd63aca2f8544abaf00fece6980fd48c_1440w.jpg)\n\n选择其中的H730Mini\n\n![](https://pica.zhimg.com/v2-95fd549595430f1bbf9d0101f71abece_1440w.jpg)\n\n我们要配置虚拟磁盘，所以选择Virtual Disk\n\n![](https://picx.zhimg.com/v2-cbd9bcc690a38041155d7d2a408efca5_1440w.jpg)\n\n选择这个虚拟磁盘，如果在Background Initialization，可以选择Stop，否则找不到Reconfigure Virtual Disks选项。\n\n![](https://pica.zhimg.com/v2-e94dab6bdcc1babbf90abeb5981cd1c2_1440w.jpg)\n\n选择重新配置虚拟磁盘选项，Go\n\n![](https://pic1.zhimg.com/v2-52c1a8655dc0f2755a78d59cca1bf7d8_1440w.jpg)\n\n选择RAID5，并选择增加新硬盘\n\n![](https://pic2.zhimg.com/v2-5c6dded93bcc54698fd7cbca3f0a6c0f_1440w.jpg)\n\n这里的配置我给忘记了，好像都行，选择完后点击Apply Changes\n\n![](https://pic1.zhimg.com/v2-eef84eb142e3624cfe8c334cff61b9b4_1440w.jpg)\n\n![](https://pic4.zhimg.com/v2-730418fa1016bd11624a655731a8608b_1440w.jpg)\n\n![](https://pic2.zhimg.com/v2-e7eaad359c9c657825ea6777b7534f69_1440w.jpg)\n\n选择Confirm\n\n![](https://picx.zhimg.com/v2-2c53bfbc2a903d230a044201e74a9235_1440w.jpg)\n\n![](https://pica.zhimg.com/v2-ee0dba9ff1a4afd063da84722af2d6ee_1440w.jpg)\n\n然后会跳转回来，选择Start Operation\n\n![](https://pic4.zhimg.com/v2-fc43dd669aad403007f704e281fc2385_1440w.jpg)\n\n提示开始执行了\n\n![](https://pic2.zhimg.com/v2-4390d8058038195fa18491f34c492d7f_1440w.jpg)\n\n当然，如果Start Operation后出现这个不要慌，可能上面哪一步错了，重新来一下\n\n![](https://pic3.zhimg.com/v2-2eaf66f807a616d06b0163ff902f06e6_1440w.jpg)\n\n然后可以看到开始重建了。可以在这个页面等着看进度，直到完成。也可以继续启动操作系统，只不过找不到能看进度的位置了。\n\n![](https://picx.zhimg.com/v2-87a1b019354b32c5a761a6c1b390aa03_1440w.jpg)\n\n在IDRAC也能看到四个磁盘联机了。但是真不知道进度是多少了，啊啊啊啊啊啊\n\n![](https://pic2.zhimg.com/v2-3ed732a97f52abaa4886cd027344e0c1_1440w.jpg)\n\n目前看2T的盘，15\\~20分钟执行了1%，全搞完得一天左右。\n\n![](https://picx.zhimg.com/v2-06238c0325e394f84d3a0c1a2748924f_1440w.jpg)\n","tags":["扩容"]},{"title":"【笔记】Linux DNS域名转发","url":"/2020/11/20/【笔记】Linux DNS域名转发/","content":"\n### 安装bind\n\n```\ndnf install bind bind-utils\n```\n\n### dns转发器配置\n\n```\nvim /etc/named.conf\n```\n\n```\n// 定义一个ACL（访问控制列表），用于分组特定的IP地址或网络范围\nacl clients {\n    // 允许10.20.0.0到10.20.255.255.255这个网段的地址进行查询\n    10.20.0.0/16;\n    // 允许本地主机进行查询\n    localhost;\n    // 允许本地网络进行查询\n    localnets;\n};\n\noptions {\n    // 设置BIND的工作目录，用于存放区域文件、日志等数据\n    directory \"/var/cache/bind\";\n\n    // 开启递归查询功能，允许BIND服务器代替客户端去查询其他DNS服务器\n    recursion yes;\n\n    // 定义允许查询本DNS服务器的客户端，这里指向之前定义的acl clients\n    allow-query { clients; };\n\n    // 定义当BIND服务器无法解析某个查询时，将查询转发给的上游DNS服务器\n    forwarders {\n        10.16.210.3;\n    };\n    // 仅使用forwarders定义的服务器进行查询，不使用自身的根提示\n    forward only;\n\n    // 设置DNSSEC验证为自动模式，BIND会自动尝试验证DNSSEC记录\n    dnssec-validation auto;\n\n    // 关闭权威NXDOMAIN响应，按照RFC1035的规定，不发送否定回答\n    auth-nxdomain no;\n\n    // 允许IPv6地址上的任何接口监听DNS查询\n    listen-on-v6 { any; };\n};\n```\n\n### dns解析配置\n\n```\nvim /etc/named.conf\n```\n\n```\noptions {\n    // 设置DNS服务器监听IPv4和IPv6的所有地址上的53号端口\n    listen-on port 53 { any; };\n    listen-on-v6 port 53 { any; };\n    // 指定BIND工作目录，用于存放所有BIND相关的文件\n    directory \t\"/var/named\";\n    // 设置缓存数据转储文件的路径\n    dump-file \t\"/var/named/data/cache_dump.db\";\n    // 设置统计信息文件的路径\n    statistics-file \"/var/named/data/named_stats.txt\";\n    // 设置内存使用统计文件的路径\n    memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n    // 设置记录递归查询的文件路径\n    recursing-file  \"/var/named/data/named.recursing\";\n    // 设置信任锚点文件的路径\n    secroots-file   \"/var/named/data/named.secroots\";\n    // 允许所有IP地址进行查询\n    allow-query     { any; };\n    // 开启递归查询功能\n    recursion yes;\n    // 开启DNSSEC功能\n    dnssec-enable yes;\n    // 开启DNSSEC验证\n    dnssec-validation yes;\n    // 指定ISC DLV（DNSSEC Lookaside Validation）密钥文件的路径\n    bindkeys-file \"/etc/named.root.key\";\n    // 指定动态更新密钥的目录\n    managed-keys-directory \"/var/named/dynamic\";\n    // 设置BIND进程的PID文件路径\n    pid-file \"/run/named/named.pid\";\n    // 设置BIND会话密钥文件的路径\n    session-keyfile \"/run/named/session.key\";\n};\nlogging {\n    // 设置日志记录的通道\n    channel default_debug {\n            // 设置日志文件的路径\n            file \"data/named.run\";\n\n            // 设置日志级别为动态，根据运行时的配置调整\n            severity dynamic;\n    };\n};\nzone \".\" IN {\n    // 定义根区域（\".\"代表根），使用\"hint\"类型，通常用于指定根服务器的地址\n    type hint;\n    // 指定根区域文件的路径\n    file \"named.ca\";\n};\n// 包含扩展的区域配置文件，通常包含正向和反向查找区域的定义\ninclude \"/etc/named.rfc1912.zones\";\n// 包含根密钥文件的配置，用于DNSSEC\ninclude \"/etc/named.root.key\";\n```\n","tags":["dns"],"categories":["Linux"]}]